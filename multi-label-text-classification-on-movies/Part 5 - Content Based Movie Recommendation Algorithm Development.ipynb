{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Content Based Movie Recommendation Algorithm Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "import decimal\n",
    "import unidecode\n",
    "\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Version 18 - Built on 25.03.2020</b><i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Functions used --------------------------------------------------------------------------------------------------\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "def get_index_from_input_movie(user_input, year_release):\n",
    "    return dataset.loc[(dataset['title_cleaned']==user_input) & (dataset['year']==year_release)]['index'].values[0]\n",
    "\n",
    "def search_words(row, list_of_words):\n",
    "    counter = 0\n",
    "    for word in list_of_words:\n",
    "        if word in row:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "def find_correct_genre(user_input, genre_list):\n",
    "    scores_sim=[]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for item in genre_list:\n",
    "        ed = nltk.edit_distance(user_input, item)\n",
    "        scores_sim.append(ed)\n",
    "    correct_genre_index = scores_sim.index(min(scores_sim))\n",
    "    correct_genre = genre_list[correct_genre_index].lower()\n",
    "    return correct_genre\n",
    "\n",
    "def union(lst1, lst2): \n",
    "    final_list = list(set(lst1) | set(lst2)) \n",
    "    return final_list\n",
    "\n",
    "def drange(x, y, jump):\n",
    "    while x < y:\n",
    "        yield float(x)\n",
    "        x += decimal.Decimal(jump)\n",
    "\n",
    "def create_imdb_range(x):\n",
    "    if x in list(drange(8, 10, '0.1')):\n",
    "        return 0.2\n",
    "    elif x in list(drange(6, 8, '0.1')):\n",
    "        return 0.4\n",
    "    elif x in list(drange(4, 6, '0.1')):\n",
    "        return 0.6\n",
    "    elif x in list(drange(2, 4, '0.1')):\n",
    "        return 0.8\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    \n",
    "    re_punc=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    \n",
    "    stripped=[re_punc.sub('', w) for w in raw_text.split(' ')]\n",
    "    \n",
    "    stripped=[token for token in stripped if token.isalpha()]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "    \n",
    "    no_stopword_text=[word for word in stripped if not word.lower() in stop_words]\n",
    "    \n",
    "    no_stopword_text = ' '.join(no_stopword_text) #i joined the text once more because a new lemmatizing approach is implemented below\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #approach 1: lemmatized_text = [lemmatizer.lemmatize(word, pos='v') for word in stripped]\n",
    "    #approach 1 was used until 21.02.2020, although we observed that only some of the tokens were lemmatized while others not.\n",
    "    #Thus, we developed an alternative approach like below to lemmatize as many tokens/words as possible\n",
    "    \n",
    "    #approach 2 developed on 22.02.2020:\n",
    "    lemmatized_text = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(word_tokenize(no_stopword_text))]\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    \n",
    "    lowercase_text = [word.lower() for word in lemmatized_text]\n",
    "    \n",
    "    return ' '.join(lowercase_text)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Import the dataset\n",
    "\n",
    "# dataset = pd.read_pickle('C:\\\\Users\\\\dq186sy\\\\Desktop\\\\Big Data Content Analytics\\\\Movie Recommendation System\\\\dataset_embedded_02092019.pkl')\n",
    "\n",
    "dataset_version=\"20072020\"\n",
    "\n",
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_4_14082020.pkl'))\n",
    "\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "dataset['index'] = np.arange(0, len(dataset))\n",
    "\n",
    "genres_list=pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\genres_list_06032020.pkl'))\n",
    "year_list=pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\year_list_{0}.pkl'.format(dataset_version)))\n",
    "movie_title_list=pd.read_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\movie_title_list_{0}.pkl'.format(dataset_version)))\n",
    "movie_title_list=[re.sub(' +', ' ', unidecode.unidecode(x).lower().replace('-', '').replace('the', '').replace(':', '').strip()) for x in movie_title_list]\n",
    "\n",
    "dataset['year']=year_list\n",
    "dataset['title_cleaned']=movie_title_list\n",
    "\n",
    "# It is important to reset the index of the dataset in order to get the correct index per movie!\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Create the movie_genre list with the unique types of genre \n",
    "\n",
    "movie_genre_list=genres_list\n",
    "\n",
    "movie_genre_list = [x.lower() for x in movie_genre_list]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 1: Get the user's input and transform it to the appropriate form\n",
    "\n",
    "input_one = input(\"Give me a movie genre (i.e romance, action, adventure): \")\n",
    "\n",
    "input_one = find_correct_genre(input_one.lower(), movie_genre_list)\n",
    "\n",
    "print(\"The movie genre selected by the user: {}\".format(input_one))\n",
    "\n",
    "input_movie = re.sub(' +', ' ', unidecode.unidecode(input(\"Give me the title of a movie: \")).lower().replace('-', '').replace('the', '').replace(':', '').strip())\n",
    "#input movie, movie title clean, duplicate movie titles should be cleaned with the same exact way.\n",
    "\n",
    "# Using the genre input given by the user, isolate those movies that match the given genre (i.e Action movies)\n",
    "\n",
    "lower_case_genres = []\n",
    "lower_case_genres=[[element.lower() for element in dataset.loc[:, 'reduced_genres'].iloc[i]] for i in range(len(dataset.loc[:, 'reduced_genres']))]\n",
    "dataset.loc[:,'lower_case_genres'] = lower_case_genres\n",
    "selected_rows = dataset.loc[:, 'lower_case_genres'].apply(lambda x: any(item for item in x if item == input_one))\n",
    "locked_frame = dataset[selected_rows]\n",
    "indexes_list = locked_frame.loc[:, 'index'].tolist()\n",
    "locked_frame.loc[:, 'index'] = np.arange(0, len(locked_frame))\n",
    "\n",
    "# Check for duplicate movies\n",
    "duplicate_movies = locked_frame[locked_frame.duplicated(['title'])]\n",
    "duplicate_titles_list=duplicate_movies['title'].values.tolist()\n",
    "duplicate_titles_list=[re.sub(' +', ' ', unidecode.unidecode(x).lower().replace('-', '').replace('the', '').replace(':', '').strip()) for x in duplicate_titles_list]\n",
    "\n",
    "if input_movie in duplicate_titles_list:\n",
    "    \n",
    "    duplicate_movie=dataset.loc[(dataset['title_cleaned']==input_movie)]\n",
    "    print(\"The movie {0} you gave me is found {1} times.\\nThe available movies with the same title\".format(duplicate_movie['title'].iloc[0], duplicate_movie.shape[0]))\n",
    "    for i in range(len(duplicate_movie)):\n",
    "\n",
    "        print(\"\\n(Title) {0}: Staring {1} & Released on {2}\\n\".format(duplicate_movie['title'].iloc[i], duplicate_movie['actors'].iloc[i][0], duplicate_movie['year'].iloc[i]))\n",
    "\n",
    "    input_option_selected = input(\"Please select one of the above options by typing Title-Year of release: \")\n",
    "    input_movie=re.sub(' +', ' ', unidecode.unidecode(input_option_selected.split('-')[0]).lower().replace('-', '').replace('the', '').replace(':', '').strip())\n",
    "    year_release=input_option_selected.split('-')[1]\n",
    "    print(\"The movie title selected by the user: {0} released on {1}\".format(input_movie, year_release))\n",
    "\n",
    "else:\n",
    "    if input_movie in locked_frame['title_cleaned'].tolist():\n",
    "        print(\"The movie title selected by the user: {0} released on {1}\".format(input_movie, locked_frame['year'].loc[(locked_frame['title_cleaned']==input_movie)].tolist()[0]))\n",
    "        year_release=locked_frame['year'].loc[(locked_frame['title_cleaned']==input_movie)].tolist()[0]\n",
    "    else:\n",
    "        print(\"The movie title you selected does not belong to the movie genre you selected\")\n",
    "        pass\n",
    "\n",
    "input_two = input(\"Now think of some reasons why you like the movie '{}':\".format(input_movie)).lower().replace(',', '').replace('.', '')\n",
    "\n",
    "inputs_list=preprocess_text(input_two).split(' ')\n",
    "inputs_list = list(dict.fromkeys(inputs_list)) # remove duplicate words\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Phase 2: Slice the dataset based on the user's input & add to the user's list of word, the words contained in the movie features\n",
    "\n",
    "# Check of the movie user gave is in the movie list of the dataset\n",
    "\n",
    "selected_genre_movies_list = locked_frame['title_cleaned'].tolist()\n",
    "\n",
    "if input_movie in selected_genre_movies_list:\n",
    "    \n",
    "    movie_plot_new = locked_frame.loc[:, \"clean_combined_features\"].loc[(locked_frame['title_cleaned']==str(input_movie)) & (locked_frame['year']==str(year_release))].apply(lambda x: list(set(re.split(' ', x.strip().lower())))).values[0]\n",
    "    \n",
    "    plot_user_input_list = inputs_list + movie_plot_new\n",
    "    \n",
    "    plot_user_input_list = list(dict.fromkeys(plot_user_input_list))\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Get the index of the movie provied by the user\n",
    "\n",
    "    movie_index = get_index_from_input_movie(input_movie, year_release)\n",
    "    \n",
    "    # Based on the index from the initial dataset locate the same in the Locked_frame.\n",
    "    # It is important to locate the same movie!\n",
    "    \n",
    "    locked_frame_index = locked_frame.loc[locked_frame['title_cleaned'].str.lower().str.replace('-', '').str.replace('the', '').str.replace(':', '').str.strip() == input_movie]['index'].values[0]\n",
    "    \n",
    "    assert dataset.title.iloc[movie_index]==locked_frame.title.iloc[locked_frame_index]\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Phase 3: Locate the word embeddings belonging to each of the three different columns (Actors, Plot, Features, Reviews)\n",
    "    \n",
    "    # Load the saved embeddings trained by the multi-input keras classifier (embeddings of 49)\n",
    "    with open(os.path.join(os.getcwd(), 'final_word_embeddings_conversational_agent\\\\keras_embeddings_array_concatenated_14082020.pkl'), 'rb') as f:\n",
    "        \n",
    "        keras_embeddings_array_concatenated = pickle.load(f)\n",
    "        \n",
    "    # Phase 3.1: Locate the embeddings of the movie selected by the user!\n",
    "    \n",
    "    selected_movie_embeddings = keras_embeddings_array_concatenated[movie_index]\n",
    "    \n",
    "    selected_movie_embeddings=selected_movie_embeddings.reshape(1,-1)\n",
    "    \n",
    "    # Phase 3.2: Locate the embeddings of the movies that match the GENRE given by the user (i.e the embeddings of all the ACTION movies)\n",
    "    \n",
    "    locked_movie_embeddings = keras_embeddings_array_concatenated[indexes_list]\n",
    "    \n",
    "    assert selected_movie_embeddings.shape[1] == locked_movie_embeddings.shape[1]\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Phase 4: Calculate Cosine Distance\n",
    "\n",
    "    cosine_dist = cosine_distances(locked_movie_embeddings, selected_movie_embeddings.reshape(1,-1))\n",
    "    \n",
    "    # Get the similar movies & Slice the dataframe on the top 15 most similar movies to the movie given  by the user\n",
    "\n",
    "    movie_return = np.argsort(cosine_dist, axis=None).tolist()[1:16]\n",
    "\n",
    "    # movie_return contains the index of the 15 movies most similar to the movie selected by the user!\n",
    "    \n",
    "    # So the next step is to isolate those 15 movies and their features\n",
    "    \n",
    "    locked_frame_new = locked_frame[locked_frame.loc[:, 'index'].isin(movie_return)]\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    # Phase 5: Create two new columns \"Unique Words\" + \"Number of words\"\n",
    "\n",
    "    # Create the new column of \"UNIQUE\" words of the combined features\n",
    "    \n",
    "    locked_frame_new.loc[:, 'unique_words'] = locked_frame_new.loc[:, 'clean_combined_features']+locked_frame_new.loc[:, 'clean_reviews']\n",
    "\n",
    "    locked_frame_new.loc[:, 'unique_words'] = locked_frame_new.loc[:, 'unique_words'].apply(lambda x: list(set(re.split(' ', x.strip().lower()))))\n",
    "\n",
    "    locked_frame_new.loc[:, 'unique_words'] = [[x for x in lst if x] for lst in locked_frame_new.loc[:, 'unique_words']]\n",
    "  \n",
    "    # Create the column \"Number of words\" for each word contained in the unique words column\n",
    "    \n",
    "    locked_frame_new.loc[:, 'number_of_words'] = locked_frame_new.loc[:, 'unique_words'].apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------------------\n",
    "\n",
    "    # Phase 6: Recommend to the user the three most similar and highly scored movies \n",
    "    \n",
    "    # Calculate the movie score\n",
    "    \n",
    "    locked_frame_new['imdb_rating_range']=locked_frame_new['imdb_rating'].apply(create_imdb_range)\n",
    "\n",
    "    locked_frame_new.loc[:, 'movie_score'] = 1*locked_frame_new.loc[:, 'imdb_rating_range'].astype(float) + 0.5*locked_frame_new.loc[:, 'number_of_words'] + 0.5*locked_frame_new.loc[:, \"sentiment_value\"] + 1*locked_frame_new.loc[:, \"rating\"]\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_four_rows = locked_frame_new.nlargest(4, 'movie_score')\n",
    "\n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_four_rows.loc[:, ['title', 'imdb_rating', 'imdb_url']].values.tolist()\n",
    "    \n",
    "    print(\"Movie Recommendations: {}\".format(recommendations_list))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"Apologize but the movie title requested is not in my records, although below are some of my best suggestions similar to the input you gave me!\")\n",
    "    \n",
    "    plot_user_input_list = inputs_list\n",
    "    \n",
    "    locked_frame.loc[:, 'unique_words'] = locked_frame.loc[:, 'clean_combined_features']+locked_frame.loc[:, 'clean_reviews']\n",
    "\n",
    "    locked_frame.loc[:, 'unique_words'] = locked_frame.loc[:, 'unique_words'].apply(lambda x: list(set(re.split(' ', x.strip().lower()))))\n",
    "\n",
    "    locked_frame.loc[:, 'unique_words'] = [[x for x in lst if x] for lst in locked_frame.loc[:, 'unique_words']]\n",
    "  \n",
    "    # Create the column \"Number of words\" for each word contained in the unique words column\n",
    "\n",
    "    locked_frame.loc[:, 'number_of_words'] = locked_frame.unique_words.apply(search_words, args=(plot_user_input_list,))\n",
    "\n",
    "    #Recommend to the user the three most similar and highly scored movies\n",
    "    \n",
    "    locked_frame['imdb_rating_range']=locked_frame['imdb_rating'].apply(create_imdb_range)\n",
    "\n",
    "    locked_frame.loc[:, 'movie_score'] = 1*locked_frame.loc[:, 'imdb_rating_range'].astype(float) + 0.5*locked_frame.loc[:, 'number_of_words'] + 0.5*locked_frame.loc[:, \"sentiment_value\"] + 0.5*locked_frame.loc[:, \"rating\"]\n",
    "    \n",
    "    # Give to the user the proper movie recommendation\n",
    "\n",
    "    top_four_rows = locked_frame.nlargest(4, 'movie_score')\n",
    "    \n",
    "    # Recommend the movie\n",
    "\n",
    "    recommendations_list = top_four_rows.loc[:, ['title', 'imdb_rating', 'imdb_url']].values.tolist()\n",
    "    \n",
    "    print(\"\\nMovie Recommendations: {}\".format(recommendations_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run chatbotapp_v21.py (using the cmd terminal on Windows):\n",
    "\n",
    "Step 1: Set the path directory to: Desktop (if you have saved the chatbotapp_v21.py file in Desktop) <br>\n",
    "Step 2 (Run the command): python chatbotapp_v21.py or FLASK_APP=hello.py flask run\n",
    "\n",
    "#### Run the https protocole (using the cmd terminal on Windows): \n",
    "\n",
    "Open a cmd terminal and then:\n",
    "Step 1: Set the path directory to the path where the ngrok.exe is saved (when you first downloaded) <br>\n",
    "Step 2  (Run the command): ngrok http + \"port number\" (port number where the app.py file runs). Since the chatbotapp.py runs on port 9090 the command should be **ngrok http 9090**<br>\n",
    "Step 3: Copy paste the **https** link that ends to .io (this link is updated every time the command is executed) <br>\n",
    "Step 4: Copy paste the link to Dialogflow engine under the tab: fulfilment & save the change.\n",
    "Step 5: Go on the upper right text box on the Dialogflow page and test the conversation with the agent.\n",
    "\n",
    "! **Important:** Webhook and MLFLOW (default port 5000) cannot run the same port **!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
