{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Compare different models trained with attention layer.ipynb","provenance":[{"file_id":"1sdiEg21xl94Ja3eAm7xEUDEG6V8m2rZ8","timestamp":1608625120659}],"collapsed_sections":["MkRThlTg7B7b","dmxu7FBx7B7d","a7mwhWd87B7g","B8aScXx17B7g","69TfXaAz7B7i","0R4jCk237B7m"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"p1M1fNDP7B7N"},"source":["### Compare models trained with attention layer.\n","\n","The purpose of the notebook is to select the best model among the various models trained with Keras Attention Layer mechanic.\n","\n","As of this moment the comparison are made between models that have been trained on custom Attention Layer and not the one provided by Tensorflow in version 2.4.1.\n","\n","To select the best model we used the following guidelines:\n","\n","* 1) The model with the lowest hamming loss & zero one loss\n","* 2) The model with the lowest test score and the highest test accuracy values\n","* 3) The model with the most accurate predictions among the 17 labels. It is of high importance the best model to identify correctly the most of the genre tags. Models that cannot identify more than 2 genre tags will not be prefered.\n","* 4) Compare model predictions on movie never seen before.\n","* 5) Training-Validation metrics comparison."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgRqjV4Q8EQe","executionInfo":{"status":"ok","timestamp":1609751641136,"user_tz":-120,"elapsed":16422,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"4b244d1c-07b5-4c76-b512-e75ba893add9"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"6ELa982z8Cjd","executionInfo":{"status":"ok","timestamp":1609751667808,"user_tz":-120,"elapsed":42550,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"44eb6897-ae92-4e4e-dda5-322cd4046722"},"source":["\"\"\"\r\n","Install those libraries if the notebook is executed on Google Colab\r\n","\"\"\"\r\n","!pip install --quiet unidecode\r\n","!pip install --quiet humanfriendly\r\n","!pip install git+https://github.com/tensorflow/docs"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 245kB 4.2MB/s \n","\u001b[K     |████████████████████████████████| 92kB 3.9MB/s \n","\u001b[?25hCollecting git+https://github.com/tensorflow/docs\n","  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-fvmlz4hb\n","  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-fvmlz4hb\n","Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0430fe3be375a1f3f6267b86300e1c0f1d79b2b26-) (0.8.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0430fe3be375a1f3f6267b86300e1c0f1d79b2b26-) (0.10.0)\n","Collecting protobuf>=3.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/fd/247ef25f5ec5f9acecfbc98ca3c6aaf66716cf52509aca9a93583d410493/protobuf-3.14.0-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 5.9MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0430fe3be375a1f3f6267b86300e1c0f1d79b2b26-) (3.13)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0430fe3be375a1f3f6267b86300e1c0f1d79b2b26-) (0.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py->tensorflow-docs===0.0.0430fe3be375a1f3f6267b86300e1c0f1d79b2b26-) (1.15.0)\n","Building wheels for collected packages: tensorflow-docs\n","  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.0430fe3be375a1f3f6267b86300e1c0f1d79b2b26_-cp36-none-any.whl size=146357 sha256=e2541c93275a011fd7de28471e3e5f448c55e3560926d8ff21b47ec722a6d464\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-z4ar9_k1/wheels/eb/1b/35/fce87697be00d2fc63e0b4b395b0d9c7e391a10e98d9a0d97f\n","Successfully built tensorflow-docs\n","Installing collected packages: protobuf, tensorflow-docs\n","  Found existing installation: protobuf 3.12.4\n","    Uninstalling protobuf-3.12.4:\n","      Successfully uninstalled protobuf-3.12.4\n","Successfully installed protobuf-3.14.0 tensorflow-docs-0.0.0430fe3be375a1f3f6267b86300e1c0f1d79b2b26-\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"F-cvpoZd7B7T"},"source":["#### Import the libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":16,"output_embedded_package_id":"1zf9xmEMAlorvNMAKZCF_iwz02TRH1QbB"},"id":"k-V2jW4q7B7T","executionInfo":{"status":"ok","timestamp":1609751673298,"user_tz":-120,"elapsed":5074,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"22e4a697-7f84-4e0c-d80e-cab416cb018f"},"source":["import collections\n","\n","try:\n","    collectionsAbc = collections.abc\n","except AttributeError:\n","    collectionsAbc = collections\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import random\n","import pickle\n","import json\n","import shutil\n","import unidecode\n","import glob\n","\n","%matplotlib inline\n","import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","\n","from datetime import datetime\n","from tabulate import tabulate\n","from packaging import version\n","from humanfriendly import format_timespan\n","from sklearn.metrics import confusion_matrix, classification_report, hamming_loss, zero_one_loss, f1_score, roc_auc_score\n","\n","from IPython.core.display import display, HTML\n","display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n","date_format='%Y-%m-%d %H-%M-%S'\n","\n","%matplotlib inline\n","import matplotlib.image as mpimg\n","import matplotlib.pyplot as plt\n","import plotly\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import plotly.offline as py\n","\n","import pydot\n","import pydotplus\n","import graphviz\n","\n","from IPython.display import SVG\n","from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n","init_notebook_mode(connected=False)\n","from pylab import rcParams\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import tensorflow_docs as tfdocs #!pip install git+https://github.com/tensorflow/docs\n","import tensorflow_docs.plots as tfplots\n","import tensorflow_docs.modeling as tfmodel\n","\n","from tensorflow.keras import layers, regularizers, models\n","from tensorflow.keras import models\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.utils import model_to_dot, plot_model\n","from tensorflow.keras.models import load_model, model_from_json\n","\n","import tensorflow_hub as hub\n","from tensorboard.plugins.hparams import api as hp\n","\n","import keras.backend as K\n","from tensorflow import keras"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6y3OBxO7B7V","executionInfo":{"status":"ok","timestamp":1609751673652,"user_tz":-120,"elapsed":5406,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"78601c43-eba2-40d3-e405-9273b806fa07"},"source":["print(\"TensorFlow version: \", tf.__version__)\n","assert version.parse(tf.__version__).release[0] >= 2, \\\n","    \"This notebook requires TensorFlow 2.0 or above.\"\n","\n","print(\"Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"Hub version: \", hub.__version__)\n","print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow version:  2.4.0\n","Version:  2.4.0\n","Eager mode:  True\n","Hub version:  0.10.0\n","GPU is NOT AVAILABLE\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GeZXaD-T7B7V"},"source":["#### Import the data already tokenized and transformed from Part 3.1"]},{"cell_type":"markdown","metadata":{"id":"TQLW740f7B7S"},"source":["Set the version data control parameter (to save the outputs of this notebook at their latest date)"]},{"cell_type":"code","metadata":{"id":"2Fiz44_q7B7S"},"source":["class initialize_notebook_variables():\r\n","    saved_word_tokenizers=\"13072020\" #the date I saved the word tokenizers for each of my five inputs\r\n","    tokenization_history_folder=\"text_tokenization_padded_sequences\" #the Drive folder were tokenizers & x,y are saved\r\n","    batch_size_value = \"32batch\" #the batch size version of the model fit()\r\n","    labelsmoothing_value = \"nolabelsmoothing\" #if label smoothing is applied\r\n","    approach_type = \"approach1\" #approach implementation of Attention layer\r\n","    saved_model_name=\"classification_attention_layer_model\"\r\n","    learning_rate_scheduler = \"inverse_time_decay\"\r\n","    dropout_rate = \"0.0dropout\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOathr_G7B7W"},"source":["\"\"\"\n","1. Import the number of words tokenized per input\n","2. Import the class tokenizer per input\n","3. Import the X_train, X_validation, y_train, y_validation data for training and validation neural network during training\n","4. Import X_test, y_test data for evaluating the performance of the trained neural networks\n","5. Import the genres\n","\n","Train-Test split ratio is: 80-20% and the data were not balanced. The initial frequency ratios have been kept\n","Train-Validation split ratio is: 80-20%\n","\"\"\"\n","\n","#1. Word tokenized\n","with open(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/words_tokenized_{initialize_notebook_variables.saved_word_tokenizers}.pkl\", 'rb') as f:\n","    words_tokenized = pickle.load(f)\n","\n","#2. Tokenizers\n","with open(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/actors_tokenizer_{words_tokenized['actors_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.pkl\",'rb') as f:\n","    actors_tokenizer = pickle.load(f)\n","    \n","with open(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/plot_tokenizer_{words_tokenized['plot_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.pkl\",'rb') as f:\n","    plot_tokenizer = pickle.load(f)\n","    \n","with open(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/features_tokenizer_{words_tokenized['features_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.pkl\",'rb') as f:\n","    features_tokenizer = pickle.load(f)\n","    \n","with open(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/reviews_tokenizer_{words_tokenized['reviews_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.pkl\",'rb') as f:\n","    reviews_tokenizer = pickle.load(f)\n","    \n","with open(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/title_tokenizer_{words_tokenized['title_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.pkl\",'rb') as f:\n","    title_tokenizer = pickle.load(f)\n","\n","try:\n","    assert len(actors_tokenizer.word_index)==words_tokenized['actors_tokenized']\n","    assert len(plot_tokenizer.word_index)==words_tokenized['plot_words_tokenized']\n","    assert len(features_tokenizer.word_index)==words_tokenized['features_words_tokenized']\n","    assert len(reviews_tokenizer.word_index)==words_tokenized['reviews_words_tokenized']\n","    assert len(title_tokenizer.word_index)==words_tokenized['title_words_tokenized']\n","except AssertionError:\n","    print(\"ERROR: The vocabulary length for some of the tokenizers, is not equal to 20000. Please verify their lengths by running the following: len(actors_tokenizer.word_index)\")\n","\n","#3. Train, Validation data samples\n","validation_data_split = \"8020\"\n","\n","X_train_seq_actors=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_train_seq_actors_stratified_{validation_data_split}.npy\")\n","X_train_seq_plot=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_train_seq_plot_stratified_{validation_data_split}.npy\")\n","X_train_seq_features=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_train_seq_features_stratified_{validation_data_split}.npy\")\n","X_train_seq_reviews=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_train_seq_reviews_stratified_{validation_data_split}.npy\")\n","X_train_seq_title=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_train_seq_title_stratified_{validation_data_split}.npy\")\n","\n","X_validation_seq_actors=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_validation_seq_actors_stratified_{validation_data_split}.npy\")\n","X_validation_seq_plot=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_validation_seq_plot_stratified_{validation_data_split}.npy\")\n","X_validation_seq_features=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_validation_seq_features_stratified_{validation_data_split}.npy\")\n","X_validation_seq_reviews=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_validation_seq_reviews_stratified_{validation_data_split}.npy\")\n","X_validation_seq_title=np.load(f\"/content/drive/MyDrive/AttentionLayer/x_validation_seq_title_stratified_{validation_data_split}.npy\")\n","\n","y_train=np.load(f\"/content/drive/MyDrive/AttentionLayer/y_train_stratified_{validation_data_split}.npy\")\n","y_validation=np.load(f\"/content/drive/MyDrive/AttentionLayer/y_valid_stratified_{validation_data_split}.npy\")\n","\n","#4. Test data samples\n","X_test_seq_actors=np.load(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/x_test_seq_actors_80-20_non-balanced_{words_tokenized['actors_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.npy\")\n","X_test_seq_plot=np.load(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/x_test_seq_plot_80-20_non-balanced_{words_tokenized['plot_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.npy\")\n","X_test_seq_features=np.load(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/x_test_seq_features_80-20_non-balanced_{words_tokenized['features_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.npy\")\n","X_test_seq_reviews=np.load(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/x_test_seq_reviews_80-20_non-balanced_{words_tokenized['reviews_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.npy\")\n","X_test_seq_title=np.load(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/x_test_seq_title_80-20_non-balanced_{words_tokenized['title_words_tokenized']}_{initialize_notebook_variables.saved_word_tokenizers}.npy\")\n","\n","y_test=np.load(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/y_test_80-20_non-balanced_{initialize_notebook_variables.saved_word_tokenizers}.npy\")\n","\n","#5. Genre tags (target label)\n","with open(f\"/content/drive/MyDrive/data/{initialize_notebook_variables.tokenization_history_folder}_{initialize_notebook_variables.saved_word_tokenizers}/genres_list_06032020.pkl\",'rb') as f:\n","    genres_list = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yn2axJpsrHW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609751737750,"user_tz":-120,"elapsed":1340,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"b6fbc8ef-e07a-4d11-dae2-21dff43129ef"},"source":["folder_path_model_saved=f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}_{initialize_notebook_variables.dropout_rate}\"\r\n","#folder_path_model_saved=f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}\"\r\n","\r\n","print(folder_path_model_saved)\r\n","model_path = glob.glob(folder_path_model_saved + '/*.h5')\r\n","saved_model_date = model_path[-1].split(\"/\")[-1].split(\"_\")[-1].rstrip(\".h5\")\r\n","#saved_df_scored_metric_name=\"df_metrics_multi_input_keras_model\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZpnuD-hDqKc","executionInfo":{"status":"ok","timestamp":1609602349403,"user_tz":-120,"elapsed":1047,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"16520976-3426-4ff6-c696-69e9b9279faa"},"source":["0.001 / (1 + 0.1 * 1 / 9791)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0009999897866429719"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIZb7IIA4U1G","executionInfo":{"status":"ok","timestamp":1609601874992,"user_tz":-120,"elapsed":1397,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"b157c15f-7cb8-4ffe-b934-5a6b22618ba0"},"source":["np.ceil((X_train_seq_actors.shape[0])//128)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["244.0"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"jrAOdpnA7B7Y"},"source":["Initialise some predefined values first:\n","\n","* Set Optimization function\n","* Model loss\n","* Model metric"]},{"cell_type":"code","metadata":{"id":"xUc8p32g7B7Z"},"source":["neural_network_parameters={}\n","optimizer_parameters={}\n","\n","#----------------------------------------------------------------------\n","\n","# Model Compilation\n","neural_network_parameters['model_loss'] = tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy', label_smoothing=0.0) #'binary_crossentropy'\n","neural_network_parameters['model_metric'] = [tfa.metrics.HammingLoss(mode=\"multilabel\", name=\"hamming_loss\"),\n","                                             tfa.metrics.F1Score(y_train.shape[-1], average=\"micro\", name=\"f1_score_micro\"), \n","                                             tfa.metrics.F1Score(y_train.shape[-1], average=None, name=\"f1_score_none\"),\n","                                             tfa.metrics.F1Score(y_train.shape[-1], average=\"macro\", name=\"f1_score_macro\")]\n","\n","#----------------------------------------------------------------------\n","# Function 1 - Optimizers\n","\n","# Optimizer: ADAM (Learning scheduler with Inverse Time Decay)\n","\n","optimizer_parameters['lr_scheduler_decay_rate'] = 0.1\n","optimizer_parameters['staircase'] = False\n","\n","def optimizer_adam_v2(hparams):\n","\n","    # Inverse Time Decay\n","    initial_learning_rate = hparams[HP_LEARNING_RATE]\n","    decay_steps = int(np.ceil((X_train_seq_actors.shape[0])//hparams[HP_HIDDEN_UNITS]))*hparams[HP_DECAY_STEPS_MULTIPLIER]\n","    decay_rate = 0.1\n","    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate, decay_steps, decay_rate, staircase=False)\n","\n","    # PiecewiseConstantDecay\n","    # step = tf.Variable(0, trainable=False)\n","    # # boundaries = [100000, 110000]\n","    # boundaries = [1000, 5000]\n","    # values = [0.001, 0.0005, 0.00025]\n","\n","    # learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n","\n","    # Later, whenever we perform an optimization step, we pass in the step.\n","    # learning_rate_adam = learning_rate_fn(step)\n","\n","    return keras.optimizers.Adam(learning_rate=learning_rate_fn)\n","#---------------------------------------------------------------------------------------\n","\n","# Optimizer: SDG (version 1)\n","\n","optimizer_parameters['SGD_momentum'] = 0.2 #default 0.0\n","optimizer_parameters['SGD_nesterov'] = True #default False\n","\n","def optimizer_sgd_v1(haparms, mode):\n","\n","    if mode==\"step decay\":\n","\n","        return keras.optimizers.SGD(lr=0.0, #Notice that we set the learning rate in the SGD class to 0 to clearly indicate that it is not used.\n","                                    momentum=0.9 #Use a large momentum. Using a larger momentum value will help the optimization algorithm to continue to make updates in the right direction when your learning rate shrinks to small values.\n","                                   )\n","    else:\n","        return keras.optimizers.SGD(lr=hparams[HP_LEARNING_RATE],\n","                              momentum=optimizer_parameters['SGD_momentum'],\n","                              nesterov=optimizer_parameters['SGD_nesterov'])\n","\n","#---------------------------------------------------------------------------------------\n","\n","# Optimizer: RMSprop (version 1)\n","\n","optimizer_parameters['RMSprop_momentum'] = 0.5\n","optimizer_parameters['RMSprop_centered'] = True\n","\n","def optimizer_rmsprop_v1(haparms):\n","\n","    return keras.optimizers.RMSprop(lr=hparams[HP_LEARNING_RATE],\n","                                    momentum=optimizer_parameters['RMSprop_momentum'],\n","                                    centered=optimizer_parameters['RMSprop_centered'])\n","\n","#----------------------------------------------------------------------\n","\n","# Function 2\n","def hamming_loss(y_true, y_pred, mode='multilabel'):\n","    if mode not in ['multiclass', 'multilabel']:\n","        raise TypeError('mode must be: [multiclass, multilabel])')\n","\n","    if mode == 'multiclass':\n","        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n","        print(nonzero)\n","        return 1.0 - nonzero\n","\n","    else:\n","        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), tf.float32)\n","        return nonzero / y_true.shape[-1]\n","\n","#Metric Wrapper to make Hamming loss a metric for model metrics\n","class HammingLoss(tfa.metrics.MeanMetricWrapper):\n","    def __init__(self, name='hamming_loss_approach2', dtype=None, mode='multilabel'):\n","        super(HammingLoss, self).__init__(hamming_loss, name, dtype=dtype, mode=mode)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ERZFsCx7B7Z"},"source":["# Function 1: Import the model structure and weights\n","def import_trained_keras_model(model_index, method, decay_steps_mode, optimizer_name, hparams):\n","    \"\"\"\n","    Load the weights of the model saved with EarlyStopping\n","    \"\"\"\n","    if method == \"import custom trained model\":\n","        \n","        if decay_steps_mode==\"on\":\n","            \n","            model_path_structure=f\"{folder_path_model_saved}/{initialize_notebook_variables.saved_model_name}_{hparams[HP_EMBEDDING_DIM]}dim_{hparams[HP_HIDDEN_UNITS]}batchsize_{hparams[HP_LEARNING_RATE]}lr_{hparams[HP_DECAY_STEPS_MULTIPLIER]}decaymultiplier\"\n","            model_structure = glob.glob(model_path_structure + '*.json')[-1]\n","            model_weights = glob.glob(model_path_structure + '*.h5')[-1]\n","            print(model_path_structure)\n","            print(model_weights)\n","            print(model_structure)\n","\n","            with open(f\"{model_structure}\",'r') as f:\n","                model_json = json.load(f)\n","\n","            model_imported = model_from_json(model_json, custom_objects={'attention': attention(return_sequences=True)})\n","            #model_imported = model_from_json(model_json, custom_objects={'peel_the_layer': peel_the_layer})\n","            #model_imported = model_from_json(model_json, custom_objects={'Attention': Attention})\n","\n","            model_imported.load_weights(f\"{model_weights}\")\n","        \n","        else:\n","            with open(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}.json\".format(folder_path_model_saved,\n","                                                                                                     saved_model_name,\n","                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                     str(hparams[HP_LEARNING_RATE]),\n","                                                                                                     version_data_control)),'r') as f:\n","                model_json = json.load(f)\n","\n","            model_imported = model_from_json(model_json)\n","\n","            model_imported.load_weights(os.path.join(os.getcwd(), '{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}.h5'.format(folder_path_model_saved,\n","                                                                                                                     saved_model_name,\n","                                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n","                                                                                                                     version_data_control)))\n","        if optimizer_name==\"adam\":\n","            optimizer = optimizer_adam_v2(hparams)\n","        \n","        elif optimizer_name==\"sgd\":\n","            optimizer = optimizer_sgd_v1(hparams, \"step decay\")\n","            \n","        else:\n","            optimizer = optimizer_rmsprop_v1(hparams)\n","            \n","        model_imported.compile(optimizer=optimizer,\n","                               loss=neural_network_parameters['model_loss'],\n","                               metrics=neural_network_parameters['model_metric'])\n","        print(f\"Model {model_index} is loaded successfully\\n\")\n","    \n","    else:\n","        \n","        with open(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.json\".format(folder_path_model_saved,\n","                                                                                                                    saved_model_name,\n","                                                                                                                    str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                                    str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                                    str(hparams[HP_LEARNING_RATE]), \n","                                                                                                                    str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","                                                                                                                    version_data_control)),'r') as f:\n","            model_json = json.load(f)\n","\n","        model_imported = model_from_json(model_json, custom_objects={'KerasLayer':hub.KerasLayer})\n","\n","        model_imported.load_weights(os.path.join(os.getcwd(), '{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.h5'.format(folder_path_model_saved,\n","                                                                                                                                    saved_model_name,\n","                                                                                                                                    str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                                                    str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                                                    str(hparams[HP_LEARNING_RATE]), \n","                                                                                                                                    str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","                                                                                                                                    version_data_control)))\n","\n","        optimizer = optimizer_adam_v2(hparams[HP_LEARNING_RATE], hparams[HP_DECAY_STEPS_MULTIPLIER], partial_x_train_actors_array.shape[0], optimizer_parameters['validation_split_ratio'], hparams[HP_HIDDEN_UNITS])\n","\n","        model_imported.compile(optimizer=optimizer,\n","                               loss=neural_network_parameters['model_loss'],\n","                               metrics=neural_network_parameters['model_metric'])\n","        print(\"Model is loaded successfully\\n\")\n","    \n","    return model_imported\n","\n","#----------------------------------------------------------------------\n","\n","# Function 2: Create the dataframe with the computed metrics.\n","def create_df_scoring_table(method, decay_steps_mode, model_tag, hparams, model):\n","    \"\"\"\n","    Create a scoring dictionary to select the best out of the four models\n","    \"\"\"\n","    if method == \"import custom trained model\":\n","        model_evaluation = model.evaluate([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title],\n","                                          y_test,\n","                                          batch_size=hparams[HP_HIDDEN_UNITS],\n","                                          verbose=2)\n","\n","        y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n","        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n","\n","        variance = np.var(y_test_predictions)\n","        sse = np.mean((np.mean(y_test_predictions) - y_test)**2)\n","        bias = sse - variance\n","\n","        hamming_loss_value = HammingLoss(mode='multilabel')\n","        hamming_loss_value.update_state(y_test, y_test_predictions)\n","        \n","        if decay_steps_mode==\"on\":\n","            df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n","                                    'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n","                                    'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n","                                    'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n","                                    'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n","                                    'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n","                                    'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n","                                    'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n","                                    'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n","                                    'Zero_one Loss (perce)':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=True)], dtype='float'),\n","                                    'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n","                                    'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n","                                    'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n","                                    'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n","                                    'Bias':pd.Series([bias], dtype='float'),\n","                                    'Variance':pd.Series([variance], dtype='float')\n","                                   })\n","\n","            # df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n","            #                                                                                                                      saved_df_scored_metric_name,\n","            #                                                                                                                      str(hparams[HP_EMBEDDING_DIM]),\n","            #                                                                                                                      str(hparams[HP_HIDDEN_UNITS]),\n","            #                                                                                                                      str(hparams[HP_LEARNING_RATE]),\n","            #                                                                                                                      str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","            #                                                                                                                      version_data_control)))\n","        else:\n","            df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n","                                    'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n","                                    'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n","                                    'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n","                                    'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n","                                    'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n","                                    'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n","                                    'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n","                                    'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n","                                    'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n","                                    'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n","                                    'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n","                                    'Bias':pd.Series([bias], dtype='float'),\n","                                    'Variance':pd.Series([variance], dtype='float')\n","                                   })\n","\n","            df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n","                                                                                                              saved_df_scored_metric_name,\n","                                                                                                              str(hparams[HP_EMBEDDING_DIM]),\n","                                                                                                              str(hparams[HP_HIDDEN_UNITS]),\n","                                                                                                              str(hparams[HP_LEARNING_RATE]),\n","                                                                                                              version_data_control)))\n","    else:\n","        model_evaluation = model.evaluate([test_bytes_list_plot, test_bytes_list_features, test_bytes_list_reviews, test_bytes_list_title],\n","                                          test_label,\n","                                          batch_size=hparams[HP_HIDDEN_UNITS],\n","                                          verbose=2)\n","\n","        y_test_pred_probs = model.predict([test_bytes_list_plot, test_bytes_list_features, test_bytes_list_reviews, test_bytes_list_title])\n","        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n","\n","        variance = np.var(y_test_predictions)\n","        sse = np.mean((np.mean(y_test_predictions) - test_label)**2)\n","        bias = sse - variance\n","\n","        hamming_loss_value = HammingLoss(mode='multilabel')\n","        hamming_loss_value.update_state(test_label, y_test_predictions)\n","\n","        df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n","                                'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n","                                'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n","                                'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n","                                'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n","                                'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n","                                'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n","                                'Zero_one Loss':pd.Series([zero_one_loss(test_label, y_test_predictions, normalize=False)], dtype='float'),\n","                                'F1_score':pd.Series([f1_score(test_label, y_test_predictions, average=\"micro\")], dtype='float'),\n","                                'F1_score_samples':pd.Series([f1_score(test_label, y_test_predictions, average=\"samples\")], dtype='float'),\n","                                'ROC_score':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n","                                'ROC_score_samples':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n","                                'Bias':pd.Series([bias], dtype='float'),\n","                                'Variance':pd.Series([variance], dtype='float')\n","                               })\n","\n","        df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n","                                                                                                                             saved_df_scored_metric_name,\n","                                                                                                                             str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                                             str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                                             str(hparams[HP_LEARNING_RATE]), \n","                                                                                                                             str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","                                                                                                                             version_data_control)))\n","    return df_scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qwkp0NFU7B7a"},"source":["**Load the models per Optimizer & Create a scoring dataframe for each model**\n","\n","Step 1 of the selection plan (based on the written thesis documentation)"]},{"cell_type":"code","metadata":{"id":"OVtR3U3yuj_z"},"source":["# Attention layer mechanism (Approach 1 referenced here: https://stackoverflow.com/a/62949137/10623444)\r\n","class attention(tf.keras.layers.Layer):\r\n","    \r\n","    def __init__(self, return_sequences=True, **kwargs):\r\n","        self.return_sequences = return_sequences\r\n","        super(attention,self).__init__(**kwargs)\r\n","\r\n","    def get_config(self):\r\n","\r\n","      config = super().get_config().copy()\r\n","      config.update({\r\n","          'return_sequences': self.return_sequences,\r\n","      })\r\n","      return config\r\n","        \r\n","    def build(self, input_shape):\r\n","        \r\n","        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\r\n","                               initializer=\"normal\")\r\n","        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\r\n","                               initializer=\"zeros\")\r\n","        \r\n","        super(attention,self).build(input_shape)\r\n","        \r\n","    def call(self, x):\r\n","        \r\n","        e = K.tanh(K.dot(x,self.W)+self.b)\r\n","        a = K.softmax(e, axis=1) #those are the attention weights. They should sum up to 1 since softmax was applied.\r\n","        output = x*a #input x multipled with the respective attention weights\r\n","        \r\n","        if self.return_sequences:\r\n","            return output\r\n","        \r\n","        return K.sum(output, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jgCLUSOEM47"},"source":["# Attention layer mechanism (Approach 2 referenced here: https://towardsdatascience.com/create-your-own-custom-attention-layer-understand-all-flavours-2201b5e8be9e)\r\n","from keras.layers import Flatten, Activation, RepeatVector, Permute, Multiply, Lambda\r\n","\r\n","class peel_the_layer(tf.keras.layers.Layer): \r\n","    def __init__(self, **kwargs):    \r\n","        # Nothing special to be done here\r\n","        super(peel_the_layer, self).__init__(**kwargs)\r\n","        \r\n","    def build(self, input_shape):\r\n","        # Define the shape of the weights and bias in this layer\r\n","        # As we discussed the layer has just 1 lonely neuron\r\n","        # We discussed the shapes of the weights and bias earlier\r\n","        num_units = 1 #This is a 1 unit layer\r\n","        self.num_dim_perword = input_shape[-1]\r\n","        self.words_perplot = input_shape[-2]\r\n","\r\n","        self.w=self.add_weight(name=\"att_weight\", shape=(self.num_dim_perword,num_units), initializer=\"normal\")\r\n","        self.b=self.add_weight(name=\"att_bias\", shape=(self.words_perplot,num_units), initializer=\"zeros\") #22 are the single words per plot summary\r\n","        super(peel_the_layer, self).build(input_shape)\r\n","        \r\n","    def call(self, x):\r\n","        # x is the input tensor of 100 dimensions. 100 Is the embedding dim\r\n","        # Below is the main processing done during training\r\n","        # K is the Keras Backend import\r\n","        e = K.tanh(K.dot(x,self.w)+self.b)\r\n","        e = Flatten()(e) #flatten = K.squeeze(). Both they reduce size of a tensor\r\n","\r\n","        a = Activation('softmax')(e)\r\n","        \r\n","        # Don't manipulate 'a'. It needs to be 'returned' intact\r\n","        temp = RepeatVector(self.num_dim_perword)(a)\r\n","        temp = Permute([2,1])(temp)\r\n","    \r\n","        output = Multiply()([x,temp])\r\n","        output = Lambda(lambda values: K.sum(values, axis=1))(output)\r\n","        \r\n","        #a = K.softmax(e, axis=1)\r\n","        #output = x*a\r\n","        \r\n","        # return the outputs. 'a' is the set of 19 attention weights\r\n","        # the second variable is the 'attention adjusted o/p state'\r\n","        return a, output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qUuKY0edQDz"},"source":["# Attention layer mechanism (Approach 3 referenced here: https://medium.com/analytics-vidhya/attention-mechanism-a-quick-intuition-26e154cdb49a)\r\n","class Attention(tf.keras.layers.Layer):\r\n","    def __init__(self, **kwargs):\r\n","        super(Attention,self).__init__(**kwargs)\r\n","\r\n","    def build(self,input_shape): \r\n","        \"\"\"\r\n","        Matrices for creating the context vector.\r\n","        \"\"\"\r\n","        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\r\n","        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \r\n","        super(Attention, self).build(input_shape)\r\n","\r\n","    def call(self,x):\r\n","        \"\"\"\r\n","        Function which does the computation and is passed through a softmax layer to calculate the attention probabilities and context vector. \r\n","        \"\"\"\r\n","        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\r\n","        at=K.softmax(et)\r\n","        at=K.expand_dims(at,axis=-1)\r\n","        output=x*at\r\n","        return K.sum(output,axis=1)\r\n","\r\n","    def compute_output_shape(self,input_shape):\r\n","        \"\"\"\r\n","        For Keras internal compatibility checking.\r\n","        \"\"\"\r\n","        return (input_shape[0],input_shape[-1])\r\n","\r\n","    def get_config(self):\r\n","        \"\"\"\r\n","        The get_config() method collects the input shape and other information about the model.\r\n","        \"\"\"\r\n","        return super(Attention,self).get_config()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fSuLlY-7B7a"},"source":["model_method_creation=\"adam\"\n","# First run the 36 models with batch 32, 64 and then the other 18 model of 128 batch size if your Ram is 16GB"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tG0gsha77B7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609752555702,"user_tz":-120,"elapsed":803892,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"ca1060d8-a1fa-45c3-94c0-4270392edfe7"},"source":["# 18 models of 32-batch (approach 1)\n","\n","list_models=[]\n","list_df=[]\n","\n","if model_method_creation==\"adam\":\n","\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([int(initialize_notebook_variables.batch_size_value.replace(\"batch\",\"\"))]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50,100,150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001,0.01]))\n","    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10,20]))\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n","                    hparams = {\n","                        HP_HIDDEN_UNITS: batch_size,\n","                        HP_EMBEDDING_DIM: embedding_dim,\n","                        HP_LEARNING_RATE: learning_rate,\n","                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n","                      }\n","                    print(f\"\\n{len(list_models)+1}/{(len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))}\")\n","                    print({h.name: hparams[h] for h in hparams},'\\n')\n","                    model_object=import_trained_keras_model(len(list_models)+1, \"import custom trained model\", \"on\", model_method_creation, hparams)\n","                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(initialize_notebook_variables.saved_model_name, len(list_models)+1), hparams, model_object)\n","                    list_models.append(model_object)\n","                    list_df.append(df_object)\n","\n","else:\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                hparams = {\n","                    HP_HIDDEN_UNITS: batch_size,\n","                    HP_EMBEDDING_DIM: embedding_dim,\n","                    HP_LEARNING_RATE: learning_rate\n","                  }\n","                print(\"\\n{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n","                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n","                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n","                list_models.append(model_object)\n","                list_df.append(df_object)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","1/12\n","{'batch_size': 32, 'embedding_dim': 50, 'learning_rate': 0.001, 'decay_steps_multiplier': 10} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.001lr_10decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.001lr_10decaymultiplier_20210102.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.001lr_10decaymultiplier_20210102.json\n","Model 1 is loaded successfully\n","\n","307/307 - 24s - loss: 0.0271 - hamming_loss: 0.0489 - f1_score_micro: 0.7055 - f1_score_none: 0.4762 - f1_score_macro: 0.4762\n","\n","2/12\n","{'batch_size': 32, 'embedding_dim': 50, 'learning_rate': 0.001, 'decay_steps_multiplier': 20} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.001lr_20decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.001lr_20decaymultiplier_20210102.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.001lr_20decaymultiplier_20210102.json\n","Model 2 is loaded successfully\n","\n","307/307 - 25s - loss: 0.0290 - hamming_loss: 0.0489 - f1_score_micro: 0.7054 - f1_score_none: 0.4818 - f1_score_macro: 0.4818\n","\n","3/12\n","{'batch_size': 32, 'embedding_dim': 50, 'learning_rate': 0.01, 'decay_steps_multiplier': 10} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.01lr_10decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.01lr_10decaymultiplier_20210102.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.01lr_10decaymultiplier_20210102.json\n","Model 3 is loaded successfully\n","\n","307/307 - 27s - loss: 0.0428 - hamming_loss: 0.0490 - f1_score_micro: 0.7046 - f1_score_none: 0.4801 - f1_score_macro: 0.4801\n","\n","4/12\n","{'batch_size': 32, 'embedding_dim': 50, 'learning_rate': 0.01, 'decay_steps_multiplier': 20} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.01lr_20decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.01lr_20decaymultiplier_20210102.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_50dim_32batchsize_0.01lr_20decaymultiplier_20210102.json\n","Model 4 is loaded successfully\n","\n","307/307 - 24s - loss: 0.0408 - hamming_loss: 0.0490 - f1_score_micro: 0.7044 - f1_score_none: 0.4706 - f1_score_macro: 0.4706\n","\n","5/12\n","{'batch_size': 32, 'embedding_dim': 100, 'learning_rate': 0.001, 'decay_steps_multiplier': 10} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.001lr_10decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.001lr_10decaymultiplier_20210103.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.001lr_10decaymultiplier_20210103.json\n","Model 5 is loaded successfully\n","\n","307/307 - 26s - loss: 0.0271 - hamming_loss: 0.0490 - f1_score_micro: 0.7048 - f1_score_none: 0.4692 - f1_score_macro: 0.4692\n","\n","6/12\n","{'batch_size': 32, 'embedding_dim': 100, 'learning_rate': 0.001, 'decay_steps_multiplier': 20} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.001lr_20decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.001lr_20decaymultiplier_20210103.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.001lr_20decaymultiplier_20210103.json\n","Model 6 is loaded successfully\n","\n","307/307 - 26s - loss: 0.0219 - hamming_loss: 0.0489 - f1_score_micro: 0.7051 - f1_score_none: 0.4735 - f1_score_macro: 0.4735\n","\n","7/12\n","{'batch_size': 32, 'embedding_dim': 100, 'learning_rate': 0.01, 'decay_steps_multiplier': 10} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.01lr_10decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.01lr_10decaymultiplier_20210103.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.01lr_10decaymultiplier_20210103.json\n","Model 7 is loaded successfully\n","\n","307/307 - 26s - loss: 0.0371 - hamming_loss: 0.0490 - f1_score_micro: 0.7049 - f1_score_none: 0.4805 - f1_score_macro: 0.4805\n","\n","8/12\n","{'batch_size': 32, 'embedding_dim': 100, 'learning_rate': 0.01, 'decay_steps_multiplier': 20} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.01lr_20decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.01lr_20decaymultiplier_20210103.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_100dim_32batchsize_0.01lr_20decaymultiplier_20210103.json\n","Model 8 is loaded successfully\n","\n","307/307 - 27s - loss: 0.0525 - hamming_loss: 0.0490 - f1_score_micro: 0.7046 - f1_score_none: 0.4816 - f1_score_macro: 0.4816\n","\n","9/12\n","{'batch_size': 32, 'embedding_dim': 150, 'learning_rate': 0.001, 'decay_steps_multiplier': 10} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.001lr_10decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.001lr_10decaymultiplier_20210103.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.001lr_10decaymultiplier_20210103.json\n","Model 9 is loaded successfully\n","\n","307/307 - 28s - loss: 0.0208 - hamming_loss: 0.0490 - f1_score_micro: 0.7048 - f1_score_none: 0.4809 - f1_score_macro: 0.4809\n","\n","10/12\n","{'batch_size': 32, 'embedding_dim': 150, 'learning_rate': 0.001, 'decay_steps_multiplier': 20} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.001lr_20decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.001lr_20decaymultiplier_20210103.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.001lr_20decaymultiplier_20210103.json\n","Model 10 is loaded successfully\n","\n","307/307 - 29s - loss: 0.0210 - hamming_loss: 0.0490 - f1_score_micro: 0.7050 - f1_score_none: 0.4807 - f1_score_macro: 0.4807\n","\n","11/12\n","{'batch_size': 32, 'embedding_dim': 150, 'learning_rate': 0.01, 'decay_steps_multiplier': 10} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.01lr_10decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.01lr_10decaymultiplier_20210104.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.01lr_10decaymultiplier_20210104.json\n","Model 11 is loaded successfully\n","\n","307/307 - 29s - loss: 0.0468 - hamming_loss: 0.0490 - f1_score_micro: 0.7046 - f1_score_none: 0.4926 - f1_score_macro: 0.4926\n","\n","12/12\n","{'batch_size': 32, 'embedding_dim': 150, 'learning_rate': 0.01, 'decay_steps_multiplier': 20} \n","\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.01lr_20decaymultiplier\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.01lr_20decaymultiplier_20210104.h5\n","/content/drive/MyDrive/AttentionLayer/attention_layer_approach1_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout/classification_attention_layer_model_150dim_32batchsize_0.01lr_20decaymultiplier_20210104.json\n","Model 12 is loaded successfully\n","\n","307/307 - 28s - loss: 0.0501 - hamming_loss: 0.0491 - f1_score_micro: 0.7043 - f1_score_none: 0.4968 - f1_score_macro: 0.4968\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"id":"OmySKizTwg14","executionInfo":{"status":"ok","timestamp":1609753130967,"user_tz":-120,"elapsed":631,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"53503b94-eab5-4d96-f440-1669446bb071"},"source":["#32 batch: Inverse time decay 0.0 dropout\r\n","model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve = list_models\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve = list_df\r\n","\r\n","frames_adam_64batch_inverse_decay=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight,\r\n","                                   df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve]\r\n","\r\n","frames_adam_64batch_inverse_decay=pd.concat(frames_adam_64batch_inverse_decay)\r\n","frames_adam_64batch_inverse_decay=frames_adam_64batch_inverse_decay.reset_index(drop=True)\r\n","frames_adam_64batch_inverse_decay=frames_adam_64batch_inverse_decay.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","frames_adam_64batch_inverse_decay.index += 1\r\n","frames_adam_64batch_inverse_decay.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}_{initialize_notebook_variables.dropout_rate}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}'}.pkl\")\r\n","frames_adam_64batch_inverse_decay # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10</th>\n","      <td>classification_attention_layer_model-10</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.020950</td>\n","      <td>0.048957</td>\n","      <td>0.004256</td>\n","      <td>660.0</td>\n","      <td>0.067354</td>\n","      <td>0.980309</td>\n","      <td>0.981292</td>\n","      <td>0.992808</td>\n","      <td>0.994352</td>\n","      <td>-0.001497</td>\n","      <td>0.097145</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>classification_attention_layer_model-9</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.020753</td>\n","      <td>0.048983</td>\n","      <td>0.004412</td>\n","      <td>692.0</td>\n","      <td>0.070619</td>\n","      <td>0.979573</td>\n","      <td>0.980408</td>\n","      <td>0.992080</td>\n","      <td>0.993824</td>\n","      <td>-0.001376</td>\n","      <td>0.097023</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>100</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.021906</td>\n","      <td>0.048943</td>\n","      <td>0.004442</td>\n","      <td>696.0</td>\n","      <td>0.071028</td>\n","      <td>0.979430</td>\n","      <td>0.980479</td>\n","      <td>0.991915</td>\n","      <td>0.993927</td>\n","      <td>-0.001343</td>\n","      <td>0.096990</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>100</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.027083</td>\n","      <td>0.048985</td>\n","      <td>0.004700</td>\n","      <td>731.0</td>\n","      <td>0.074599</td>\n","      <td>0.978182</td>\n","      <td>0.979772</td>\n","      <td>0.990167</td>\n","      <td>0.992954</td>\n","      <td>-0.000936</td>\n","      <td>0.096581</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>classification_attention_layer_model-7</td>\n","      <td>100</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.037120</td>\n","      <td>0.048971</td>\n","      <td>0.005583</td>\n","      <td>860.0</td>\n","      <td>0.087764</td>\n","      <td>0.974154</td>\n","      <td>0.976532</td>\n","      <td>0.989032</td>\n","      <td>0.992172</td>\n","      <td>-0.001381</td>\n","      <td>0.097027</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.027090</td>\n","      <td>0.048871</td>\n","      <td>0.005595</td>\n","      <td>866.0</td>\n","      <td>0.088376</td>\n","      <td>0.973914</td>\n","      <td>0.974973</td>\n","      <td>0.985869</td>\n","      <td>0.989040</td>\n","      <td>-0.000189</td>\n","      <td>0.095832</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.028972</td>\n","      <td>0.048883</td>\n","      <td>0.005835</td>\n","      <td>906.0</td>\n","      <td>0.092458</td>\n","      <td>0.972626</td>\n","      <td>0.973334</td>\n","      <td>0.982529</td>\n","      <td>0.986415</td>\n","      <td>0.000851</td>\n","      <td>0.094793</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>classification_attention_layer_model-11</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.046846</td>\n","      <td>0.049027</td>\n","      <td>0.006375</td>\n","      <td>956.0</td>\n","      <td>0.097561</td>\n","      <td>0.970264</td>\n","      <td>0.971777</td>\n","      <td>0.983657</td>\n","      <td>0.987610</td>\n","      <td>-0.000123</td>\n","      <td>0.095766</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.040771</td>\n","      <td>0.049049</td>\n","      <td>0.006471</td>\n","      <td>1001.0</td>\n","      <td>0.102153</td>\n","      <td>0.969601</td>\n","      <td>0.970006</td>\n","      <td>0.980299</td>\n","      <td>0.984487</td>\n","      <td>0.001070</td>\n","      <td>0.094576</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>classification_attention_layer_model-12</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.050094</td>\n","      <td>0.049072</td>\n","      <td>0.006945</td>\n","      <td>1037.0</td>\n","      <td>0.105827</td>\n","      <td>0.967563</td>\n","      <td>0.970136</td>\n","      <td>0.981611</td>\n","      <td>0.986644</td>\n","      <td>0.000090</td>\n","      <td>0.095554</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>classification_attention_layer_model-8</td>\n","      <td>100</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.052525</td>\n","      <td>0.049014</td>\n","      <td>0.006957</td>\n","      <td>1054.0</td>\n","      <td>0.107562</td>\n","      <td>0.967576</td>\n","      <td>0.969621</td>\n","      <td>0.982517</td>\n","      <td>0.986370</td>\n","      <td>-0.000269</td>\n","      <td>0.095912</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.042798</td>\n","      <td>0.049013</td>\n","      <td>0.008290</td>\n","      <td>1197.0</td>\n","      <td>0.122155</td>\n","      <td>0.961242</td>\n","      <td>0.962710</td>\n","      <td>0.977628</td>\n","      <td>0.982382</td>\n","      <td>0.000269</td>\n","      <td>0.095375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   Tag Name  ...  Variance\n","10  classification_attention_layer_model-10  ...  0.097145\n","9    classification_attention_layer_model-9  ...  0.097023\n","6    classification_attention_layer_model-6  ...  0.096990\n","5    classification_attention_layer_model-5  ...  0.096581\n","7    classification_attention_layer_model-7  ...  0.097027\n","1    classification_attention_layer_model-1  ...  0.095832\n","2    classification_attention_layer_model-2  ...  0.094793\n","11  classification_attention_layer_model-11  ...  0.095766\n","4    classification_attention_layer_model-4  ...  0.094576\n","12  classification_attention_layer_model-12  ...  0.095554\n","8    classification_attention_layer_model-8  ...  0.095912\n","3    classification_attention_layer_model-3  ...  0.095375\n","\n","[12 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"DBIuHqtI79gW"},"source":["# Read the table results of the second round trained models\r\n","class initialize_notebook_variables():\r\n","    saved_word_tokenizers=\"13072020\" #the date I saved the word tokenizers for each of my five inputs\r\n","    tokenization_history_folder=\"text_tokenization_padded_sequences\" #the Drive folder were tokenizers & x,y are saved\r\n","    batch_size_value=\"64batch\" #the batch size version of the model fit()\r\n","    labelsmoothing_value=\"nolabelsmoothing\" #if label smoothing is applied\r\n","    approach_type=\"approach3\" #approach implementation of Attention layer\r\n","    saved_model_name=\"classification_attention_layer_model\"\r\n","    # learning_rate_scheduler = \"piecewise_constant_decay_shorter_boundary\"\r\n","    # learning_rate_scheduler = \"piecewise_constant_decay\"\r\n","    # learning_rate_scheduler = \"piecewise_constant_decay_even_shorter_boundary\"\r\n","    learning_rate_scheduler=\"inverse_time_decay\"\r\n","    dropout_rate=\"0.0dropout\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dwh03aHu8CF9","executionInfo":{"status":"ok","timestamp":1609755437239,"user_tz":-120,"elapsed":996,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"810cf6ef-f1c4-4102-9314-90b9e46a5740"},"source":["#table 1\r\n","table_1 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_64batch_nolabelsmoothing_inverse_time_decay.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5qvYiSF9acy","executionInfo":{"status":"ok","timestamp":1609755491482,"user_tz":-120,"elapsed":638,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"9a3ece0f-b83c-4459-bc92-4b36d3cb293b"},"source":["#table 2\r\n","table_2 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_64batch_nolabelsmoothing_piecewise_constant_decay_even_shorter_boundary.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1t4DAJxz9aaS","executionInfo":{"status":"ok","timestamp":1609755536778,"user_tz":-120,"elapsed":630,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"29aec0a6-9c58-46c1-cc4d-4ec42a4322ee"},"source":["#table 3\r\n","table_3 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_64batch_nolabelsmoothing_piecewise_constant_decay_shorter_boundary.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZDflI2C9aW4","executionInfo":{"status":"ok","timestamp":1609755569910,"user_tz":-120,"elapsed":691,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"e55bfb0e-7903-4654-ca5a-d7d2db39b8f8"},"source":["#table 4\r\n","table_4 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_64batch_nolabelsmoothing_piecewise_constant_decay.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFxeGXI09aUV","executionInfo":{"status":"ok","timestamp":1609755612879,"user_tz":-120,"elapsed":649,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"083fbd12-e17b-4ca3-920a-2e771dfc8068"},"source":["#table 5\r\n","table_5 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_128batch_nolabelsmoothing_inverse_time_decay.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-CQopbVo9aRu","executionInfo":{"status":"ok","timestamp":1609755700222,"user_tz":-120,"elapsed":610,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"fdd9f20e-ccbe-476d-8ee9-a4404435852a"},"source":["#table 6\r\n","table_6 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_128batch_nolabelsmoothing_piecewise_constant_decay_even_shorter_boundary.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqY90Tgd9aPN","executionInfo":{"status":"ok","timestamp":1609755727920,"user_tz":-120,"elapsed":631,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"b71502e8-c11e-4f80-dc96-94453baa5ecb"},"source":["#table 7\r\n","table_7 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_128batch_nolabelsmoothing_piecewise_constant_decay.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NhbKhCsM9aMH","executionInfo":{"status":"ok","timestamp":1609755836510,"user_tz":-120,"elapsed":662,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"27c9995d-dc9e-489a-ae65-0449a542a39b"},"source":["#table 8\r\n","table_8 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_approach3_32batch_nolabelsmoothing.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FM3-ZqkV-lEl","executionInfo":{"status":"ok","timestamp":1609755862170,"user_tz":-120,"elapsed":653,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"53460fe4-d1aa-4892-93f5-add1e583c5c4"},"source":["#table 9\r\n","table_9 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_approach3_64batch_nolabelsmoothing.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ETVZCFAi-lB_","executionInfo":{"status":"ok","timestamp":1609755873705,"user_tz":-120,"elapsed":626,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"0a1080db-c6c4-4210-a24d-6100a2a74673"},"source":["#table 10\r\n","table_10 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_approach3_128batch_nolabelsmoothing.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dP33Mr9-k-4","executionInfo":{"status":"ok","timestamp":1609755928281,"user_tz":-120,"elapsed":631,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"8613dcaf-3832-4495-e3be-627d4f53920d"},"source":["#table 11\r\n","table_11 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}_{initialize_notebook_variables.dropout_rate}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}_{initialize_notebook_variables.dropout_rate}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_32batch_nolabelsmoothing_inverse_time_decay_0.0dropout.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-U23WQTzx0v","executionInfo":{"status":"ok","timestamp":1609755955335,"user_tz":-120,"elapsed":743,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"8c6c4776-4d73-47b2-ebfe-28b7946727f3"},"source":["#table 12\r\n","table_12 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}_{initialize_notebook_variables.dropout_rate}.pkl\")\r\n","print(f\"result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}_{initialize_notebook_variables.dropout_rate}.pkl\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["result_adam_64batch_nolabelsmoothing_inverse_time_decay_0.0dropout.pkl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aj9SpWUO_4vT"},"source":["table_1_top = table_1.head(1)\r\n","table_2_top = table_2.head(1)\r\n","table_3_top = table_3.head(1)\r\n","table_4_top = table_4.head(1)\r\n","table_5_top = table_5.head(1)\r\n","table_6_top = table_6.head(1)\r\n","table_7_top = table_7.head(1)\r\n","table_8_top = table_8.head(1)\r\n","table_9_top = table_9.head(1)\r\n","table_10_top = table_10.head(1)\r\n","table_11_top = table_11.head(1)\r\n","table_12_top = table_12.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"id":"mad0m9ZW_4si","executionInfo":{"status":"ok","timestamp":1609756062779,"user_tz":-120,"elapsed":626,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"0842c0be-09b0-41a7-b136-40330979c730"},"source":["table_final = pd.concat([table_1_top, table_2_top, table_3_top, table_4_top, table_5_top, table_6_top, table_7_top, table_8_top, table_9_top, table_10_top, table_11_top, table_12_top])\r\n","table_final.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results_second_round/final_scores_table.pkl\")\r\n","table_final"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>11</th>\n","      <td>classification_attention_layer_model-11</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.035385</td>\n","      <td>0.048975</td>\n","      <td>0.004346</td>\n","      <td>676.0</td>\n","      <td>0.068987</td>\n","      <td>0.979903</td>\n","      <td>0.980988</td>\n","      <td>0.992807</td>\n","      <td>0.994451</td>\n","      <td>-0.001586</td>\n","      <td>0.097234</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.022153</td>\n","      <td>0.048798</td>\n","      <td>0.004064</td>\n","      <td>633.0</td>\n","      <td>0.064598</td>\n","      <td>0.981186</td>\n","      <td>0.982155</td>\n","      <td>0.993014</td>\n","      <td>0.994538</td>\n","      <td>-0.001385</td>\n","      <td>0.097032</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.025574</td>\n","      <td>0.049182</td>\n","      <td>0.004688</td>\n","      <td>718.0</td>\n","      <td>0.073273</td>\n","      <td>0.978198</td>\n","      <td>0.979002</td>\n","      <td>0.989410</td>\n","      <td>0.992017</td>\n","      <td>-0.000635</td>\n","      <td>0.096280</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.027518</td>\n","      <td>0.048844</td>\n","      <td>0.004718</td>\n","      <td>741.0</td>\n","      <td>0.075620</td>\n","      <td>0.978069</td>\n","      <td>0.978392</td>\n","      <td>0.989541</td>\n","      <td>0.991853</td>\n","      <td>-0.000715</td>\n","      <td>0.096360</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>classification_attention_layer_model-11</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.035431</td>\n","      <td>0.049031</td>\n","      <td>0.004989</td>\n","      <td>774.0</td>\n","      <td>0.078988</td>\n","      <td>0.976912</td>\n","      <td>0.978427</td>\n","      <td>0.990721</td>\n","      <td>0.992814</td>\n","      <td>-0.001432</td>\n","      <td>0.097079</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.025842</td>\n","      <td>0.048856</td>\n","      <td>0.004802</td>\n","      <td>757.0</td>\n","      <td>0.077253</td>\n","      <td>0.977604</td>\n","      <td>0.977969</td>\n","      <td>0.987842</td>\n","      <td>0.990298</td>\n","      <td>-0.000151</td>\n","      <td>0.095794</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.029332</td>\n","      <td>0.048898</td>\n","      <td>0.004304</td>\n","      <td>677.0</td>\n","      <td>0.069089</td>\n","      <td>0.980065</td>\n","      <td>0.980500</td>\n","      <td>0.992189</td>\n","      <td>0.993830</td>\n","      <td>-0.001310</td>\n","      <td>0.096957</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.032109</td>\n","      <td>0.048942</td>\n","      <td>0.005481</td>\n","      <td>815.0</td>\n","      <td>0.083172</td>\n","      <td>0.974233</td>\n","      <td>0.974815</td>\n","      <td>0.982530</td>\n","      <td>0.986674</td>\n","      <td>0.001208</td>\n","      <td>0.094438</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.035318</td>\n","      <td>0.048866</td>\n","      <td>0.004700</td>\n","      <td>733.0</td>\n","      <td>0.074804</td>\n","      <td>0.978110</td>\n","      <td>0.979336</td>\n","      <td>0.988713</td>\n","      <td>0.991480</td>\n","      <td>-0.000382</td>\n","      <td>0.096025</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.037176</td>\n","      <td>0.049066</td>\n","      <td>0.005505</td>\n","      <td>821.0</td>\n","      <td>0.083784</td>\n","      <td>0.974212</td>\n","      <td>0.974322</td>\n","      <td>0.984021</td>\n","      <td>0.987076</td>\n","      <td>0.000610</td>\n","      <td>0.095034</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>classification_attention_layer_model-10</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.020950</td>\n","      <td>0.048957</td>\n","      <td>0.004256</td>\n","      <td>660.0</td>\n","      <td>0.067354</td>\n","      <td>0.980309</td>\n","      <td>0.981292</td>\n","      <td>0.992808</td>\n","      <td>0.994352</td>\n","      <td>-0.001497</td>\n","      <td>0.097145</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.021501</td>\n","      <td>0.048988</td>\n","      <td>0.004286</td>\n","      <td>673.0</td>\n","      <td>0.068680</td>\n","      <td>0.980105</td>\n","      <td>0.981187</td>\n","      <td>0.991262</td>\n","      <td>0.993406</td>\n","      <td>-0.000941</td>\n","      <td>0.096586</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   Tag Name  ...  Variance\n","11  classification_attention_layer_model-11  ...  0.097234\n","3    classification_attention_layer_model-3  ...  0.097032\n","6    classification_attention_layer_model-6  ...  0.096280\n","5    classification_attention_layer_model-5  ...  0.096360\n","11  classification_attention_layer_model-11  ...  0.097079\n","5    classification_attention_layer_model-5  ...  0.095794\n","6    classification_attention_layer_model-6  ...  0.096957\n","3    classification_attention_layer_model-3  ...  0.094438\n","2    classification_attention_layer_model-2  ...  0.096025\n","2    classification_attention_layer_model-2  ...  0.095034\n","10  classification_attention_layer_model-10  ...  0.097145\n","5    classification_attention_layer_model-5  ...  0.096586\n","\n","[12 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"id":"2gs7yiIsrKBK","executionInfo":{"status":"ok","timestamp":1609689511747,"user_tz":-120,"elapsed":616,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"d16baae5-dac6-4cf2-e0d5-577bd629fc72"},"source":["#64 batch: Inverse time decay\r\n","model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve = list_models\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve = list_df\r\n","\r\n","frames_adam_64batch_inverse_decay=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight,\r\n","                                   df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve]\r\n","\r\n","frames_adam_64batch_inverse_decay=pd.concat(frames_adam_64batch_inverse_decay)\r\n","frames_adam_64batch_inverse_decay=frames_adam_64batch_inverse_decay.reset_index(drop=True)\r\n","frames_adam_64batch_inverse_decay=frames_adam_64batch_inverse_decay.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","frames_adam_64batch_inverse_decay.index += 1\r\n","frames_adam_64batch_inverse_decay.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}_{initialize_notebook_variables.dropout_rate}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}'}.pkl\")\r\n","frames_adam_64batch_inverse_decay # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.021501</td>\n","      <td>0.048988</td>\n","      <td>0.004286</td>\n","      <td>673.0</td>\n","      <td>0.068680</td>\n","      <td>0.980105</td>\n","      <td>0.981187</td>\n","      <td>0.991262</td>\n","      <td>0.993406</td>\n","      <td>-0.000941</td>\n","      <td>0.096586</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>classification_attention_layer_model-10</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.023315</td>\n","      <td>0.049054</td>\n","      <td>0.004610</td>\n","      <td>725.0</td>\n","      <td>0.073987</td>\n","      <td>0.978562</td>\n","      <td>0.979259</td>\n","      <td>0.989626</td>\n","      <td>0.991898</td>\n","      <td>-0.000640</td>\n","      <td>0.096284</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>classification_attention_layer_model-9</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.030323</td>\n","      <td>0.049083</td>\n","      <td>0.004832</td>\n","      <td>750.0</td>\n","      <td>0.076538</td>\n","      <td>0.977507</td>\n","      <td>0.977934</td>\n","      <td>0.988614</td>\n","      <td>0.990865</td>\n","      <td>-0.000476</td>\n","      <td>0.096120</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>classification_attention_layer_model-11</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.037596</td>\n","      <td>0.049040</td>\n","      <td>0.004940</td>\n","      <td>778.0</td>\n","      <td>0.079396</td>\n","      <td>0.977066</td>\n","      <td>0.978253</td>\n","      <td>0.989515</td>\n","      <td>0.991890</td>\n","      <td>-0.000926</td>\n","      <td>0.096571</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>classification_attention_layer_model-8</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.035931</td>\n","      <td>0.049123</td>\n","      <td>0.005121</td>\n","      <td>790.0</td>\n","      <td>0.080620</td>\n","      <td>0.976236</td>\n","      <td>0.977679</td>\n","      <td>0.989168</td>\n","      <td>0.991658</td>\n","      <td>-0.000973</td>\n","      <td>0.096618</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>classification_attention_layer_model-7</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.035722</td>\n","      <td>0.049095</td>\n","      <td>0.005187</td>\n","      <td>812.0</td>\n","      <td>0.082866</td>\n","      <td>0.975838</td>\n","      <td>0.975744</td>\n","      <td>0.987306</td>\n","      <td>0.989576</td>\n","      <td>-0.000330</td>\n","      <td>0.095974</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.027261</td>\n","      <td>0.048943</td>\n","      <td>0.005541</td>\n","      <td>857.0</td>\n","      <td>0.087458</td>\n","      <td>0.974091</td>\n","      <td>0.975228</td>\n","      <td>0.984740</td>\n","      <td>0.988361</td>\n","      <td>0.000297</td>\n","      <td>0.095346</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.037154</td>\n","      <td>0.048943</td>\n","      <td>0.005943</td>\n","      <td>893.0</td>\n","      <td>0.091132</td>\n","      <td>0.972437</td>\n","      <td>0.974135</td>\n","      <td>0.987302</td>\n","      <td>0.989831</td>\n","      <td>-0.001081</td>\n","      <td>0.096727</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.030402</td>\n","      <td>0.048859</td>\n","      <td>0.006189</td>\n","      <td>926.0</td>\n","      <td>0.094499</td>\n","      <td>0.970931</td>\n","      <td>0.972015</td>\n","      <td>0.981098</td>\n","      <td>0.985638</td>\n","      <td>0.001046</td>\n","      <td>0.094599</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.034929</td>\n","      <td>0.048835</td>\n","      <td>0.006441</td>\n","      <td>981.0</td>\n","      <td>0.100112</td>\n","      <td>0.969813</td>\n","      <td>0.971235</td>\n","      <td>0.981400</td>\n","      <td>0.985603</td>\n","      <td>0.000676</td>\n","      <td>0.094968</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>classification_attention_layer_model-12</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.042529</td>\n","      <td>0.049059</td>\n","      <td>0.006994</td>\n","      <td>1063.0</td>\n","      <td>0.108480</td>\n","      <td>0.967234</td>\n","      <td>0.967438</td>\n","      <td>0.980080</td>\n","      <td>0.984668</td>\n","      <td>0.000629</td>\n","      <td>0.095016</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.041634</td>\n","      <td>0.049051</td>\n","      <td>0.008512</td>\n","      <td>1203.0</td>\n","      <td>0.122768</td>\n","      <td>0.959898</td>\n","      <td>0.960287</td>\n","      <td>0.973706</td>\n","      <td>0.979215</td>\n","      <td>0.001555</td>\n","      <td>0.094093</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   Tag Name  ...  Variance\n","5    classification_attention_layer_model-5  ...  0.096586\n","10  classification_attention_layer_model-10  ...  0.096284\n","9    classification_attention_layer_model-9  ...  0.096120\n","11  classification_attention_layer_model-11  ...  0.096571\n","8    classification_attention_layer_model-8  ...  0.096618\n","7    classification_attention_layer_model-7  ...  0.095974\n","6    classification_attention_layer_model-6  ...  0.095346\n","3    classification_attention_layer_model-3  ...  0.096727\n","2    classification_attention_layer_model-2  ...  0.094599\n","1    classification_attention_layer_model-1  ...  0.094968\n","12  classification_attention_layer_model-12  ...  0.095016\n","4    classification_attention_layer_model-4  ...  0.094093\n","\n","[12 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"yhdnMktwrJ--","executionInfo":{"status":"ok","timestamp":1609608081210,"user_tz":-120,"elapsed":1270,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"de8afa8f-30e8-4d47-c1b1-5d861d2bf7eb"},"source":["#128 batch: Piecewise Constant Decay\r\n","model_one, model_two, model_three, model_four, model_five, model_six = list_models\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six = list_df\r\n","\r\n","frames_adam_64batch_piecewise_decay=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six]\r\n","\r\n","frames_adam_64batch_piecewise_decay=pd.concat(frames_adam_64batch_piecewise_decay)\r\n","frames_adam_64batch_piecewise_decay=frames_adam_64batch_piecewise_decay.reset_index(drop=True)\r\n","frames_adam_64batch_piecewise_decay=frames_adam_64batch_piecewise_decay.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","frames_adam_64batch_piecewise_decay.index += 1\r\n","frames_adam_64batch_piecewise_decay.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}'}.pkl\")\r\n","frames_adam_64batch_piecewise_decay # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.029332</td>\n","      <td>0.048898</td>\n","      <td>0.004304</td>\n","      <td>677.0</td>\n","      <td>0.069089</td>\n","      <td>0.980065</td>\n","      <td>0.980500</td>\n","      <td>0.992189</td>\n","      <td>0.993830</td>\n","      <td>-0.001310</td>\n","      <td>0.096957</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.024636</td>\n","      <td>0.048946</td>\n","      <td>0.004694</td>\n","      <td>720.0</td>\n","      <td>0.073477</td>\n","      <td>0.978116</td>\n","      <td>0.978680</td>\n","      <td>0.988297</td>\n","      <td>0.990962</td>\n","      <td>-0.000217</td>\n","      <td>0.095860</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.029661</td>\n","      <td>0.048977</td>\n","      <td>0.004724</td>\n","      <td>727.0</td>\n","      <td>0.074191</td>\n","      <td>0.978053</td>\n","      <td>0.978079</td>\n","      <td>0.989759</td>\n","      <td>0.991816</td>\n","      <td>-0.000804</td>\n","      <td>0.096449</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.026106</td>\n","      <td>0.049055</td>\n","      <td>0.004790</td>\n","      <td>753.0</td>\n","      <td>0.076845</td>\n","      <td>0.977666</td>\n","      <td>0.978128</td>\n","      <td>0.987996</td>\n","      <td>0.990478</td>\n","      <td>-0.000198</td>\n","      <td>0.095842</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.027842</td>\n","      <td>0.049185</td>\n","      <td>0.005007</td>\n","      <td>768.0</td>\n","      <td>0.078375</td>\n","      <td>0.976663</td>\n","      <td>0.977499</td>\n","      <td>0.987530</td>\n","      <td>0.990371</td>\n","      <td>-0.000236</td>\n","      <td>0.095879</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.048578</td>\n","      <td>0.049553</td>\n","      <td>0.012588</td>\n","      <td>1830.0</td>\n","      <td>0.186754</td>\n","      <td>0.940346</td>\n","      <td>0.943686</td>\n","      <td>0.960499</td>\n","      <td>0.969762</td>\n","      <td>0.002544</td>\n","      <td>0.093110</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","6  classification_attention_layer_model-6  ...  0.096957\n","5  classification_attention_layer_model-5  ...  0.095860\n","4  classification_attention_layer_model-4  ...  0.096449\n","3  classification_attention_layer_model-3  ...  0.095842\n","2  classification_attention_layer_model-2  ...  0.095879\n","1  classification_attention_layer_model-1  ...  0.093110\n","\n","[6 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"b2e3fWjovYlB","executionInfo":{"status":"ok","timestamp":1609459699546,"user_tz":-120,"elapsed":627,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"385d7902-af63-4d43-8847-b863933b9f65"},"source":["#128 batch: Piecewise Constant Decay Shorter Boundaries\r\n","model_one, model_two, model_three, model_four, model_five, model_six = list_models\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six = list_df\r\n","\r\n","frames_adam_64batch_piecewise_decay=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six]\r\n","\r\n","frames_adam_64batch_piecewise_decay=pd.concat(frames_adam_64batch_piecewise_decay)\r\n","frames_adam_64batch_piecewise_decay=frames_adam_64batch_piecewise_decay.reset_index(drop=True)\r\n","frames_adam_64batch_piecewise_decay=frames_adam_64batch_piecewise_decay.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","frames_adam_64batch_piecewise_decay.index += 1\r\n","frames_adam_64batch_piecewise_decay.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}'}.pkl\")\r\n","frames_adam_64batch_piecewise_decay # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.025574</td>\n","      <td>0.049182</td>\n","      <td>0.004688</td>\n","      <td>718.0</td>\n","      <td>0.073273</td>\n","      <td>0.978198</td>\n","      <td>0.979002</td>\n","      <td>0.989410</td>\n","      <td>0.992017</td>\n","      <td>-0.000635</td>\n","      <td>0.096280</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.025018</td>\n","      <td>0.049235</td>\n","      <td>0.004964</td>\n","      <td>768.0</td>\n","      <td>0.078375</td>\n","      <td>0.976833</td>\n","      <td>0.977876</td>\n","      <td>0.987134</td>\n","      <td>0.990286</td>\n","      <td>-0.000042</td>\n","      <td>0.095686</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.029001</td>\n","      <td>0.049410</td>\n","      <td>0.005049</td>\n","      <td>761.0</td>\n","      <td>0.077661</td>\n","      <td>0.976500</td>\n","      <td>0.977280</td>\n","      <td>0.988024</td>\n","      <td>0.990769</td>\n","      <td>-0.000466</td>\n","      <td>0.096110</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.027031</td>\n","      <td>0.049312</td>\n","      <td>0.005091</td>\n","      <td>793.0</td>\n","      <td>0.080927</td>\n","      <td>0.976209</td>\n","      <td>0.977186</td>\n","      <td>0.986151</td>\n","      <td>0.989290</td>\n","      <td>0.000208</td>\n","      <td>0.095436</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>10</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.067805</td>\n","      <td>0.049176</td>\n","      <td>0.019588</td>\n","      <td>2668.0</td>\n","      <td>0.272273</td>\n","      <td>0.904044</td>\n","      <td>0.909276</td>\n","      <td>0.928049</td>\n","      <td>0.943966</td>\n","      <td>0.008143</td>\n","      <td>0.087603</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>10</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.085481</td>\n","      <td>0.049543</td>\n","      <td>0.024720</td>\n","      <td>3296.0</td>\n","      <td>0.336361</td>\n","      <td>0.878086</td>\n","      <td>0.884955</td>\n","      <td>0.911835</td>\n","      <td>0.933059</td>\n","      <td>0.009272</td>\n","      <td>0.086503</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","6  classification_attention_layer_model-6  ...  0.096280\n","5  classification_attention_layer_model-5  ...  0.095686\n","3  classification_attention_layer_model-3  ...  0.096110\n","4  classification_attention_layer_model-4  ...  0.095436\n","1  classification_attention_layer_model-1  ...  0.087603\n","2  classification_attention_layer_model-2  ...  0.086503\n","\n","[6 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"8IqtvKjxvYgw","executionInfo":{"status":"ok","timestamp":1609609240483,"user_tz":-120,"elapsed":778,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"8255fe37-d070-4cc7-f286-d22a00287522"},"source":["#128 batch: Piecewise Constant Decay Even Shorter Boundaries\r\n","model_one, model_two, model_three, model_four, model_five, model_six = list_models\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six = list_df\r\n","\r\n","frames_adam_64batch_piecewise_decay=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six]\r\n","\r\n","frames_adam_64batch_piecewise_decay=pd.concat(frames_adam_64batch_piecewise_decay)\r\n","frames_adam_64batch_piecewise_decay=frames_adam_64batch_piecewise_decay.reset_index(drop=True)\r\n","frames_adam_64batch_piecewise_decay=frames_adam_64batch_piecewise_decay.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","frames_adam_64batch_piecewise_decay.index += 1\r\n","frames_adam_64batch_piecewise_decay.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}_{initialize_notebook_variables.learning_rate_scheduler}'}.pkl\")\r\n","frames_adam_64batch_piecewise_decay # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.025842</td>\n","      <td>0.048856</td>\n","      <td>0.004802</td>\n","      <td>757.0</td>\n","      <td>0.077253</td>\n","      <td>0.977604</td>\n","      <td>0.977969</td>\n","      <td>0.987842</td>\n","      <td>0.990298</td>\n","      <td>-0.000151</td>\n","      <td>0.095794</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.027747</td>\n","      <td>0.048850</td>\n","      <td>0.004838</td>\n","      <td>771.0</td>\n","      <td>0.078681</td>\n","      <td>0.977410</td>\n","      <td>0.977453</td>\n","      <td>0.987254</td>\n","      <td>0.989802</td>\n","      <td>0.000038</td>\n","      <td>0.095606</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.031686</td>\n","      <td>0.048879</td>\n","      <td>0.005319</td>\n","      <td>821.0</td>\n","      <td>0.083784</td>\n","      <td>0.975192</td>\n","      <td>0.975514</td>\n","      <td>0.986418</td>\n","      <td>0.989051</td>\n","      <td>-0.000123</td>\n","      <td>0.095766</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.031276</td>\n","      <td>0.048868</td>\n","      <td>0.005361</td>\n","      <td>836.0</td>\n","      <td>0.085315</td>\n","      <td>0.974931</td>\n","      <td>0.975079</td>\n","      <td>0.985162</td>\n","      <td>0.988295</td>\n","      <td>0.000316</td>\n","      <td>0.095327</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.029254</td>\n","      <td>0.048919</td>\n","      <td>0.005853</td>\n","      <td>862.0</td>\n","      <td>0.087968</td>\n","      <td>0.972659</td>\n","      <td>0.974333</td>\n","      <td>0.984368</td>\n","      <td>0.988179</td>\n","      <td>0.000127</td>\n","      <td>0.095516</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.027887</td>\n","      <td>0.048859</td>\n","      <td>0.006393</td>\n","      <td>971.0</td>\n","      <td>0.099092</td>\n","      <td>0.969952</td>\n","      <td>0.971636</td>\n","      <td>0.980268</td>\n","      <td>0.984967</td>\n","      <td>0.001160</td>\n","      <td>0.094486</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","5  classification_attention_layer_model-5  ...  0.095794\n","6  classification_attention_layer_model-6  ...  0.095606\n","3  classification_attention_layer_model-3  ...  0.095766\n","4  classification_attention_layer_model-4  ...  0.095327\n","2  classification_attention_layer_model-2  ...  0.095516\n","1  classification_attention_layer_model-1  ...  0.094486\n","\n","[6 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"O8EJYEu0vYef"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oj89GYyZvYbw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"id":"_ySbhxMIlksb","executionInfo":{"status":"ok","timestamp":1609273948085,"user_tz":-120,"elapsed":1047,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"4e12ffd5-8493-4f58-9c34-afbf988fab5a"},"source":["df_object_inverse_time_decay"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce misclassification</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.034748</td>\n","      <td>0.048985</td>\n","      <td>0.007972</td>\n","      <td>1144.0</td>\n","      <td>0.116747</td>\n","      <td>0.962256</td>\n","      <td>0.963804</td>\n","      <td>0.972948</td>\n","      <td>0.979722</td>\n","      <td>0.002396</td>\n","      <td>0.093257</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","0  classification_attention_layer_model-1  ...  0.093257\n","\n","[1 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"g0aE5dHIsvfm","executionInfo":{"status":"ok","timestamp":1609265871830,"user_tz":-120,"elapsed":8712,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"c6ebe9b1-8cb2-4b6c-90a6-a44e02809edd"},"source":["df_object_exponential_decay"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.043349</td>\n","      <td>0.049251</td>\n","      <td>0.009713</td>\n","      <td>1405.0</td>\n","      <td>0.9537</td>\n","      <td>0.956516</td>\n","      <td>0.965463</td>\n","      <td>0.974008</td>\n","      <td>0.00354</td>\n","      <td>0.092124</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","0  classification_attention_layer_model-1  ...  0.092124\n","\n","[1 rows x 15 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"kp4jJS1xs0x2","executionInfo":{"status":"ok","timestamp":1609266057849,"user_tz":-120,"elapsed":647,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"75d697b0-2d0f-4c30-c07b-64db42045f78"},"source":["df_object_polynomial_decay"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.047539</td>\n","      <td>0.048919</td>\n","      <td>0.010751</td>\n","      <td>1570.0</td>\n","      <td>0.948167</td>\n","      <td>0.94705</td>\n","      <td>0.957903</td>\n","      <td>0.967099</td>\n","      <td>0.005447</td>\n","      <td>0.090243</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","0  classification_attention_layer_model-1  ...  0.090243\n","\n","[1 rows x 15 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"irYZn0EVs0ur","executionInfo":{"status":"ok","timestamp":1609266378531,"user_tz":-120,"elapsed":682,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"c2e1c1a3-de92-4d95-bd33-60bbeeff39a1"},"source":["df_object_piecewise_constant_decay"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.02837</td>\n","      <td>0.04875</td>\n","      <td>0.005901</td>\n","      <td>913.0</td>\n","      <td>0.972378</td>\n","      <td>0.974431</td>\n","      <td>0.98333</td>\n","      <td>0.987632</td>\n","      <td>0.000477</td>\n","      <td>0.095167</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","0  classification_attention_layer_model-1  ...  0.095167\n","\n","[1 rows x 15 columns]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"id":"MhiolXOrs8VY","executionInfo":{"status":"ok","timestamp":1609268392850,"user_tz":-120,"elapsed":608,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"82fd803d-79da-4cd1-817a-908da15efeef"},"source":["df_object_no_decay"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce misclassification</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.035577</td>\n","      <td>0.048847</td>\n","      <td>0.009377</td>\n","      <td>1377.0</td>\n","      <td>0.140525</td>\n","      <td>0.955481</td>\n","      <td>0.957845</td>\n","      <td>0.968068</td>\n","      <td>0.976012</td>\n","      <td>0.002866</td>\n","      <td>0.092791</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","0  classification_attention_layer_model-1  ...  0.092791\n","\n","[1 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"TyfR-fAkxXS4"},"source":["history_dataframe_piecewise_constant_decay=pd.read_pickle(f\"{folder_path_model_saved}/metrics_histogram_classification_attention_layer_{50}dim_{32}batchsize_{0.001}lr_{10}decaymultiplier_{saved_model_date}.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":857},"id":"cCy6XWjJzBS2","executionInfo":{"status":"ok","timestamp":1609267157636,"user_tz":-120,"elapsed":596,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"b43e3709-6e7a-4f0d-e0b4-9e00cbdecd5d"},"source":["history_dataframe_piecewise_constant_decay"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>loss</th>\n","      <th>hamming_loss</th>\n","      <th>f1_score_micro</th>\n","      <th>f1_score_none</th>\n","      <th>f1_score_macro</th>\n","      <th>val_loss</th>\n","      <th>val_hamming_loss</th>\n","      <th>val_f1_score_micro</th>\n","      <th>val_f1_score_none</th>\n","      <th>val_f1_score_macro</th>\n","      <th>epoch</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.321761</td>\n","      <td>0.094573</td>\n","      <td>0.431774</td>\n","      <td>[0.02782071, 0.018044027, 0.0, 0.0, 0.5701574,...</td>\n","      <td>0.121993</td>\n","      <td>0.220275</td>\n","      <td>0.064282</td>\n","      <td>0.613656</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.83583605, 0.0, 0.939264...</td>\n","      <td>0.230908</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.203607</td>\n","      <td>0.065014</td>\n","      <td>0.609358</td>\n","      <td>[0.2375, 0.0, 0.0, 0.0, 0.8748191, 0.0, 0.9344...</td>\n","      <td>0.232531</td>\n","      <td>0.180767</td>\n","      <td>0.059687</td>\n","      <td>0.641274</td>\n","      <td>[0.5081561, 0.0, 0.0, 0.0, 0.81845385, 0.0, 0....</td>\n","      <td>0.256285</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.168691</td>\n","      <td>0.060340</td>\n","      <td>0.637461</td>\n","      <td>[0.48584092, 0.0016090106, 0.0, 0.0, 0.8786994...</td>\n","      <td>0.257939</td>\n","      <td>0.147926</td>\n","      <td>0.057945</td>\n","      <td>0.651744</td>\n","      <td>[0.537057, 0.0, 0.0, 0.0, 0.81872255, 0.0, 0.9...</td>\n","      <td>0.271879</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.140534</td>\n","      <td>0.057593</td>\n","      <td>0.653913</td>\n","      <td>[0.5422905, 0.07028112, 0.0942623, 0.0, 0.8889...</td>\n","      <td>0.287580</td>\n","      <td>0.124546</td>\n","      <td>0.055167</td>\n","      <td>0.668442</td>\n","      <td>[0.55095184, 0.13691932, 0.4342105, 0.0, 0.861...</td>\n","      <td>0.316275</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.118214</td>\n","      <td>0.054268</td>\n","      <td>0.673909</td>\n","      <td>[0.5967223, 0.121561974, 0.544008, 0.016794961...</td>\n","      <td>0.352252</td>\n","      <td>0.100711</td>\n","      <td>0.051337</td>\n","      <td>0.691457</td>\n","      <td>[0.63925236, 0.17150398, 0.63883847, 0.0, 0.96...</td>\n","      <td>0.388432</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.099244</td>\n","      <td>0.052351</td>\n","      <td>0.685519</td>\n","      <td>[0.6206474, 0.13651137, 0.6020408, 0.16041397,...</td>\n","      <td>0.396619</td>\n","      <td>0.085530</td>\n","      <td>0.050376</td>\n","      <td>0.697234</td>\n","      <td>[0.6534898, 0.16, 0.6679462, 0.25853658, 0.836...</td>\n","      <td>0.411849</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.085954</td>\n","      <td>0.051534</td>\n","      <td>0.690356</td>\n","      <td>[0.6282966, 0.14051692, 0.56971514, 0.37233427...</td>\n","      <td>0.418037</td>\n","      <td>0.073683</td>\n","      <td>0.050466</td>\n","      <td>0.696692</td>\n","      <td>[0.62965304, 0.14076246, 0.59677416, 0.4263736...</td>\n","      <td>0.413070</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.076089</td>\n","      <td>0.051235</td>\n","      <td>0.692169</td>\n","      <td>[0.62515485, 0.13441058, 0.5737293, 0.4451754,...</td>\n","      <td>0.432411</td>\n","      <td>0.064628</td>\n","      <td>0.050031</td>\n","      <td>0.699310</td>\n","      <td>[0.6446384, 0.13469985, 0.6104418, 0.40178573,...</td>\n","      <td>0.430166</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.067986</td>\n","      <td>0.050857</td>\n","      <td>0.694400</td>\n","      <td>[0.6219856, 0.14589444, 0.57963705, 0.4771136,...</td>\n","      <td>0.448537</td>\n","      <td>0.057724</td>\n","      <td>0.049926</td>\n","      <td>0.699941</td>\n","      <td>[0.63522017, 0.13333334, 0.42727274, 0.6031128...</td>\n","      <td>0.436575</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.062014</td>\n","      <td>0.050536</td>\n","      <td>0.696300</td>\n","      <td>[0.62879497, 0.15226185, 0.55885357, 0.5290456...</td>\n","      <td>0.459975</td>\n","      <td>0.052321</td>\n","      <td>0.049640</td>\n","      <td>0.701656</td>\n","      <td>[0.51775956, 0.13670133, 0.41647598, 0.5949119...</td>\n","      <td>0.441641</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.056636</td>\n","      <td>0.050277</td>\n","      <td>0.697948</td>\n","      <td>[0.62723565, 0.17372262, 0.5958732, 0.5102362,...</td>\n","      <td>0.469248</td>\n","      <td>0.047298</td>\n","      <td>0.049535</td>\n","      <td>0.702288</td>\n","      <td>[0.5143638, 0.1506647, 0.54008436, 0.60582525,...</td>\n","      <td>0.453006</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.052021</td>\n","      <td>0.050038</td>\n","      <td>0.699378</td>\n","      <td>[0.63625044, 0.17769653, 0.62072384, 0.5057956...</td>\n","      <td>0.478571</td>\n","      <td>0.043517</td>\n","      <td>0.049280</td>\n","      <td>0.703822</td>\n","      <td>[0.5116598, 0.16934305, 0.56133056, 0.6529081,...</td>\n","      <td>0.459913</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.047410</td>\n","      <td>0.049861</td>\n","      <td>0.700378</td>\n","      <td>[0.63195974, 0.19248997, 0.6345776, 0.52353853...</td>\n","      <td>0.483174</td>\n","      <td>0.038700</td>\n","      <td>0.049265</td>\n","      <td>0.703913</td>\n","      <td>[0.6343612, 0.16519174, 0.5319149, 0.5215606, ...</td>\n","      <td>0.452083</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.043336</td>\n","      <td>0.049974</td>\n","      <td>0.699714</td>\n","      <td>[0.6337191, 0.18954247, 0.6369427, 0.5203762, ...</td>\n","      <td>0.487330</td>\n","      <td>0.035147</td>\n","      <td>0.049295</td>\n","      <td>0.703732</td>\n","      <td>[0.5868575, 0.17058824, 0.73260075, 0.30588236...</td>\n","      <td>0.455501</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.039692</td>\n","      <td>0.049875</td>\n","      <td>0.700350</td>\n","      <td>[0.6189121, 0.21331425, 0.6534749, 0.5079198, ...</td>\n","      <td>0.495039</td>\n","      <td>0.032015</td>\n","      <td>0.049250</td>\n","      <td>0.704003</td>\n","      <td>[0.55140185, 0.16519174, 0.6653772, 0.49159664...</td>\n","      <td>0.478822</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.036945</td>\n","      <td>0.049760</td>\n","      <td>0.700975</td>\n","      <td>[0.6292982, 0.2074991, 0.67772514, 0.5078534, ...</td>\n","      <td>0.501514</td>\n","      <td>0.030889</td>\n","      <td>0.049325</td>\n","      <td>0.703552</td>\n","      <td>[0.45221114, 0.18128654, 0.7664884, 0.35023043...</td>\n","      <td>0.479430</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.034899</td>\n","      <td>0.049734</td>\n","      <td>0.701227</td>\n","      <td>[0.6247631, 0.23166841, 0.6589484, 0.49894294,...</td>\n","      <td>0.503896</td>\n","      <td>0.027754</td>\n","      <td>0.049235</td>\n","      <td>0.704093</td>\n","      <td>[0.5872396, 0.17862372, 0.59756094, 0.5185185,...</td>\n","      <td>0.463632</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.032570</td>\n","      <td>0.049674</td>\n","      <td>0.701535</td>\n","      <td>[0.6163622, 0.23035523, 0.66409266, 0.51789474...</td>\n","      <td>0.509965</td>\n","      <td>0.026558</td>\n","      <td>0.049310</td>\n","      <td>0.703642</td>\n","      <td>[0.6369906, 0.18367347, 0.7986111, 0.42197803,...</td>\n","      <td>0.475469</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.031029</td>\n","      <td>0.049670</td>\n","      <td>0.701564</td>\n","      <td>[0.6134814, 0.23838241, 0.70680875, 0.51809126...</td>\n","      <td>0.513232</td>\n","      <td>0.025763</td>\n","      <td>0.049265</td>\n","      <td>0.703913</td>\n","      <td>[0.54130286, 0.15976332, 0.6187625, 0.53846157...</td>\n","      <td>0.472186</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.029471</td>\n","      <td>0.049544</td>\n","      <td>0.702230</td>\n","      <td>[0.5944723, 0.26013395, 0.69520706, 0.51786643...</td>\n","      <td>0.517690</td>\n","      <td>0.024821</td>\n","      <td>0.049265</td>\n","      <td>0.703913</td>\n","      <td>[0.44857144, 0.22285713, 0.92211837, 0.2992874...</td>\n","      <td>0.485593</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.028392</td>\n","      <td>0.049707</td>\n","      <td>0.701459</td>\n","      <td>[0.58438617, 0.2645614, 0.71042824, 0.4968085,...</td>\n","      <td>0.520245</td>\n","      <td>0.023958</td>\n","      <td>0.049250</td>\n","      <td>0.704003</td>\n","      <td>[0.6135204, 0.15976332, 0.755877, 0.2877698, 0...</td>\n","      <td>0.489372</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0.027534</td>\n","      <td>0.049570</td>\n","      <td>0.702031</td>\n","      <td>[0.5808559, 0.2960684, 0.6933962, 0.523735, 0....</td>\n","      <td>0.523386</td>\n","      <td>0.023814</td>\n","      <td>0.049235</td>\n","      <td>0.704093</td>\n","      <td>[0.6458853, 0.18658893, 0.7255985, 0.32946637,...</td>\n","      <td>0.463532</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.026224</td>\n","      <td>0.049572</td>\n","      <td>0.702306</td>\n","      <td>[0.6026438, 0.27557412, 0.6901079, 0.55284554,...</td>\n","      <td>0.527631</td>\n","      <td>0.023796</td>\n","      <td>0.049235</td>\n","      <td>0.704093</td>\n","      <td>[0.76407045, 0.14880952, 0.5991903, 0.46680945...</td>\n","      <td>0.473878</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.025034</td>\n","      <td>0.049508</td>\n","      <td>0.702488</td>\n","      <td>[0.5902066, 0.27771947, 0.6922349, 0.5385406, ...</td>\n","      <td>0.528974</td>\n","      <td>0.023314</td>\n","      <td>0.049280</td>\n","      <td>0.703822</td>\n","      <td>[0.67114097, 0.17595309, 0.76785713, 0.5425101...</td>\n","      <td>0.472625</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0.024430</td>\n","      <td>0.049425</td>\n","      <td>0.702951</td>\n","      <td>[0.5843755, 0.30722073, 0.7293578, 0.48309177,...</td>\n","      <td>0.536204</td>\n","      <td>0.022045</td>\n","      <td>0.049265</td>\n","      <td>0.703913</td>\n","      <td>[0.41921398, 0.21264367, 0.690702, 0.5206612, ...</td>\n","      <td>0.505287</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0.024119</td>\n","      <td>0.049480</td>\n","      <td>0.702761</td>\n","      <td>[0.58974355, 0.31220177, 0.70110697, 0.5378670...</td>\n","      <td>0.538770</td>\n","      <td>0.021920</td>\n","      <td>0.049220</td>\n","      <td>0.704183</td>\n","      <td>[0.47899157, 0.24788733, 0.6575875, 0.52049184...</td>\n","      <td>0.491033</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0.023274</td>\n","      <td>0.049467</td>\n","      <td>0.702683</td>\n","      <td>[0.5942286, 0.30461752, 0.69585687, 0.52296454...</td>\n","      <td>0.539423</td>\n","      <td>0.022611</td>\n","      <td>0.049265</td>\n","      <td>0.703913</td>\n","      <td>[0.7222549, 0.15703703, 0.84087104, 0.4568966,...</td>\n","      <td>0.497677</td>\n","      <td>27</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        loss  hamming_loss  ...  val_f1_score_macro epoch\n","1   0.321761      0.094573  ...            0.230908     1\n","2   0.203607      0.065014  ...            0.256285     2\n","3   0.168691      0.060340  ...            0.271879     3\n","4   0.140534      0.057593  ...            0.316275     4\n","5   0.118214      0.054268  ...            0.388432     5\n","6   0.099244      0.052351  ...            0.411849     6\n","7   0.085954      0.051534  ...            0.413070     7\n","8   0.076089      0.051235  ...            0.430166     8\n","9   0.067986      0.050857  ...            0.436575     9\n","10  0.062014      0.050536  ...            0.441641    10\n","11  0.056636      0.050277  ...            0.453006    11\n","12  0.052021      0.050038  ...            0.459913    12\n","13  0.047410      0.049861  ...            0.452083    13\n","14  0.043336      0.049974  ...            0.455501    14\n","15  0.039692      0.049875  ...            0.478822    15\n","16  0.036945      0.049760  ...            0.479430    16\n","17  0.034899      0.049734  ...            0.463632    17\n","18  0.032570      0.049674  ...            0.475469    18\n","19  0.031029      0.049670  ...            0.472186    19\n","20  0.029471      0.049544  ...            0.485593    20\n","21  0.028392      0.049707  ...            0.489372    21\n","22  0.027534      0.049570  ...            0.463532    22\n","23  0.026224      0.049572  ...            0.473878    23\n","24  0.025034      0.049508  ...            0.472625    24\n","25  0.024430      0.049425  ...            0.505287    25\n","26  0.024119      0.049480  ...            0.491033    26\n","27  0.023274      0.049467  ...            0.497677    27\n","\n","[27 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"47U7HIaD7B7a"},"source":["#### Import the first 36 models trained by the Adam Optimizer - Keras custom neural network\n","\n","Run this cell only if model_method_creation=\"adam\""]},{"cell_type":"code","metadata":{"id":"vsq9wzPpYmUm","colab":{"base_uri":"https://localhost:8080/","height":344},"executionInfo":{"status":"ok","timestamp":1609289066757,"user_tz":-120,"elapsed":632,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"8337eb51-92bd-4b81-9612-00a0e27102ec"},"source":["model_one, model_two, model_three, model_four, model_five, model_six=list_models\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six=list_df\r\n","\r\n","frames_adam_32batch_10multiplier=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six]\r\n","frames_adam_32batch_10multiplier=pd.concat(frames_adam_32batch_10multiplier)\r\n","frames_adam_32batch_10multiplier=frames_adam_32batch_10multiplier.reset_index(drop=True)\r\n","frames_adam_32batch_10multiplier=frames_adam_32batch_10multiplier.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","frames_adam_32batch_10multiplier.index += 1\r\n","#result_adam_32batch.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}'}.pkl\")\r\n","frames_adam_32batch_10multiplier # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.037176</td>\n","      <td>0.049066</td>\n","      <td>0.005505</td>\n","      <td>821.0</td>\n","      <td>0.083784</td>\n","      <td>0.974212</td>\n","      <td>0.974322</td>\n","      <td>0.984021</td>\n","      <td>0.987076</td>\n","      <td>0.000610</td>\n","      <td>0.095034</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.048551</td>\n","      <td>0.049048</td>\n","      <td>0.007930</td>\n","      <td>1185.0</td>\n","      <td>0.120931</td>\n","      <td>0.962299</td>\n","      <td>0.965530</td>\n","      <td>0.971270</td>\n","      <td>0.978868</td>\n","      <td>0.003092</td>\n","      <td>0.092567</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.040042</td>\n","      <td>0.049054</td>\n","      <td>0.008110</td>\n","      <td>1248.0</td>\n","      <td>0.127360</td>\n","      <td>0.961515</td>\n","      <td>0.960018</td>\n","      <td>0.971613</td>\n","      <td>0.977248</td>\n","      <td>0.002774</td>\n","      <td>0.092881</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.037694</td>\n","      <td>0.049055</td>\n","      <td>0.008908</td>\n","      <td>1342.0</td>\n","      <td>0.136953</td>\n","      <td>0.957564</td>\n","      <td>0.954925</td>\n","      <td>0.967862</td>\n","      <td>0.973852</td>\n","      <td>0.003424</td>\n","      <td>0.092238</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.051789</td>\n","      <td>0.049049</td>\n","      <td>0.009059</td>\n","      <td>1396.0</td>\n","      <td>0.142464</td>\n","      <td>0.956850</td>\n","      <td>0.957189</td>\n","      <td>0.967482</td>\n","      <td>0.974592</td>\n","      <td>0.003419</td>\n","      <td>0.092243</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.070723</td>\n","      <td>0.049062</td>\n","      <td>0.017655</td>\n","      <td>2499.0</td>\n","      <td>0.255026</td>\n","      <td>0.915665</td>\n","      <td>0.918502</td>\n","      <td>0.943804</td>\n","      <td>0.956149</td>\n","      <td>0.003892</td>\n","      <td>0.091775</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","2  classification_attention_layer_model-2  ...  0.095034\n","4  classification_attention_layer_model-4  ...  0.092567\n","5  classification_attention_layer_model-5  ...  0.092881\n","3  classification_attention_layer_model-3  ...  0.092238\n","6  classification_attention_layer_model-6  ...  0.092243\n","1  classification_attention_layer_model-1  ...  0.091775\n","\n","[6 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":147}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"Fv1eofWZ1qKI","executionInfo":{"status":"ok","timestamp":1609289052704,"user_tz":-120,"elapsed":661,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"0d456de7-b724-4f5d-ef9c-a9a3939d6ecb"},"source":["frames_adam_32batch_20multiplier"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.045390</td>\n","      <td>0.048974</td>\n","      <td>0.006976</td>\n","      <td>989.0</td>\n","      <td>0.100929</td>\n","      <td>0.967455</td>\n","      <td>0.971124</td>\n","      <td>0.981964</td>\n","      <td>0.986521</td>\n","      <td>-0.000075</td>\n","      <td>0.095719</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.050460</td>\n","      <td>0.048968</td>\n","      <td>0.007096</td>\n","      <td>1063.0</td>\n","      <td>0.108480</td>\n","      <td>0.966419</td>\n","      <td>0.967859</td>\n","      <td>0.975412</td>\n","      <td>0.981010</td>\n","      <td>0.002329</td>\n","      <td>0.093324</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.035031</td>\n","      <td>0.048973</td>\n","      <td>0.007402</td>\n","      <td>1127.0</td>\n","      <td>0.115012</td>\n","      <td>0.964977</td>\n","      <td>0.957692</td>\n","      <td>0.974698</td>\n","      <td>0.977035</td>\n","      <td>0.002295</td>\n","      <td>0.093357</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.035805</td>\n","      <td>0.048980</td>\n","      <td>0.007996</td>\n","      <td>1244.0</td>\n","      <td>0.126952</td>\n","      <td>0.962243</td>\n","      <td>0.963835</td>\n","      <td>0.974044</td>\n","      <td>0.980081</td>\n","      <td>0.001946</td>\n","      <td>0.093704</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.053285</td>\n","      <td>0.048970</td>\n","      <td>0.009245</td>\n","      <td>1366.0</td>\n","      <td>0.139402</td>\n","      <td>0.955882</td>\n","      <td>0.959102</td>\n","      <td>0.966194</td>\n","      <td>0.975080</td>\n","      <td>0.003733</td>\n","      <td>0.091933</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.043897</td>\n","      <td>0.048972</td>\n","      <td>0.011250</td>\n","      <td>1680.0</td>\n","      <td>0.171446</td>\n","      <td>0.946041</td>\n","      <td>0.940446</td>\n","      <td>0.958783</td>\n","      <td>0.965230</td>\n","      <td>0.004585</td>\n","      <td>0.091092</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","2  classification_attention_layer_model-2  ...  0.095719\n","4  classification_attention_layer_model-4  ...  0.093324\n","5  classification_attention_layer_model-5  ...  0.093357\n","3  classification_attention_layer_model-3  ...  0.093704\n","6  classification_attention_layer_model-6  ...  0.091933\n","1  classification_attention_layer_model-1  ...  0.091092\n","\n","[6 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":146}]},{"cell_type":"code","metadata":{"id":"-ywJqpnv2dLd"},"source":["final_result = pd.concat([frames_adam_32batch_10multiplier, frames_adam_32batch_20multiplier])\r\n","final_result.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}'}.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"BUH02vo-8smH","executionInfo":{"status":"ok","timestamp":1609287074489,"user_tz":-120,"elapsed":644,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"5dd039dc-2940-4cd6-8fa6-0a457bb2ff19"},"source":["final_result"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>Zero_one Loss (perce)</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.035318</td>\n","      <td>0.048866</td>\n","      <td>0.004700</td>\n","      <td>733.0</td>\n","      <td>0.074804</td>\n","      <td>0.978110</td>\n","      <td>0.979336</td>\n","      <td>0.988713</td>\n","      <td>0.991480</td>\n","      <td>-0.000382</td>\n","      <td>0.096025</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.042303</td>\n","      <td>0.048955</td>\n","      <td>0.004802</td>\n","      <td>739.0</td>\n","      <td>0.075416</td>\n","      <td>0.977645</td>\n","      <td>0.977351</td>\n","      <td>0.988655</td>\n","      <td>0.990602</td>\n","      <td>-0.000462</td>\n","      <td>0.096105</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.024138</td>\n","      <td>0.048840</td>\n","      <td>0.004880</td>\n","      <td>757.0</td>\n","      <td>0.077253</td>\n","      <td>0.977107</td>\n","      <td>0.977888</td>\n","      <td>0.985085</td>\n","      <td>0.988655</td>\n","      <td>0.000828</td>\n","      <td>0.094817</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.030873</td>\n","      <td>0.048767</td>\n","      <td>0.006375</td>\n","      <td>964.0</td>\n","      <td>0.098377</td>\n","      <td>0.969883</td>\n","      <td>0.969812</td>\n","      <td>0.978084</td>\n","      <td>0.982923</td>\n","      <td>0.002023</td>\n","      <td>0.093628</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.030027</td>\n","      <td>0.048921</td>\n","      <td>0.006453</td>\n","      <td>982.0</td>\n","      <td>0.100214</td>\n","      <td>0.969416</td>\n","      <td>0.970966</td>\n","      <td>0.976486</td>\n","      <td>0.982558</td>\n","      <td>0.002563</td>\n","      <td>0.093091</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.051989</td>\n","      <td>0.048945</td>\n","      <td>0.006687</td>\n","      <td>1039.0</td>\n","      <td>0.106031</td>\n","      <td>0.968528</td>\n","      <td>0.969854</td>\n","      <td>0.978920</td>\n","      <td>0.983816</td>\n","      <td>0.001383</td>\n","      <td>0.094263</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.042315</td>\n","      <td>0.048926</td>\n","      <td>0.005799</td>\n","      <td>897.0</td>\n","      <td>0.091540</td>\n","      <td>0.972815</td>\n","      <td>0.974183</td>\n","      <td>0.982944</td>\n","      <td>0.987263</td>\n","      <td>0.000728</td>\n","      <td>0.094916</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>classification_attention_layer_model-5</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.033051</td>\n","      <td>0.048948</td>\n","      <td>0.005835</td>\n","      <td>897.0</td>\n","      <td>0.091540</td>\n","      <td>0.972572</td>\n","      <td>0.973283</td>\n","      <td>0.981666</td>\n","      <td>0.985850</td>\n","      <td>0.001184</td>\n","      <td>0.094462</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>classification_attention_layer_model-3</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.036165</td>\n","      <td>0.048915</td>\n","      <td>0.007348</td>\n","      <td>1106.0</td>\n","      <td>0.112869</td>\n","      <td>0.965130</td>\n","      <td>0.965485</td>\n","      <td>0.973569</td>\n","      <td>0.979631</td>\n","      <td>0.002789</td>\n","      <td>0.092867</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>classification_attention_layer_model-4</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.050231</td>\n","      <td>0.048955</td>\n","      <td>0.007450</td>\n","      <td>1109.0</td>\n","      <td>0.113175</td>\n","      <td>0.964811</td>\n","      <td>0.965469</td>\n","      <td>0.975337</td>\n","      <td>0.980635</td>\n","      <td>0.001999</td>\n","      <td>0.093651</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>50</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.035634</td>\n","      <td>0.048936</td>\n","      <td>0.007474</td>\n","      <td>1154.0</td>\n","      <td>0.117767</td>\n","      <td>0.964593</td>\n","      <td>0.963721</td>\n","      <td>0.973992</td>\n","      <td>0.979182</td>\n","      <td>0.002496</td>\n","      <td>0.093157</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>classification_attention_layer_model-6</td>\n","      <td>150</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.051308</td>\n","      <td>0.048951</td>\n","      <td>0.008752</td>\n","      <td>1317.0</td>\n","      <td>0.134401</td>\n","      <td>0.958883</td>\n","      <td>0.962450</td>\n","      <td>0.974311</td>\n","      <td>0.980353</td>\n","      <td>0.001079</td>\n","      <td>0.094566</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Tag Name  ...  Variance\n","2  classification_attention_layer_model-2  ...  0.096025\n","4  classification_attention_layer_model-4  ...  0.096105\n","3  classification_attention_layer_model-3  ...  0.094817\n","1  classification_attention_layer_model-1  ...  0.093628\n","5  classification_attention_layer_model-5  ...  0.093091\n","6  classification_attention_layer_model-6  ...  0.094263\n","2  classification_attention_layer_model-2  ...  0.094916\n","5  classification_attention_layer_model-5  ...  0.094462\n","3  classification_attention_layer_model-3  ...  0.092867\n","4  classification_attention_layer_model-4  ...  0.093651\n","1  classification_attention_layer_model-1  ...  0.093157\n","6  classification_attention_layer_model-6  ...  0.094566\n","\n","[12 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":138}]},{"cell_type":"code","metadata":{"id":"XverE3Tm8sjh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPIhMMgf8shU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvsHznyC8seY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6wt4CJJ4gWB"},"source":["#32 batch: Approach 1k\r\n","model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine = list_models[0:9]\r\n","model_ten, model_eleven, model_twelve, model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen = list_models[9:]\r\n","\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine = list_df[0:9]\r\n","df_scores_ten, df_scores_eleven, df_scores_twelve, df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen = list_df[9:]\r\n","\r\n","frames_adam_32batch=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight,\r\n","                 df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve, df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen]\r\n","\r\n","result_adam_32batch=pd.concat(frames_adam_32batch)\r\n","result_adam_32batch=result_adam_32batch.reset_index(drop=True)\r\n","result_adam_32batch=result_adam_32batch.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","result_adam_32batch.index += 1\r\n","result_adam_32batch.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_{initialize_notebook_variables.approach_type}_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}/{f'result_adam_{initialize_notebook_variables.batch_size_value}_{initialize_notebook_variables.labelsmoothing_value}'}.pkl\")\r\n","result_adam_32batch # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_f_aH19VmenK"},"source":["batch32_approach1=pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_approach1/{'result_adam_32batch_approach1'}.pkl\")\r\n","batch32_approach2=pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_approach2/{'result_adam_32batch_approach2'}.pkl\")\r\n","batch32_approach3=pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_approach3/{'result_adam_32batch_approach3'}.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1i3cKej7oM2W"},"source":["batch32_approach1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_KpSoxuoOlC"},"source":["batch32_approach2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zSUCizQoQFa"},"source":["batch32_approach3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-8e7E2ZRVBR"},"source":["#32 batch: Approach 2\r\n","model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine = list_models[0:9]\r\n","model_ten, model_eleven, model_twelve, model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen = list_models[9:]\r\n","\r\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine = list_df[0:9]\r\n","df_scores_ten, df_scores_eleven, df_scores_twelve, df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen = list_df[9:]\r\n","\r\n","frames_adam_32batch=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight,\r\n","                 df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve, df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen]\r\n","\r\n","result_adam_32batch=pd.concat(frames_adam_32batch)\r\n","result_adam_32batch=result_adam_32batch.reset_index(drop=True)\r\n","result_adam_32batch=result_adam_32batch.sort_values(by=['Hamming Loss', 'Zero_one Loss'])\r\n","result_adam_32batch.index += 1\r\n","result_adam_32batch.to_pickle(f\"/content/drive/MyDrive/AttentionLayer/attention_layer_approach2_13_12_2020/{'result_adam_32batch_approach2'}_{datetime.now()}.pkl\")\r\n","result_adam_32batch # Note: The best model should match the best performed model from tensorboard visualizations."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"861_3WCKZRQi"},"source":["Compare the models from the 11 different metric-scores dataframes trained on the Attention layer neural networks."]},{"cell_type":"code","metadata":{"id":"TBXHZx8-ZQtJ"},"source":["df_score1 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_32batch_nolabelsmoothing.pkl\")\r\n","df_score2 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach2_32batch_nolabelsmoothing.pkl\")\r\n","df_score3 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach3_32batch_nolabelsmoothing.pkl\")\r\n","df_score4 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_64batch_nolabelsmoothing.pkl\")\r\n","df_score5 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_128batch_nolabelsmoothing.pkl\")\r\n","df_score6 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_256batch_nolabelsmoothing.pkl\")\r\n","df_score7 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_200embedding_64batch_nolabelsmoothing.pkl\")\r\n","df_score8 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_300embedding_64batch_nolabelsmoothing.pkl\")\r\n","df_score9 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach2_64batch_nolabelsmoothing.pkl\")\r\n","df_score10 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_64batch.pkl\")\r\n","df_score11 = pd.read_pickle(f\"/content/drive/MyDrive/AttentionLayer/metric_score_results/result_adam_approach1_64batch_0.1labelsmoothing.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEbwgHuFjzTD"},"source":["df_score1 = df_score1.head(1)\r\n","df_score2 = df_score2.head(1)\r\n","df_score3 = df_score3.head(1)\r\n","df_score4 = df_score4.head(1)\r\n","df_score5 = df_score5.head(1)\r\n","df_score6 = df_score6.head(1)\r\n","df_score7 = df_score7.head(1)\r\n","df_score8 = df_score8.head(1)\r\n","df_score9 = df_score9.head(1)\r\n","df_score10 = df_score10.head(1)\r\n","df_score11 = df_score11.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":594},"id":"-hirJbu-ZQq5","executionInfo":{"status":"ok","timestamp":1609004713426,"user_tz":-120,"elapsed":645,"user":{"displayName":"Nikos Spanos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl2q4AbQqOKOvPbsosIdVutClZBUsKvlKiVQLi0g=s64","userId":"13357938318257837589"}},"outputId":"944b43e4-e868-48b0-8fb5-17331cd58d87"},"source":["df_score_final = pd.concat([df_score1,df_score2,df_score3,df_score4,df_score5,df_score6,df_score7,df_score8,df_score9,df_score10,df_score11]).reset_index()\r\n","df_score_final.to_pickle(\"/content/drive/MyDrive/AttentionLayer/metric_score_results/final_dataframe_all_models_trained\")\r\n","df_score_final"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>Tag Name</th>\n","      <th>Embedding Dimension tag</th>\n","      <th>Batch tag</th>\n","      <th>Learning Rate tag</th>\n","      <th>Decay Multiplier tag</th>\n","      <th>Test Loss</th>\n","      <th>Test Hamming Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>Zero_one Loss</th>\n","      <th>F1_score</th>\n","      <th>F1_score_samples</th>\n","      <th>ROC_score</th>\n","      <th>ROC_score_samples</th>\n","      <th>Bias</th>\n","      <th>Variance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13</td>\n","      <td>classification_attention_layer_model-13</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.023548</td>\n","      <td>0.060081</td>\n","      <td>0.004496</td>\n","      <td>698.0</td>\n","      <td>0.979081</td>\n","      <td>0.980495</td>\n","      <td>0.989690</td>\n","      <td>0.992377</td>\n","      <td>-0.000551</td>\n","      <td>0.096195</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15</td>\n","      <td>classification_attention_layer_model-15</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.040359</td>\n","      <td>0.057421</td>\n","      <td>0.004778</td>\n","      <td>745.0</td>\n","      <td>0.977861</td>\n","      <td>0.979594</td>\n","      <td>0.990790</td>\n","      <td>0.993137</td>\n","      <td>-0.001250</td>\n","      <td>0.096896</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>15</td>\n","      <td>classification_attention_layer_model-15</td>\n","      <td>150</td>\n","      <td>32</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.047936</td>\n","      <td>0.058783</td>\n","      <td>0.004862</td>\n","      <td>758.0</td>\n","      <td>0.977475</td>\n","      <td>0.979324</td>\n","      <td>0.990644</td>\n","      <td>0.993051</td>\n","      <td>-0.001278</td>\n","      <td>0.096924</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7</td>\n","      <td>classification_attention_layer_model-7</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.022314</td>\n","      <td>0.049645</td>\n","      <td>0.004550</td>\n","      <td>717.0</td>\n","      <td>0.978887</td>\n","      <td>0.979840</td>\n","      <td>0.990745</td>\n","      <td>0.992787</td>\n","      <td>-0.001006</td>\n","      <td>0.096651</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14</td>\n","      <td>classification_attention_layer_model-14</td>\n","      <td>150</td>\n","      <td>128</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.025769</td>\n","      <td>0.050410</td>\n","      <td>0.004454</td>\n","      <td>697.0</td>\n","      <td>0.979376</td>\n","      <td>0.980455</td>\n","      <td>0.991933</td>\n","      <td>0.993862</td>\n","      <td>-0.001362</td>\n","      <td>0.097008</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>16</td>\n","      <td>classification_attention_layer_model-16</td>\n","      <td>150</td>\n","      <td>256</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.039171</td>\n","      <td>0.049367</td>\n","      <td>0.004946</td>\n","      <td>761.0</td>\n","      <td>0.977011</td>\n","      <td>0.977824</td>\n","      <td>0.988994</td>\n","      <td>0.991706</td>\n","      <td>-0.000734</td>\n","      <td>0.096379</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>classification_attention_layer_model-1</td>\n","      <td>200</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.026594</td>\n","      <td>0.048979</td>\n","      <td>0.004520</td>\n","      <td>716.0</td>\n","      <td>0.978989</td>\n","      <td>0.979034</td>\n","      <td>0.990046</td>\n","      <td>0.991879</td>\n","      <td>-0.000711</td>\n","      <td>0.096355</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2</td>\n","      <td>classification_attention_layer_model-2</td>\n","      <td>300</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>20</td>\n","      <td>0.025427</td>\n","      <td>0.048744</td>\n","      <td>0.004502</td>\n","      <td>701.0</td>\n","      <td>0.979107</td>\n","      <td>0.979708</td>\n","      <td>0.990821</td>\n","      <td>0.992887</td>\n","      <td>-0.000987</td>\n","      <td>0.096633</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>7</td>\n","      <td>classification_attention_layer_model-7</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.001</td>\n","      <td>10</td>\n","      <td>0.026359</td>\n","      <td>0.050990</td>\n","      <td>0.005073</td>\n","      <td>786.0</td>\n","      <td>0.976233</td>\n","      <td>0.977160</td>\n","      <td>0.985077</td>\n","      <td>0.988554</td>\n","      <td>0.000638</td>\n","      <td>0.095006</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>classification_attention_layer_model-9</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>10</td>\n","      <td>0.338576</td>\n","      <td>0.050150</td>\n","      <td>0.004010</td>\n","      <td>638.0</td>\n","      <td>0.981523</td>\n","      <td>0.982415</td>\n","      <td>0.995264</td>\n","      <td>0.996116</td>\n","      <td>-0.002182</td>\n","      <td>0.097834</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>classification_attention_layer_model-10</td>\n","      <td>100</td>\n","      <td>64</td>\n","      <td>0.010</td>\n","      <td>20</td>\n","      <td>0.219131</td>\n","      <td>0.050300</td>\n","      <td>0.004394</td>\n","      <td>697.0</td>\n","      <td>0.979633</td>\n","      <td>0.980650</td>\n","      <td>0.991621</td>\n","      <td>0.993596</td>\n","      <td>-0.001184</td>\n","      <td>0.096830</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    index                                 Tag Name  ...      Bias  Variance\n","0      13  classification_attention_layer_model-13  ... -0.000551  0.096195\n","1      15  classification_attention_layer_model-15  ... -0.001250  0.096896\n","2      15  classification_attention_layer_model-15  ... -0.001278  0.096924\n","3       7   classification_attention_layer_model-7  ... -0.001006  0.096651\n","4      14  classification_attention_layer_model-14  ... -0.001362  0.097008\n","5      16  classification_attention_layer_model-16  ... -0.000734  0.096379\n","6       1   classification_attention_layer_model-1  ... -0.000711  0.096355\n","7       2   classification_attention_layer_model-2  ... -0.000987  0.096633\n","8       7   classification_attention_layer_model-7  ...  0.000638  0.095006\n","9       9   classification_attention_layer_model-9  ... -0.002182  0.097834\n","10     10  classification_attention_layer_model-10  ... -0.001184  0.096830\n","\n","[11 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"ydBa9WvFlBX_"},"source":["----------------------------------------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"xl9YFKl17B7a"},"source":["model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n","model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen, model_nineteen, model_twenty, model_twenty_one, model_twenty_two, model_twenty_three, model_twenty_four=list_models[12:24]\n","model_twenty_five, model_twenty_six, model_twenty_seven, model_twenty_eight, model_twenty_nine, model_twenty_thirty, model_thirty_one, model_thirty_two, model_thirty_three, model_thirty_four, model_thirty_five, model_thirty_six=list_models[24:36]\n","\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n","df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four=list_df[12:24]\n","df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven, df_scores_twenty_eight, df_scores_twenty_nine, df_scores_thirty, df_scores_thirty_one, df_scores_thirty_two, df_scores_thirty_three, df_scores_thirty_four, df_scores_thirty_five, df_scores_thirty_six=list_df[24:36]\n","\n","frames_adam_one=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n","                 df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four,\n","                 df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven, df_scores_twenty_eight, df_scores_twenty_nine, df_scores_thirty, df_scores_thirty_one, df_scores_thirty_two, df_scores_thirty_three, df_scores_thirty_four, df_scores_thirty_five, df_scores_thirty_six]\n","\n","result_adam_one=pd.concat(frames_adam_one)\n","result_adam_one=result_adam_one.reset_index(drop=True)\n","result_adam_one.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUf_jTs-7B7a"},"source":["print(result_adam_one.to_latex(index=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVjcGR-C7B7a"},"source":["result_adam_one.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n","                                                                                                                           \"results_table_adam_one\",\n","                                                                                                                           str(100), #Embedding size of the the best model estimator\n","                                                                                                                           str(32), #Batch size of the the best model estimator\n","                                                                                                                           str(0.001), #Learning rate of the the best model estimator\n","                                                                                                                           str(10),  #Decay Steps Multiplayer of the the best model estimator\n","                                                                                                                           version_data_control)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5QGo2mL7B7b"},"source":["Best model of 36 presented above is the model 7 with:\n","* Embedding size: 100\n","* Batch size: 32\n","* Learning rate: 0.001\n","* Decay Steps Multiplier: 10\n","* Hamming loss & Zeron-one loss: 0.003620 - 566.0"]},{"cell_type":"code","metadata":{"id":"S7KGhQms7B7b"},"source":["# The rest 18 models (37-54)\n","\n","list_models=[]\n","list_df=[]\n","\n","if model_method_creation==\"adam\":\n","\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([128]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n","    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10, 20]))\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n","                    hparams = {\n","                        HP_HIDDEN_UNITS: batch_size,\n","                        HP_EMBEDDING_DIM: embedding_dim,\n","                        HP_LEARNING_RATE: learning_rate,\n","                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n","                      }\n","                    print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n","                    model_object=import_trained_keras_model(\"import custom trained model\", \"on\", model_method_creation, hparams)\n","                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n","                    list_models.append(model_object)\n","                    list_df.append(df_object)\n","\n","else:\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                hparams = {\n","                    HP_HIDDEN_UNITS: batch_size,\n","                    HP_EMBEDDING_DIM: embedding_dim,\n","                    HP_LEARNING_RATE: learning_rate\n","                  }\n","                print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n","                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n","                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n","                list_models.append(model_object)\n","                list_df.append(df_object)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"foOyusR57B7b"},"source":["#### Import the rest 18 models trained by the Adam Optimizer - Keras custom neural network\n","\n","Run this cell only if model_method_creation=\"adam\""]},{"cell_type":"code","metadata":{"id":"ShnHHR7D7B7b"},"source":["model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n","model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen=list_models[12:18]\n","\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n","df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen=list_df[12:18]\n","\n","frames_adam_two=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n","                 df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen]\n","\n","result_adam_two=pd.concat(frames_adam_two)\n","result_adam_two=result_adam_two.reset_index(drop=True)\n","result_adam_two.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSEnyZSd7B7b"},"source":["print(result_adam_two.to_latex(index=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"khTpKTOq7B7b"},"source":["result_adam_two.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n","                                                                                                                           \"results_table_adam_two\",\n","                                                                                                                           str(100), #Embedding size of the the best model estimator\n","                                                                                                                           str(128), #Batch size of the the best model estimator\n","                                                                                                                           str(0.001), #Learning rate of the the best model estimator\n","                                                                                                                           str(20),  #Decay Steps Multiplayer of the the best model estimator\n","                                                                                                                           version_data_control)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"elLeJGa57B7b"},"source":["Best model of 36 (0-36) presented above is the model 7 with:\n","* Embedding size: 100\n","* Batch size: 32\n","* Learning rate: 0.001\n","* Decay Steps Multiplier: 10\n","* Hamming loss & Zeron-one loss: 0.003620 - 566.0\n","    \n","Best model of 18 (36-54) presented above is the model 44 with:\n","* Embedding size: 100\n","* Batch size: 128\n","* Learning rate: 0.001\n","* Decay Steps Multiplier: 20\n","* Hamming loss & Zeron-one loss: 0.003986 - 622.0\n","\n","The best out of the two is the model 7"]},{"cell_type":"markdown","metadata":{"id":"MkRThlTg7B7b"},"source":["#### -----------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"BfK9jqgb7B7c"},"source":["#### Import all the models trained by the SGD Optimizer - Keras custom neural network\n","\n","Run this cell only if model_method_creation=\"sgd\""]},{"cell_type":"code","metadata":{"id":"avUIavaU7B7c"},"source":["saved_model_name=\"multi_input_keras_model\"\n","folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n","saved_df_scored_metric_name=\"df_metrics_multi_input_keras_model\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AH-wPqx17B7c"},"source":["model_method_creation=\"sgd\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhsWxMv27B7c"},"source":["list_models=[]\n","list_df=[]\n","\n","if model_method_creation==\"adam\":\n","\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n","    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10, 20])) #only used in adam optimizer\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n","                    hparams = {\n","                        HP_HIDDEN_UNITS: batch_size,\n","                        HP_EMBEDDING_DIM: embedding_dim,\n","                        HP_LEARNING_RATE: learning_rate,\n","                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n","                      }\n","                    print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n","                    model_object=import_trained_keras_model(\"import custom trained model\", \"on\", model_method_creation, hparams)\n","                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n","                    list_models.append(model_object)\n","                    list_df.append(df_object)\n","\n","else:\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                hparams = {\n","                    HP_HIDDEN_UNITS: batch_size,\n","                    HP_EMBEDDING_DIM: embedding_dim,\n","                    HP_LEARNING_RATE: learning_rate\n","                  }\n","                print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n","                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n","                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n","                list_models.append(model_object)\n","                list_df.append(df_object)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUptc-bN7B7c"},"source":["model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n","model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen, model_nineteen, model_twenty, model_twenty_one, model_twenty_two, model_twenty_three, model_twenty_four=list_models[12:24]\n","model_twenty_five, model_twenty_six, model_twenty_seven=list_models[24:27]\n","\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n","df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four=list_df[12:24]\n","df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven=list_df[24:27]\n","\n","frames_glove=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n","              df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four,\n","              df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven]\n","\n","result_sgd=pd.concat(frames_glove)\n","result_sgd=result_sgd.reset_index(drop=True)\n","result_sgd.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58i6bNJ37B7c"},"source":["print(result_sgd.to_latex(index=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hYbY54gu7B7c"},"source":["result_sgd.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n","                                                                                                   \"results_table_sgd\",\n","                                                                                                   str(100), #Embedding size of the the best model estimator\n","                                                                                                   str(64), #Batch size of the the best model estimator\n","                                                                                                   str(0.1), #Learning rate of the the best model estimator\n","                                                                                                   version_data_control)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8MoaC8Z7B7d"},"source":["Best model of 27 presented above is the model 15 with:\n","* Embedding size: 100\n","* Batch size: 64\n","* Learning rate: 0.1\n","* Hamming loss & Zeron-one loss: 0.009035 - 1363.0"]},{"cell_type":"markdown","metadata":{"id":"dmxu7FBx7B7d"},"source":["#### -----------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"JQquYIs67B7d"},"source":["#### Import all the models trained by the RMSprop Optimizer - Keras custom neural network\n","\n","Run this cell only if model_method_creation=\"rmsprop\""]},{"cell_type":"code","metadata":{"id":"cDMKvEik7B7d"},"source":["# Initialize name variables\n","\n","saved_model_name=\"multi_input_keras_model\"\n","folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n","saved_df_scored_metric_name=\"df_metrics_multi_input_keras_model\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"huDZs2-V7B7d"},"source":["model_method_creation=\"rmsprop\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cYLoZsw7B7d"},"source":["list_models=[]\n","list_df=[]\n","\n","if model_method_creation==\"adam\":\n","\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n","    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10])) #only used in adam optimizer\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n","                    hparams = {\n","                        HP_HIDDEN_UNITS: batch_size,\n","                        HP_EMBEDDING_DIM: embedding_dim,\n","                        HP_LEARNING_RATE: learning_rate,\n","                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n","                      }\n","                    print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n","                    model_object=import_trained_keras_model(\"import custom trained model\", \"on\", model_method_creation, hparams)\n","                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n","                    list_models.append(model_object)\n","                    list_df.append(df_object)\n","\n","else:\n","    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n","    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n","    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n","\n","    for batch_size in HP_HIDDEN_UNITS.domain.values:\n","        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","            for learning_rate in HP_LEARNING_RATE.domain.values:\n","                hparams = {\n","                    HP_HIDDEN_UNITS: batch_size,\n","                    HP_EMBEDDING_DIM: embedding_dim,\n","                    HP_LEARNING_RATE: learning_rate\n","                  }\n","                print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n","                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n","                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n","                list_models.append(model_object)\n","                list_df.append(df_object)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNsUdKVr7B7d"},"source":["model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n","model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen, model_nineteen, model_twenty, model_twenty_one, model_twenty_two, model_twenty_three, model_twenty_four=list_models[12:24]\n","model_twenty_five, model_twenty_six, model_twenty_seven=list_models[24:27]\n","\n","df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n","df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four=list_df[12:24]\n","df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven=list_df[24:27]\n","\n","frames_rmsprop=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n","                df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four,\n","                df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven]\n","\n","result_rmsprop=pd.concat(frames_rmsprop)\n","result_rmsprop=result_rmsprop.reset_index(drop=True)\n","result_rmsprop.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX6ba9dk7B7e"},"source":["print(result_rmsprop.to_latex(index=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARoltPYU7B7e"},"source":["result_rmsprop.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n","                                                                                                       \"results_table_rmsprop\",\n","                                                                                                       str(150), #Embedding size of the the best model estimator\n","                                                                                                       str(64), #Batch size of the the best model estimator\n","                                                                                                       str(0.001), #Learning rate of the the best model estimator\n","                                                                                                       version_data_control)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kx2xrzhR7B7e"},"source":["Best model of 27 presented above is the model 16 with:\n","* Embedding size: 150\n","* Batch size: 64\n","* Learning rate: 0.001\n","* Hamming loss & Zeron-one loss: 0.004064 - 637.0"]},{"cell_type":"markdown","metadata":{"id":"-6rB6JMs7B7e"},"source":["# <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"]},{"cell_type":"markdown","metadata":{"id":"oqoCMSWM7B7e"},"source":["The three final best models, one per optimizer, are:\n","\n","**Adam**\n","* Model 7\n","* Embedding size: 100\n","* Batch size: 32\n","* Learning rate: 0.001\n","* Decay Steps Multiplier: 10\n","* Hamming loss & Zeron-one loss: 0.003620 - 566.0\n","\n","**SGD**\n","* Model 15\n","* Embedding size: 100\n","* Batch size: 64\n","* Learning rate: 0.1\n","* Hamming loss & Zeron-one loss: 0.009035 - 1363.0\n","\n","**RMSprop**\n","* Model 16\n","* Embedding size: 150\n","* Batch size: 64\n","* Learning rate: 0.001\n","* Hamming loss & Zeron-one loss: 0.004064 - 637.0"]},{"cell_type":"markdown","metadata":{"id":"3aeX1iiz7B7e"},"source":["# <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"]},{"cell_type":"markdown","metadata":{"id":"flj41zpV7B7e"},"source":["**Comparison 2: Create a classification report and a confusion matrix for the two closest models (per optimizer)** <br>\n","Additionally, create a bias-variance tradeoff tample per optimizer"]},{"cell_type":"code","metadata":{"id":"oxzli7hG7B7e"},"source":["# Best model selected-Adam\n","\n","HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n","HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n","                hparams_adam = {\n","                    HP_HIDDEN_UNITS: batch_size,\n","                    HP_EMBEDDING_DIM: embedding_dim,\n","                    HP_LEARNING_RATE: learning_rate,\n","                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n","                  }\n","                folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n","                model_one = import_trained_keras_model(\"import custom trained model\", \"on\", \"adam\", hparams_adam)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1OVhWWk7B7f"},"source":["# Best model selected-SGD\n","\n","HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.1]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            hparams_sgd = {\n","                HP_HIDDEN_UNITS: batch_size,\n","                HP_EMBEDDING_DIM: embedding_dim,\n","                HP_LEARNING_RATE: learning_rate\n","              }\n","            folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n","            model_two = import_trained_keras_model(\"import custom trained model\", \"off\", \"sgd\", hparams_sgd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IR_axhds7B7f"},"source":["# Best model selected-RMSprop\n","\n","HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([150]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            hparams = {\n","                HP_HIDDEN_UNITS: batch_size,\n","                HP_EMBEDDING_DIM: embedding_dim,\n","                HP_LEARNING_RATE: learning_rate\n","              }\n","            folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n","            model_three = import_trained_keras_model(\"import custom trained model\", \"off\", \"rmsprop\", hparams)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PxteAVN7B7f"},"source":["saved_version_control=\"20072020\"\n","\n","history_dataframe_one=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_{0}\\\\metrics_histogram_multi_input_keras_{1}dim_{2}batchsize_{3}lr_{4}decaymultiplier_16072020.pkl\".format(saved_version_control, str(100), str(32), str(0.001), str(10))))\n","history_dataframe_two=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\sgd_models_{0}\\\\metrics_histogram_multi_input_keras_{1}dim_{2}batchsize_{3}lr_16072020.pkl\".format(saved_version_control, str(100), str(64), str(0.1))))\n","history_dataframe_three=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\rmsprop_models_{0}\\\\metrics_histogram_multi_input_keras_{1}dim_{2}batchsize_{3}lr_16072020.pkl\".format(saved_version_control, str(150), str(64), str(0.001))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjXeCI597B7f"},"source":["def create_classification_table(model):\n","    \n","    y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n","    y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n","    \n","    variance = np.var(y_test_predictions)\n","    sse = np.mean((np.mean(y_test_predictions) - y_test)**2)\n","    bias = sse - variance\n","\n","    classification_table = classification_report(y_true=y_test, y_pred=y_test_predictions)\n","    \n","    return classification_table, variance, bias\n","\n","def create_confusion_matrix(mode, decay_steps_mode, embedding_dim_mode,  model, hparams):\n","\n","    if mode == \"custom trained model\":\n","        \n","        if decay_steps_mode==\"on\":\n","            \n","            if embedding_dim_mode==\"on\":\n","        \n","                y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n","                y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n","\n","                conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n","\n","                conf_matrix=pd.DataFrame(conf_mat,\n","                                         columns=genres_list,\n","                                         index=genres_list)\n","\n","                conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n","                                                                                                                                       \"confusion_matrix\",\n","                                                                                                                                       str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                                                       str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                                                       str(hparams[HP_LEARNING_RATE]), \n","                                                                                                                                       str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","                                                                                                                                       version_data_control)))\n","            else:\n","                \n","                y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n","                y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n","\n","                conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n","\n","                conf_matrix=pd.DataFrame(conf_mat,\n","                                         columns=genres_list,\n","                                         index=genres_list)\n","\n","                conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n","                                                                                                                                \"confusion_matrix\",\n","                                                                                                                                str(hparams[HP_HIDDEN_UNITS]),\n","                                                                                                                                str(hparams[HP_LEARNING_RATE]),\n","                                                                                                                                str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","                                                                                                                                version_data_control)))\n","        else:\n","            y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n","            y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n","\n","            conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n","\n","            conf_matrix=pd.DataFrame(conf_mat,\n","                                     columns=genres_list,\n","                                     index=genres_list)\n","\n","            conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n","                                                                                                                \"confusion_matrix\",\n","                                                                                                                str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                                str(hparams[HP_HIDDEN_UNITS]),\n","                                                                                                                str(hparams[HP_LEARNING_RATE]),\n","                                                                                                                version_data_control)))\n","    else:\n","        \n","        y_test_pred_probs = model.predict([test_bytes_list_plot, test_bytes_list_features, test_bytes_list_reviews, test_bytes_list_title])\n","        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n","\n","        conf_mat=confusion_matrix(test_label.argmax(axis=1), y_test_predictions.argmax(axis=1))\n","\n","        conf_matrix=pd.DataFrame(conf_mat,\n","                                 columns=genres_list,\n","                                 index=genres_list)\n","\n","        conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n","                                                                                                                        \"confusion_matrix\",\n","                                                                                                                        str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                                        str(hparams[HP_LEARNING_RATE]), \n","                                                                                                                        str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","                                                                                                                        version_data_control)))\n","    return conf_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"LdVjT6Qd7B7f"},"source":["#ADAM Optimizer\n","\n","folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n","\n","classification_table_one, variance_adam, bias_adam=create_classification_table(model_one)\n","print(\"Classification report for the best model estimator of the Adam optimizaion function:\\n\\n\" + str(classification_table_one))\n","\n","bias_variance_tradeoff_adam=pd.DataFrame({'Tag Name':pd.Series(\"model seven adam\", dtype='str'),\n","                                          'Bias': pd.Series(bias_adam, dtype='str'),\n","                                          'Variance': pd.Series(variance_adam, dtype='str'), \n","                                          'Average Training loss': pd.Series(np.mean(history_dataframe_one.loss), dtype='str'),\n","                                          'Average Validation loss': pd.Series(np.mean(history_dataframe_one.val_loss), dtype='str')})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wG6W8EgX7B7f"},"source":["#### ---------------------------------------------------------------------------#ADAM Optimizer\n","\n","HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n","HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n","                hparams_adam = {\n","                    HP_HIDDEN_UNITS: batch_size,\n","                    HP_EMBEDDING_DIM: embedding_dim,\n","                    HP_LEARNING_RATE: learning_rate,\n","                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n","                  }\n","                folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n","                confusion_matrix_one=create_confusion_matrix(\"custom trained model\", \"on\", \"on\", model_one, hparams_adam)\n","confusion_matrix_one"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lQ4U_d_7B7f"},"source":["print(confusion_matrix_one.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7mwhWd87B7g"},"source":["#### ---------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"02Loc8qW7B7g"},"source":["#SGD Optimizer\n","\n","folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n","\n","classification_table_two, variance_sgd, bias_sgd=create_classification_table(model_two)\n","print(\"Classification report for the best model estimator of the SGD optimizaion function:\\n\\n\" + str(classification_table_two))\n","\n","bias_variance_tradeoff_sgd=pd.DataFrame({'Tag Name':pd.Series(\"model fifteen sgd\", dtype='str'),\n","                                         'Bias': pd.Series(bias_sgd, dtype='str'),\n","                                         'Variance': pd.Series(variance_sgd, dtype='str'),\n","                                         'Average Training loss': pd.Series(np.mean(history_dataframe_two.loss), dtype='str'),\n","                                         'Average Validation loss': pd.Series(np.mean(history_dataframe_two.val_loss), dtype='str')})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"nc3kAziv7B7g"},"source":["#SGD Optimizer\n","\n","HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.1]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            hparams_sgd = {\n","                HP_HIDDEN_UNITS: batch_size,\n","                HP_EMBEDDING_DIM: embedding_dim,\n","                HP_LEARNING_RATE: learning_rate\n","              }\n","            folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n","            confusion_matrix_two=create_confusion_matrix(\"custom trained model\", \"off\", \"on\", model_two, hparams_sgd)\n","confusion_matrix_two"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHCFlFqt7B7g"},"source":["print(confusion_matrix_two.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8aScXx17B7g"},"source":["#### ---------------------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"f8k7QIMw7B7g"},"source":["#RMSprop Optimizer\n","\n","folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n","\n","classification_table_three, variance_rmsprop, bias_rmsprop=create_classification_table(model_three)\n","print(\"Classification report for the best model estimator of the SGD optimizaion function:\\n\\n\" + str(classification_table_three))\n","\n","bias_variance_tradeoff_rmsprop=pd.DataFrame({'Tag Name':pd.Series(\"model sixteen rmsprop\", dtype='str'),\n","                                             'Bias': pd.Series(bias_rmsprop, dtype='str'),\n","                                             'Variance': pd.Series(variance_rmsprop, dtype='str'),\n","                                             'Average Training loss': pd.Series(np.mean(history_dataframe_three.loss), dtype='str'),\n","                                             'Average Validation loss': pd.Series(np.mean(history_dataframe_three.val_loss), dtype='str')})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tktqaw0n7B7g"},"source":["#RMSprop Optimizer\n","\n","HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([150]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            hparams_rmsprop = {\n","                HP_HIDDEN_UNITS: batch_size,\n","                HP_EMBEDDING_DIM: embedding_dim,\n","                HP_LEARNING_RATE: learning_rate\n","              }\n","            folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n","            confusion_matrix_three=create_confusion_matrix(\"custom trained model\", \"off\", \"on\", model_three, hparams_rmsprop)\n","confusion_matrix_three"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kD_XjZCq7B7g"},"source":["print(confusion_matrix_three.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"we7LJZ2s7B7h"},"source":["bias_variance_frames = [bias_variance_tradeoff_adam, bias_variance_tradeoff_sgd, bias_variance_tradeoff_rmsprop]\n","bias_variance_result = pd.concat(bias_variance_frames)\n","bias_variance_result = bias_variance_result.reset_index(drop=True)\n","bias_variance_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6Lvst3x7B7h"},"source":["print(bias_variance_result.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VvZ49J47B7h"},"source":["**Comparison 3: Test Accuracy - Test Score/Loss**"]},{"cell_type":"code","metadata":{"id":"cjo1jDb87B7h"},"source":["# Concat the df_metric_score dataframes of the three models under review.\n","\n","df_score_one=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_20072020\\\\df_metrics_multi_input_keras_model_100dim_32batchsize_0.001lr_10decaymultiplier_16072020.pkl\"))\n","df_score_two=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\sgd_models_20072020\\\\df_metrics_multi_input_keras_model_100dim_64batchsize_0.1lr_16072020.pkl\"))\n","df_score_three=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\rmsprop_models_20072020\\\\df_metrics_multi_input_keras_model_150dim_64batchsize_0.001lr_16072020.pkl\"))\n","\n","result=pd.concat([df_score_one, df_score_two, df_score_three])\n","result=result.reset_index(drop=True)\n","result.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNPAAhCt7B7h"},"source":["print(result.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36Ewx4Xt7B7h"},"source":["colormin = 'red'\n","colorother = 'black'\n","clrs_acc = [colormin if result['Test Hamming Loss'].iloc[row]== result['Test Hamming Loss'].min() else colorother for row in range(len(result['Test Hamming Loss']))]\n","clrs_loss = [colormin if result['Test Loss'].iloc[row]== result['Test Loss'].min() else colorother for row in range(len(result['Test Loss']))]\n","\n","x=result['Tag Name'].values.tolist()\n","y=result['Test Hamming Loss'].values.tolist()\n","fig5 = go.Figure()\n","fig5.add_trace(go.Scatter(x=x, y=y,\n","                          mode='markers',\n","                          marker=dict(color=clrs_acc)\n","                        ))\n","fig5.update_layout(title=\"Hamming Loss on test set per model\",\n","                   xaxis_title=\"Model number\",\n","                   yaxis_title=\"Hamming Loss value/model\")\n","fig5.show()\n","\n","#--------------------------------------------------\n","\n","fig6 = go.Figure()\n","fig6.add_trace(go.Scatter(x=result['Tag Name'].values.tolist(), \n","                          y=result['Test Loss'].values.tolist(),\n","                          mode='markers',\n","                          marker=dict(color=clrs_loss)\n","                         ))\n","\n","fig6.update_layout(title=\"Loss score on test set per model\",\n","                  xaxis_title=\"Model number\",\n","                  yaxis_title=\"Test loss/model\")\n","fig6.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69TfXaAz7B7i"},"source":["#### Comparison 4: Predicted vs Actual Genre Tags"]},{"cell_type":"code","metadata":{"id":"z744v3JR7B7i"},"source":["X_test=pd.read_pickle(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_13072020\\\\x_test_13072020.pkl\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmEfg44k7B7i"},"source":["def predict_genre_tags(indx, model, genres_list):\n","        \n","    test_sequence_actors = X_test_seq_actors[indx:indx+1]\n","    \n","    test_sequence_plot = X_test_seq_plot[indx:indx+1]\n","    \n","    test_sequence_features = X_test_seq_features[indx:indx+1]\n","    \n","    test_sequence_reviews = X_test_seq_reviews[indx:indx+1]\n","    \n","    test_sequence_title = X_test_seq_title[indx:indx+1]\n","    \n","    text_prediction = model.predict([test_sequence_actors, test_sequence_plot, test_sequence_features, test_sequence_reviews, test_sequence_title])\n","    \n","    [float(i) for i in text_prediction[0]]\n","    \n","    genres_length=len(X_test['reduced_genres'].iloc[indx])\n","    \n","    tag_probabilities = text_prediction[0][np.argsort(text_prediction[0])[-genres_length:]]\n","    \n","    indexes = np.argsort(text_prediction[0])[::-1][:genres_length]\n","    \n","    indexes = np.sort(indexes)\n","    \n","    predicted_tags = []\n","    \n","    predicted_tags = [genres_list[i] for i in indexes]\n","    \n","    return predicted_tags\n","\n","def create_predictions_df(model, random_numbers_list, file_name, optimizer_name,  hparams):\n","    \n","    if optimizer_name==\"adam\":\n","    \n","        df_predictions = pd.DataFrame({'Movie Title': pd.Series([X_test['title'].iloc[random_numbers_list[0]]], dtype='str'),\n","                                       'Predicted Genre tags (top 3)': pd.Series([predict_genre_tags(random_numbers_list[0], model, genres_list)], dtype='str'),\n","                                       'Real Genre tags': pd.Series([X_test['reduced_genres'].iloc[random_numbers_list[0]]], dtype='str')})\n","\n","        for i in range(len(random_numbers_list)):\n","\n","            df_predictions = df_predictions.append({'Movie Title': X_test['title'].iloc[random_numbers_list[i]], \n","                                                    'Predicted Genre tags (top 3)': predict_genre_tags(random_numbers_list[i], model, genres_list),\n","                                                    'Real Genre tags': X_test['reduced_genres'].iloc[random_numbers_list[i]]} , ignore_index=True)\n","\n","        df_predictions = df_predictions.drop(df_predictions.index[0])\n","        df_predictions.to_pickle(\"model_one\\\\{0}\\\\{1}_df_predictions_{2}dim_{3}batchsize_{4}lr_{5}decatmultiplier_{6}.pkl\".format(file_name, \n","                                                                                                                                  optimizer_name, \n","                                                                                                                                  str(hparams[HP_EMBEDDING_DIM]), \n","                                                                                                                                  str(hparams[HP_HIDDEN_UNITS]), \n","                                                                                                                                  str(hparams[HP_LEARNING_RATE]), \n","                                                                                                                                  str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n","                                                                                                                                  version_data_control))\n","    else:\n","        \n","        df_predictions = pd.DataFrame({'Movie Title': pd.Series([X_test['title'].iloc[random_numbers_list[0]]], dtype='str'),\n","                                       'Predicted Genre tags (top 3)': pd.Series([predict_genre_tags(random_numbers_list[0], model, genres_list)], dtype='str'),\n","                                       'Real Genre tags': pd.Series([X_test['reduced_genres'].iloc[random_numbers_list[0]]], dtype='str')})\n","\n","        for i in range(len(random_numbers_list)):\n","\n","            df_predictions = df_predictions.append({'Movie Title': X_test['title'].iloc[random_numbers_list[i]], \n","                                                    'Predicted Genre tags (top 3)': predict_genre_tags(random_numbers_list[i], model, genres_list),\n","                                                    'Real Genre tags': X_test['reduced_genres'].iloc[random_numbers_list[i]]} , ignore_index=True)\n","\n","        df_predictions = df_predictions.drop(df_predictions.index[0])\n","        df_predictions.to_pickle(\"model_one\\\\{0}\\\\{1}_df_predictions_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(file_name, \n","                                                                                                               optimizer_name, \n","                                                                                                               str(hparams[HP_EMBEDDING_DIM]),\n","                                                                                                               str(hparams[HP_HIDDEN_UNITS]),\n","                                                                                                               str(hparams[HP_LEARNING_RATE]),\n","                                                                                                               version_data_control))\n","    return df_predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p2EHFiNQ7B7i"},"source":["random_numbers = random.sample(range(1, y_test.shape[0]), 20)\n","\n","save_index_of_numbers = random_numbers\n","\n","print(\"Randomly saved numbers to make predictions: {}\".format(save_index_of_numbers))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgvthVPz7B7i"},"source":["HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n","HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n","                hparams_adam = {\n","                    HP_HIDDEN_UNITS: batch_size,\n","                    HP_EMBEDDING_DIM: embedding_dim,\n","                    HP_LEARNING_RATE: learning_rate,\n","                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n","                  }\n","                predictions_dataframe_one=create_predictions_df(model_one, save_index_of_numbers, \"adam_v2_models_20072020\", \"adam\", hparams_adam)\n","predictions_dataframe_one"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQ32d7Wz7B7i"},"source":["print(predictions_dataframe_one.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mfl1eU87B7j"},"source":["HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n","HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n","HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.1]))\n","\n","for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            hparams_sgd = {\n","                HP_HIDDEN_UNITS: batch_size,\n","                HP_EMBEDDING_DIM: embedding_dim,\n","                HP_LEARNING_RATE: learning_rate\n","              }\n","            predictions_dataframe_two=create_predictions_df(model_two, save_index_of_numbers, \"sgd_models_20072020\", \"sgd\", hparams_sgd)\n","predictions_dataframe_two"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Fcua3vs7B7j"},"source":["print(predictions_dataframe_two.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43ruQiSo7B7j"},"source":["for batch_size in HP_HIDDEN_UNITS.domain.values:\n","    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n","        for learning_rate in HP_LEARNING_RATE.domain.values:\n","            hparams_rmsprop = {\n","                HP_HIDDEN_UNITS: batch_size,\n","                HP_EMBEDDING_DIM: embedding_dim,\n","                HP_LEARNING_RATE: learning_rate\n","              }\n","            predictions_dataframe_three=create_predictions_df(model_three, save_index_of_numbers, \"rmsprop_models_20072020\", \"rmsprop\", hparams_rmsprop)\n","predictions_dataframe_three"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOyM7WQV7B7j"},"source":["print(predictions_dataframe_three.to_latex(index=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Qzfd-Ca7B7j"},"source":["#### Comparison 5: Training and Validation plots"]},{"cell_type":"markdown","metadata":{"id":"7pmRHrBF7B7j"},"source":["Before creating the learning curves for each of the three best model estimators, some examples of underfitting and overfitting learning curves are presents with random data."]},{"cell_type":"code","metadata":{"id":"Gb91rzi57B7j"},"source":["# Example 1-Underfitting Learning Curve part 1\n","\n","fig1=go.Figure()\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[0.005, 0.003, 0.001, 0.0009, 0.0007,  0.0005],\n","                          mode='lines+markers',\n","                          name=\"Train\",\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[0.05, 0.055, 0.065, 0.068, 0.07, 0.075],\n","                          mode='lines+markers',\n","                          name=\"Validation\",\n","                          line=dict(color='rgb(252, 141, 98)')))\n","\n","fig1.update_layout(template=\"simple_white\",\n","                   title=\"Loss - Underfitting learning curve\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","fig1.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Et1h2YsP7B7k"},"source":["# Example 2-Underfitting Learning Curve part 2\n","\n","fig1=go.Figure()\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[1.075, 1.071, 1.066, 1.061, 1.057, 1.037],\n","                          mode='lines+markers',\n","                          name=\"Train\",\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[1.065, 1.061, 1.056, 1.051, 1.047, 1.027],\n","                          mode='lines+markers',\n","                          name=\"Validation\",\n","                          line=dict(color='rgb(252, 141, 98)')))\n","\n","fig1.update_layout(template=\"simple_white\",\n","                   title=\"Loss - Underfitting learning curve\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","fig1.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xFZsB1O7B7k"},"source":["# Example 3-Underfitting Learning Curve\n","\n","fig1=go.Figure()\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[1.075, 0.85, 0.45, 0.35, 0.25, 0.05],\n","                          mode='lines+markers',\n","                          name=\"Train\",\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[1.005, 0.75, 0.51, 0.28, 0.38, 0.49],\n","                          mode='lines+markers',\n","                          name=\"Validation\",\n","                          line=dict(color='rgb(252, 141, 98)')))\n","\n","fig1.update_layout(template=\"simple_white\",\n","                   title=\"Loss - Overfitting learning curve\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","fig1.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEhgBiOn7B7k"},"source":["# Example 4-Good Fit Learning Curves\n","\n","fig1=go.Figure()\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[1.075, 0.85, 0.45, 0.35, 0.25, 0.05],\n","                          mode='lines+markers',\n","                          name=\"Train\",\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n","                          y=[1.069, 0.82, 0.43, 0.32, 0.22, 0.03],\n","                          mode='lines+markers',\n","                          name=\"Validation\",\n","                          line=dict(color='rgb(252, 141, 98)')))\n","\n","fig1.update_layout(template=\"simple_white\",\n","                   title=\"Loss - Good fit learning curve\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","fig1.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T99TUD3P7B7k"},"source":["Start of the pre-final step of the selection plan | Training-Validation Accuracy/Loss Learning Curves"]},{"cell_type":"markdown","metadata":{"id":"CCl8dihu7B7k"},"source":["**Hamming Loss performance models**"]},{"cell_type":"code","metadata":{"id":"tt73rzLn7B7k"},"source":["colormin = 'black'\n","colormax = 'black'\n","colorother = 'rgb(252, 141, 98)'\n","\n","clrs_acc_model_adam = [colormax if history_dataframe_one.val_hamming_loss.iloc[row]==history_dataframe_one.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_one.val_hamming_loss))]\n","clrs_acc_model_sgd = [colormax if history_dataframe_two.val_hamming_loss.iloc[row]==history_dataframe_two.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_two.val_hamming_loss))]\n","clrs_acc_model_rmsprop = [colormax if history_dataframe_three.val_hamming_loss.iloc[row]==history_dataframe_three.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_three.val_hamming_loss))]\n","\n","#Hamming Loss of Adam optimizer model\n","fig1=go.Figure()\n","\n","fig1.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.hamming_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Hamming Loss',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig1.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.val_hamming_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Hamming Loss',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_acc_model_adam)))\n","\n","fig1.update_layout(template=\"simple_white\",\n","                   title=\"Hamming Loss score on train & validation sets (Model estimator of the Adam Optimizer)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Hamming Loss/epoch\")\n","\n","fig1.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_hamming_loss==history_dataframe_one.val_hamming_loss.min()].tolist()[0],\n","                                     y=history_dataframe_one.val_hamming_loss.min(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the highest validation Hamming Loss\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=40)])\n","\n","fig1.update_layout(legend_title_text='Training & Validation Hamming Loss points per epoch')\n","\n","fig1.show()\n","#---------------------------------------------------------\n","\n","#Hamming Loss of SGD optimizer model\n","\n","fig2=go.Figure()\n","\n","fig2.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n","                          y=history_dataframe_two.hamming_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Hamming Loss',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig2.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n","                          y=history_dataframe_two.val_hamming_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Hamming Loss',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_acc_model_sgd)))\n","\n","fig2.update_layout(template=\"simple_white\",\n","                   title=\"Hamming Loss score on train & validation sets (Model estimator of the SGD Optimizer)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Hamming Loss/epoch\")\n","\n","fig2.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_two.epoch[history_dataframe_two.val_hamming_loss==history_dataframe_two.val_hamming_loss.min()].tolist()[0],\n","                                     y=history_dataframe_two.val_hamming_loss.min(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the highest validation Hamming Loss\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=40)])\n","\n","fig2.update_layout(legend_title_text='Training & Validation Hamming Loss points per epoch')\n","\n","fig2.show()\n","\n","#---------------------------------------------------------\n","\n","#Hamming Loss of RMSprop optimizer model\n","\n","fig3=go.Figure()\n","\n","fig3.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(),\n","                          y=history_dataframe_three.hamming_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Hamming Loss',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig3.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n","                          y=history_dataframe_three.val_hamming_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Hamming Loss',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_acc_model_rmsprop)))\n","\n","fig3.update_layout(template=\"simple_white\",\n","                   title=\"Hamming Loss score on train & validation sets (Model estimator of the RMSprop Optimizer)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Hamming Loss/epoch\")\n","\n","fig3.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_three.epoch[history_dataframe_three.val_hamming_loss==history_dataframe_three.val_hamming_loss.min()].tolist()[0],\n","                                     y=history_dataframe_three.val_hamming_loss.min(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the highest validation Hamming Loss\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=40)])\n","\n","fig3.update_layout(legend_title_text='Training & Validation Hamming Loss points per epoch')\n","\n","fig3.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"JCkoShfp7B7k"},"source":["colormin = 'black'\n","colorother = 'rgb(252, 141, 98)'\n","\n","clrs_loss_model_adam=[colormin if history_dataframe_one.val_loss.iloc[row]==history_dataframe_one.val_loss.min() else colorother for row in range(len(history_dataframe_one.val_loss))]\n","clrs_loss_model_sgd=[colormin if history_dataframe_two.val_loss.iloc[row]==history_dataframe_two.val_loss.min() else colorother for row in range(len(history_dataframe_two.val_loss))]\n","clrs_loss_model_rmsprop=[colormin if history_dataframe_three.val_loss.iloc[row]==history_dataframe_three.val_loss.min() else colorother for row in range(len(history_dataframe_three.val_loss))]\n","\n","#Loss of Adam optimizer model\n","\n","fig3=go.Figure()\n","\n","fig3.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Loss',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig3.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.val_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Loss',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_loss_model_adam)))\n","\n","fig3.update_layout(template=\"simple_white\",\n","                   title=\"Loss score on train & validation sets (Model estimator of the Adam Optimizer)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","\n","fig3.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_loss==history_dataframe_one.val_loss.min()].tolist()[0],\n","                                     y=history_dataframe_one.val_loss.min(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the lowest validation loss\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=-40)])\n","fig3.show()\n","\n","#---------------------------------------------------------\n","\n","#Loss of SGD optimizer model\n","\n","fig4=go.Figure()\n","\n","fig4.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n","                          y=history_dataframe_two.loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Loss',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig4.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n","                          y=history_dataframe_two.val_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Loss',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_loss_model_sgd)))\n","\n","fig4.update_layout(template=\"simple_white\",\n","                   title=\"Loss score on train & validation sets (Model estimator of the SGD Optimizer)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","\n","fig4.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_two.epoch[history_dataframe_two.val_loss==history_dataframe_two.val_loss.min()].tolist()[0],\n","                                     y=history_dataframe_two.val_loss.min(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the lowest validation loss\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=-65)])\n","fig4.show()\n","\n","#---------------------------------------------------------\n","\n","#Loss of RMSprop optimizer model\n","\n","fig5=go.Figure()\n","\n","fig5.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n","                          y=history_dataframe_three.loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Loss',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig5.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n","                          y=history_dataframe_three.val_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Loss',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_loss_model_rmsprop)))\n","\n","fig5.update_layout(template=\"simple_white\",\n","                   title=\"Loss score on train & validation sets (Model estimator of the RMSprop Optimizer)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","\n","fig5.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_three.epoch[history_dataframe_three.val_loss==history_dataframe_three.val_loss.min()].tolist()[0],\n","                                     y=history_dataframe_three.val_loss.min(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the lowest validation loss\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=-65)])\n","fig5.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"xJCarxPl7B7l"},"source":["def visualize_model(model):\n","    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65,).create(prog='dot', format='svg'))\n","visualize_model(model_one)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3To1oCxT7B7l"},"source":["tf.keras.utils.plot_model(\n","model_one,\n","to_file=\"model.png\",\n","show_shapes=True,\n","show_layer_names=True,\n","rankdir=\"TB\",\n","expand_nested=False,\n","dpi=96,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TgaBWs9E7B7l"},"source":["<b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"]},{"cell_type":"markdown","metadata":{"id":"BM1x4Whj7B7l"},"source":["Model estimator trained on binary accuracy performance metric"]},{"cell_type":"code","metadata":{"id":"e00YfJcA7B7l"},"source":["saved_version_data_control=\"22042020\"\n","\n","X_train_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_actors_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","X_train_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_plot_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","X_train_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_features_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","X_train_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_reviews_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","\n","print(\"X_train data inputs have been loaded!\\n\")\n","\n","X_test_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_actors_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","X_test_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_plot_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","X_test_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_features_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","X_test_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_reviews_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","\n","print(\"X_test data inputs have been loaded!\\n\")\n","\n","y_train=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\y_train_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","y_test=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\y_test_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n","\n","print(\"y_train & y_test have been loaded!\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"puOuc4uR7B7l"},"source":["\"\"\"\n","Import the tokenizers of each input, fitted on part 3.1\n","\"\"\"\n","with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\actors_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n","    actors_tokenizer = pickle.load(f)\n","    \n","with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\plot_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n","    plot_tokenizer = pickle.load(f)\n","    \n","with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\features_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n","    features_tokenizer = pickle.load(f)\n","    \n","with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\reviews_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n","    reviews_tokenizer = pickle.load(f)\n","\n","try:\n","    assert len(actors_tokenizer.word_index)==20000\n","    assert len(plot_tokenizer.word_index)==20000\n","    assert len(features_tokenizer.word_index)==20000\n","    assert len(reviews_tokenizer.word_index)==20000\n","except AssertionError:\n","    print(\"ERROR: The vocabulary length for some of the tokenizers, is not equal to 20000. Please verify their lengths by running the following: len(actors_tokenizer.word_index)\")\n","\n","print(\"Tokenizers are loaded successfully!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qKMPtgE7B7l"},"source":["\"\"\"\n","Import the X_train, X_test, y_train & y_test data pickled from dataset part 3.1\n","\"\"\"\n","saved_version_data_control=\"22042020\"\n","\n","X_train=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\X_train_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n","X_test=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\X_test_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n","y_train=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\y_train_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n","y_test=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\y_test_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n","\n","assert X_train.shape[0]==y_train.shape[0]\n","assert X_test.shape[0]==y_test.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O16689Pp7B7m"},"source":["neural_network_parameters={}\n","optimizer_parameters={}\n","\n","neural_network_parameters['model_loss'] = 'binary_crossentropy'\n","neural_network_parameters['model_metric'] = 'accuracy'\n","validation_split_ratio=0.8\n","\n","def optimizer_adam_v2_accuracy(batch_size_value):\n","\n","    optimizer_parameters['steps_per_epoch'] = int(np.ceil((X_train_seq_features.shape[0]*validation_split_ratio)//batch_size_value))\n","    optimizer_parameters['lr_schedule_learning_rate'] = 0.01\n","    optimizer_parameters['lr_schedule_decay_steps'] = optimizer_parameters['steps_per_epoch']*1000\n","    optimizer_parameters['lr_schedule_decay_rate'] = 1\n","    optimizer_parameters['staircase'] = False\n","    \n","    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n","        optimizer_parameters['lr_schedule_learning_rate'],\n","        decay_steps=optimizer_parameters['lr_schedule_decay_steps'],\n","        decay_rate=optimizer_parameters['lr_schedule_decay_rate'],\n","        staircase=optimizer_parameters['staircase'])\n","    \n","    return keras.optimizers.Adam(lr_schedule)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0R4jCk237B7m"},"source":["#### Model 16 - 300 embedding dimenstion & 128 batch size"]},{"cell_type":"code","metadata":{"id":"CNc9d7rY7B7m"},"source":["with open(os.path.join(os.getcwd(), 'model_one\\\\adam_v2_models_22042020\\\\multi_input_keras_model_{0}dim_{1}batchsize_{2}lr_{3}decaymultiplier_22042020.json'.format(str(300), str(128), str(0.01), str(1000))),'r') as f:\n","    model_json = json.load(f)\n","\n","model_sixteen = model_from_json(model_json)\n","\n","model_sixteen.load_weights(os.path.join(os.getcwd(), 'model_one\\\\adam_v2_models_22042020\\\\multi_input_keras_model_{0}dim_{1}batchsize_{2}lr_{3}decaymultiplier_22042020.h5'.format(str(300), str(128), str(0.01), str(1000))))\n","\n","model_sixteen.compile(optimizer=optimizer_adam_v2_accuracy(128),\n","                      loss=neural_network_parameters['model_loss'],\n","                      metrics=[neural_network_parameters['model_metric']])\n","\n","print(type(model_sixteen))\n","print(\"\\nModel is loaded successfully\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Miuos2NP7B7m"},"source":["df_scores_adam_accuracy=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_22042020\\\\df_metrics_multy_input_keras_300dim_128batchsize_26052020.pkl\"))\n","df_scores_adam_accuracy['Bias'], df_scores_adam_accuracy['Variance']=[-0.00047,0.096426]\n","df_scores_adam_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V1GFS79S7B7m"},"source":["print(df_scores_adam_accuracy.to_latex(index=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8ttYzjS7B7m"},"source":["history_dataframe_one=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_22042020\\\\metrics_histogram_multi_input_keras_300dim_128batchsize_22042020.pkl\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LHtLb1g7B7m"},"source":["colormax = 'black'\n","colorother = 'rgb(252, 141, 98)'\n","clrs_acc_model = [colormax if history_dataframe_one.val_accuracy.iloc[row]==history_dataframe_one.val_accuracy.max() else colorother for row in range(len(history_dataframe_one.val_accuracy))]\n","\n","clrs_loss=[colormin if history_dataframe_one.val_loss.iloc[row]==history_dataframe_one.val_loss.min() else colorother for row in range(len(history_dataframe_one.val_loss))]\n","\n","#Accuracy of model four\n","fig11=go.Figure()\n","\n","fig11.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.accuracy.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Accuracy',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig11.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.val_accuracy.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Accuracy',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_acc_model)))\n","\n","fig11.update_layout(template=\"simple_white\",\n","                   title=\"Accuracy score on train & validation sets (adam model accuracy)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Accuracy/epoch\")\n","\n","fig11.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_accuracy==history_dataframe_one.val_accuracy.max()].tolist()[0],\n","                                     y=history_dataframe_one.val_accuracy.max(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the highest validation accuracy\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=40)])\n","fig11.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pcB-yNI_7B7n"},"source":["colormin = 'black'\n","colorother = 'rgb(252, 141, 98)'\n","\n","clrs_loss_model=[colormin if history_dataframe_one.val_loss.iloc[row]==history_dataframe_one.val_loss.min() else colorother for row in range(len(history_dataframe_one.val_loss))]\n","\n","#Loss of model four\n","\n","fig12=go.Figure()\n","\n","fig12.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Training Loss',\n","                          line=dict(color='rgb(102, 194, 165)')))\n","\n","fig12.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n","                          y=history_dataframe_one.val_loss.tolist(),\n","                          mode='lines+markers',\n","                          name='Validation Loss',\n","                          line=dict(color='rgb(252, 141, 98)'),\n","                          marker=dict(color=clrs_loss_model)))\n","\n","fig12.update_layout(template=\"simple_white\",\n","                   title=\"Loss score on train & validation sets (adam model trained on accuracy)\",\n","                   xaxis_title=\"Number of epochs\",\n","                   yaxis_title=\"Loss/epoch\")\n","\n","fig12.update_layout(showlegend=True,\n","                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_loss==history_dataframe_one.val_loss.min()].tolist()[0],\n","                                     y=history_dataframe_one.val_loss.min(),\n","                                     xref=\"x\",yref=\"y\",\n","                                     text=\"Epoch with the lowest validation loss\",\n","                                     showarrow=True,\n","                                     arrowhead=5,\n","                                     ax=0,ay=-40)])\n","fig12.show()"],"execution_count":null,"outputs":[]}]}