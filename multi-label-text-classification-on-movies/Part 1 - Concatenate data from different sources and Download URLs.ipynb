{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Concatenate data from different sources and Download URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning and preparing the dataset\n",
    "# -> dataframe manipulation\n",
    "# -> text manipulation\n",
    "# -> Web Scrapping\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "# Example of Use: tqdm_notebook(examples, desc=\"Converting examples to features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the dataset that will join the information found in csv of links and movie title of Grouplens research & development database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Each of the four datasets has a different number of rows. As a result, they cannot be concatenated at once. \n",
    "In order, to concatenate them, we should bring them to a number of rows equal to the number of movies in the dataset. \n",
    "The only dataframe with those number of rows is the \"movies.csv\" containing the title and the genre(s) of each movie.\n",
    "\"\"\"\n",
    "dataset_links = pd.read_csv(os.path.join(os.getcwd(), \"ml-latest//links.csv\"))\n",
    "dataset_movie_names = pd.read_csv(os.path.join(os.getcwd(), \"ml-latest//movies.csv\"))\n",
    "dataset_movie_ranks = pd.read_csv(os.path.join(os.getcwd(), \"ml-latest//ratings.csv\"))\n",
    "dataset_movie_tags = pd.read_csv(os.path.join(os.getcwd(), \"ml-latest//tags.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_movie_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_movie_ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_movie_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below we can see the ratings of a movie given by different users. Thus, in order to have a single user rating for a movie-\n",
    "we should find the average value per movie.\n",
    "\"\"\"\n",
    "dataset_movie_ranks[dataset_movie_ranks.movieId == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the average rating per movie. We get 53889 ratings although we have 58098 movies. This means that 4209 movies-\n",
    "did not recieve any user rating. Thus, those movie's rating will be 0.\n",
    "\"\"\"\n",
    "list_sums = dataset_movie_ranks.groupby(['movieId'])['rating'].sum().tolist()\n",
    "list_count = dataset_movie_ranks.groupby(['movieId'])['rating'].count().tolist()\n",
    "\n",
    "list_final_rating = [round((x*1.0)/y,2) for x, y in zip(list_sums, list_count)]\n",
    "len(list_final_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Left join the datasets containing movie titles and user ratings. THe dataset_rating includes information about movie title, genre and user rating for a single movie\n",
    "\"\"\"\n",
    "dataset_rating = pd.merge(dataset_movie_names, round(dataset_movie_ranks.groupby(['movieId'])['rating'].sum()/dataset_movie_ranks.groupby(['movieId'])['rating'].count(),2), on='movieId', how='left')\n",
    "\n",
    "# As already mentioned, the rating of non-rated movies will be zero.\n",
    "dataset_rating['rating'].fillna(0, inplace=True)\n",
    "\n",
    "dataset_rating.head(15)\n",
    "\n",
    "print('\\n', dataset_rating.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Last but not least, join the above dataset with the dataset containing the movie links so as to generate the urls that will help us -\n",
    "to download the content, the synopsis and the reviews for a single movie.\n",
    "\"\"\"\n",
    "dataset = pd.concat([dataset_links, dataset_rating], axis=1, join='outer')\n",
    "\n",
    "dataset = dataset.loc[:,~dataset.columns.duplicated()]\n",
    "\n",
    "dataset = dataset.drop(['tmdbId'], axis=1)\n",
    "\n",
    "print(dataset.columns, '\\n')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finally, since our problem is a multilabel classification one, we should bring the y_label, in this case the genres, to a list of multiple genres.\n",
    "To do so, we replaced the symbol \"|\", with the comma punctuation and then splitted the string.\n",
    "\"\"\"\n",
    "dataset['genres'] = dataset['genres'].apply(lambda x: x.replace('|', ','))\n",
    "\n",
    "dataset['genres'] = dataset['genres'].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "for i in tqdm_notebook(range(len(dataset['genres']))):\n",
    "    if len(dataset['genres'].iloc[i]) > 3:\n",
    "        dataset['genres'].iloc[i] = dataset['genres'].iloc[i][0:3]\n",
    "    elif len(dataset['genres'].iloc[i]) <= 3:\n",
    "        dataset['genres'].iloc[i] = dataset['genres'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As already informed by the Grouplens website, the number of total movies is equal to 58098.\n",
    "\"\"\"\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "No null values in the ratings column.\n",
    "\"\"\"\n",
    "dataset.rating.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Links of movie features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to create a valid hyper reference link that we could later use for data extraction, we should apply to the column \"imdbId\",\n",
    "the following string:\n",
    "\n",
    "\"http://www.imdb.com/title/tt\" plus (+) a number of 0's depending on the imbdId of the movie.\n",
    "\n",
    "Prior deciding to create the online links, we observed that every imdb page of a movie had the above http protocol reference incommon.\n",
    "\"\"\"\n",
    "\n",
    "imdb_url = []\n",
    "\n",
    "imdb_ids = dataset['imdbId'].values.tolist()\n",
    "\n",
    "for i in tqdm_notebook(range(len(imdb_ids))):\n",
    "\n",
    "    if len(str(dataset['imdbId'].iloc[i])) == 7:\n",
    "        \n",
    "        imdb_url.append(\"http://www.imdb.com/title/tt\" + str(dataset['imdbId'].iloc[i]) + \"/\")\n",
    "\n",
    "    elif len(str(dataset['imdbId'].iloc[i])) == 6:\n",
    "\n",
    "        imdb_url.append(\"http://www.imdb.com/title/tt0\" + str(dataset['imdbId'].iloc[i]) + \"/\")\n",
    "        \n",
    "    elif len(str(dataset['imdbId'].iloc[i])) == 5:\n",
    "        \n",
    "        imdb_url.append(\"http://www.imdb.com/title/tt00\" + str(dataset['imdbId'].iloc[i]) + \"/\")\n",
    "        \n",
    "    elif len(str(dataset['imdbId'].iloc[i])) == 4:\n",
    "        \n",
    "        imdb_url.append(\"http://www.imdb.com/title/tt000\" + str(dataset['imdbId'].iloc[i]) + \"/\")\n",
    "        \n",
    "    elif len(str(dataset['imdbId'].iloc[i])) == 3:\n",
    "        \n",
    "        imdb_url.append(\"http://www.imdb.com/title/tt0000\" + str(dataset['imdbId'].iloc[i]) + \"/\")\n",
    "    \n",
    "    elif len(str(dataset['imdbId'].iloc[i])) == 2:\n",
    "        \n",
    "        imdb_url.append(\"http://www.imdb.com/title/tt00000\" + str(dataset['imdbId'].iloc[i]) + \"/\")\n",
    "        \n",
    "    elif len(str(dataset['imdbId'].iloc[i])) == 1:\n",
    "        \n",
    "        imdb_url.append(\"http://www.imdb.com/title/tt000000\" + str(dataset['imdbId'].iloc[i]) + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pandas module can create a new dataframe column using the content of a python list. Thus, we created the column \"imdb_url\",\n",
    "based on the list created on the previous cell.\n",
    "\"\"\"\n",
    "dataset['imdb_url'] = imdb_url\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Having the main movie page ready to download, we then created the custom urls to access the synopsis page for each movie,\n",
    "since we observed that a common pattern was followed like in the urls of the main movie page.\n",
    "\n",
    "Pattern:\"plotsummary?ref_=tt_stry_pl#synopsis\" after the imdb_url\n",
    "\"\"\"\n",
    "dataset[\"synopsis_url\"] = dataset[\"imdb_url\"].apply(lambda x: x + \"plotsummary?ref_=tt_stry_pl#synopsis\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The reviews url for a movie createed with the same procedure as the synopsis url.\n",
    "\n",
    "Pattern: \"reviews?spoiler=hide&sort=helpfulnessScore&dir=desc&ratingFilter=0\"\n",
    "\"\"\"\n",
    "dataset[\"reviews_url\"] = dataset[\"imdb_url\"].apply(lambda x: x + \"reviews?spoiler=hide&sort=helpfulnessScore&dir=desc&ratingFilter=0\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the final dataset containing the:\n",
    "\n",
    "1) movieId,\n",
    "2) imdbId,\n",
    "3) title,\n",
    "4) genres.\n",
    "5) rating (user rating)\n",
    "6) imdb_url (main page of the movie on IMDB website)\n",
    "7) sysopsis_url (web-page containing plot summary & plot sunopsis of a movie)\n",
    "8) reviews_url (web-page containing the top n reviews of a movie). We did not include all the reviews, only those contained in the first HTML page\n",
    "\"\"\"\n",
    "#dataset.to_pickle('dataset_58,000_14012020_latest_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "No null values accross the 8 columns and 58098 rows.\n",
    "\"\"\"\n",
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_58,000_14012020_latest_version.pkl\"))\n",
    "print(dataset.isnull().sum(), '\\n')\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the online HTML docs for each movie. The online web-pages will contain content of:\n",
    "\n",
    "    1) Movie content,\n",
    "    2) Plot synopsis content,\n",
    "    3) User reviews content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the first five thousand movies (58098-5000 = 53098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_one = dataset.imdb_url.iloc[:5000]\n",
    "\n",
    "list_one = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_one, desc = \"Extract the content of the first 5000 movies\"):\n",
    "    \n",
    "    list_one.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_one = dataset['synopsis_url'].iloc[:5000]\n",
    "\n",
    "synopsis_one = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_one, desc = \"Extract the plot synopsis of the first 5000 movies\"):\n",
    "    \n",
    "    synopsis_one.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_one = dataset['reviews_url'].iloc[:5000]\n",
    "\n",
    "reviews_one = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_one, desc = \"Extract the user reviews of the first 5000 movies\"):\n",
    "    \n",
    "    reviews_one.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the second five thousand movies (53098-5000 = 48098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_two = dataset.imdb_url.iloc[5000:10000]\n",
    "\n",
    "list_two = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_two, desc = \"Extract the content of the second 5000 movies (10,000)\"):\n",
    "    \n",
    "    list_two.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_two = dataset['synopsis_url'].iloc[5000:10000]\n",
    "\n",
    "synopsis_two = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_two, desc = \"Extract the plot synopsis of the second 5000 movies (10,000)\"):\n",
    "    \n",
    "    synopsis_two.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_two = dataset['reviews_url'].iloc[5000:10000]\n",
    "\n",
    "reviews_two = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_two, desc = \"Extract the user reviews of the second 5000 movies (10,000)\"):\n",
    "    \n",
    "    reviews_two.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the third five thousand movies (48098-5000 = 43098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_three = dataset.imdb_url.iloc[10000:15000]\n",
    "\n",
    "list_three = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_three, desc = \"Extract the content of the third 5000 movies (15,000)\"):\n",
    "    \n",
    "    list_three.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_three = dataset['synopsis_url'].iloc[10000:15000]\n",
    "\n",
    "synopsis_three = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_three, desc = \"Extract the plot synopsis of the third 5000 movies (15,000)\"):\n",
    "    \n",
    "    synopsis_three.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_three = dataset['reviews_url'].iloc[10000:15000]\n",
    "\n",
    "reviews_three = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_three, desc = \"Extract the user reviews of the third 5000 movies (15,000)\"):\n",
    "    \n",
    "    reviews_three.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the fourth five thousand movies (43098-5000 = 38098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_four = dataset.imdb_url.iloc[15000:20000]\n",
    "\n",
    "list_four = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_four, desc = \"Extract the content of the fourth 5000 movies (20,000)\"):\n",
    "    \n",
    "    list_four.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_four = dataset['synopsis_url'].iloc[15000:20000]\n",
    "\n",
    "synopsis_four = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_four, desc = \"Extract the plot synopsis of the fourth 5000 movies (20,000)\"):\n",
    "    \n",
    "    synopsis_four.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_four = dataset['reviews_url'].iloc[15000:20000]\n",
    "\n",
    "reviews_four = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_four, desc = \"Extract the user reviews of the fourth 5000 movies (20,000)\"):\n",
    "    \n",
    "    reviews_four.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the fifth five thousand movies (38098-5000 = 33098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_five = dataset.imdb_url.iloc[20000:25000]\n",
    "\n",
    "list_five = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_five, desc = \"Extract the content of the fifth 5000 movies (25,000)\"):\n",
    "    \n",
    "    list_five.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_five = dataset['synopsis_url'].iloc[20000:25000]\n",
    "\n",
    "synopsis_five = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_five, desc = \"Extract the plot synopsis of the fifth 5000 movies (25,000)\"):\n",
    "    \n",
    "    synopsis_five.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_five = dataset['reviews_url'].iloc[20000:25000]\n",
    "\n",
    "reviews_five = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_five, desc = \"Extract the user reviews of the fifth 5000 movies (25,000)\"):\n",
    "    \n",
    "    reviews_five.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the sixth five thousand movies (33098-5000 = 28098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_six = dataset.imdb_url.iloc[25000:30000]\n",
    "\n",
    "list_six = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_six, desc = \"Extract the content of the sixth 5000 movies (30,000)\"):\n",
    "    \n",
    "    list_six.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_six = dataset['synopsis_url'].iloc[25000:30000]\n",
    "\n",
    "synopsis_six = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_six, desc = \"Extract the plot synopsis of the sixth 5000 movies (30,000)\"):\n",
    "    \n",
    "    synopsis_six.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_six = dataset['reviews_url'].iloc[25000:30000]\n",
    "\n",
    "reviews_six = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_six, desc = \"Extract the user reviews of the sixth 5000 movies (30,000)\"):\n",
    "    \n",
    "    reviews_six.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the seventh five thousand movies (28098-5000 = 23098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_seven = dataset.imdb_url.iloc[30000:35000]\n",
    "\n",
    "list_seven = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_seven, desc = \"Extract the content of the seventh 5000 movies (35,000)\"):\n",
    "    \n",
    "    list_seven.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_seven = dataset['synopsis_url'].iloc[30000:35000]\n",
    "\n",
    "synopsis_seven = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_seven, desc = \"Extract the plot synopsis of the seventh 5000 movies (35,000)\"):\n",
    "    \n",
    "    synopsis_seven.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_seven = dataset['reviews_url'].iloc[30000:35000]\n",
    "\n",
    "reviews_seven = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_seven, desc = \"Extract the user reviews of the seventh 5000 movies (35,000)\"):\n",
    "    \n",
    "    reviews_seven.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the eigth five thousand movies (23098-5000 = 18098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_eight = dataset.imdb_url.iloc[35000:40000]\n",
    "\n",
    "list_eight = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_eight, desc = \"Extract the content of the eighth 5000 movies (40,000)\"):\n",
    "    \n",
    "    list_eight.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_eight = dataset['synopsis_url'].iloc[35000:40000]\n",
    "\n",
    "synopsis_eight = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_eight, desc = \"Extract the plot synopsis of the eighth 5000 movies (40,000)\"):\n",
    "    \n",
    "    synopsis_eight.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_eight = dataset['reviews_url'].iloc[35000:40000]\n",
    "\n",
    "reviews_eight = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_eight, desc = \"Extract the user reviews of the eighth 5000 movies (40,000)\"):\n",
    "    \n",
    "    reviews_eight.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the ninth five thousand movies (18098-5000 = 13098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_nine = dataset.imdb_url.iloc[40000:45000]\n",
    "\n",
    "list_nine = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_nine, desc = \"Extract the content of the ninth 5000 movies (45,000)\"):\n",
    "    \n",
    "    list_nine.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_nine = dataset['synopsis_url'].iloc[40000:45000]\n",
    "\n",
    "synopsis_nine = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_nine, desc = \"Extract the plot synopsis of the ninth 5000 movies (45,000)\"):\n",
    "    \n",
    "    synopsis_nine.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_nine = dataset['reviews_url'].iloc[40000:45000]\n",
    "\n",
    "reviews_nine = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_nine, desc = \"Extract the user reviews of the ninth 5000 movies (45,000)\"):\n",
    "    \n",
    "    reviews_nine.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the tenth five thousand movies (13098-5000 = 8098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_ten = dataset.imdb_url.iloc[45000:50000]\n",
    "\n",
    "list_ten = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_ten, desc = \"Extract the content of the tenth 5000 movies (50,000)\"):\n",
    "    \n",
    "    list_ten.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_ten = dataset['synopsis_url'].iloc[45000:50000]\n",
    "\n",
    "synopsis_ten = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_ten, desc = \"Extract the plot synopsis of the tenth 5000 movies (50,000)\"):\n",
    "    \n",
    "    synopsis_ten.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_ten = dataset['reviews_url'].iloc[45000:50000]\n",
    "\n",
    "reviews_ten = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_ten, desc = \"Extract the user reviews of the tenth 5000 movies (50,000)\"):\n",
    "    \n",
    "    reviews_ten.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the eleventh five thousand movies (8098-5000 = 3098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_eleven = dataset.imdb_url.iloc[50000:55000]\n",
    "\n",
    "list_eleven = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_eleven, desc = \"Extract the content of the eleventh 5000 movies (55,000)\"):\n",
    "    \n",
    "    list_eleven.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_eleven = dataset['synopsis_url'].iloc[50000:55000]\n",
    "\n",
    "synopsis_eleven = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_eleven, desc = \"Extract the plot synopsis of the eleventh 5000 movies (55,000)\"):\n",
    "    \n",
    "    synopsis_eleven.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_eleven = dataset['reviews_url'].iloc[50000:55000]\n",
    "\n",
    "reviews_eleven = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_eleven, desc = \"Extract the user reviews of the eleventh 5000 movies (55,000)\"):\n",
    "    \n",
    "    reviews_eleven.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the twelve five thousand movies (3098-3098 = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie content (column: imdb_url)\n",
    "\"\"\"\n",
    "dataset_twelve = dataset.imdb_url.iloc[55000:]\n",
    "\n",
    "list_twelve = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_twelve, desc = \"Extract the content of the remainig 3098 movies (58,098)\"):\n",
    "    \n",
    "    list_twelve.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the movie synopsis (column: synopsis_url)\n",
    "\"\"\"\n",
    "dataset_synopsis_twelve = dataset['synopsis_url'].iloc[55000:]\n",
    "\n",
    "synopsis_twelve = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_synopsis_twelve, desc = \"Extract the plot synopsis of the remainig 3098 movies (58,098)\"):\n",
    "    \n",
    "    synopsis_twelve.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the user reviews (column: reviews_url)\n",
    "\"\"\"\n",
    "dataset_reviews_twelve = dataset['reviews_url'].iloc[55000:]\n",
    "\n",
    "reviews_twelve = []\n",
    "\n",
    "for i in tqdm_notebook(dataset_reviews_twelve, desc = \"Extract the user reviews of the remainig 3098 movies (58,098)\"):\n",
    "    \n",
    "    reviews_twelve.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - - - - - - - - -  - - - - - - - - -  - - - - - - - - -  - - - - - - - - -  - - - - - - - - -  - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have extracted the online HTML docs for each movie, we will proceed into the information extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to be shown next has been splitted into 12 different Jupyter Notebooks. Thus, we advise to have a look on the Notebooks from number 1.1 to 1.12 in order to cehck the information extraction.\n",
    "\n",
    "We extracted the following 6 fields:\n",
    "\n",
    "* field 1: Plot Summary (a short summary of the movies scenario)\n",
    "* field 2: Actors (15 or less number of actors/actresses in the cast)\n",
    "* field 3: Directors (Name of the director(s))\n",
    "* field 4: IMDB rating (The IMDB rate of the movie given by the users)\n",
    "* field 5: Plot Synopsis (The whole synopsis text of the movie)\n",
    "* field 6: Reviews (The first user reviews)\n",
    "\n",
    "To extract those 6 fields, we first transformed the extracted urls list to a beautiful soup object. This process consumed an important amount of time and RAM capacity. Thus, we propose to chech each notebook independently.\n",
    "\n",
    "After the transformation to a beautiful soup object each field is extracted with some for loops and the .find_all() method, which is an appropriate search tool of HTML tags (i.e /href string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - - - - - - - - -  - - - - - - - - -  - - - - - - - - -  - - - - - - - - -  - - - - - - - - -  - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having extracted the movie information we continue into the construction of the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the FINAL DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly, import the twelve dataset created as the output from each of the 12 Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_one = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movies_one\\\\dataset_one_final_25012020.pkl\"))\n",
    "data_two = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_two\\\\dataset_two_final_25012020.pkl\"))\n",
    "data_three = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_three\\\\dataset_three_final_25012020.pkl\"))\n",
    "data_four = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_four\\\\dataset_four_final_25012020.pkl\"))\n",
    "data_five = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_five\\\\dataset_five_final_25012020.pkl\"))\n",
    "data_six = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_six\\\\dataset_six_final_25012020.pkl\"))\n",
    "data_seven = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_seven\\\\dataset_seven_final_25012020.pkl\"))\n",
    "data_eight = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_eight\\\\dataset_eight_final_25012020.pkl\"))\n",
    "data_nine = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_nine\\\\dataset_nine_final_25012020.pkl\"))\n",
    "data_ten = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_ten\\\\dataset_ten_final_25012020.pkl\"))\n",
    "data_eleven = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_eleven\\\\dataset_eleven_final_25012020.pkl\"))\n",
    "data_twelve = pd.read_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\movie_twelve\\\\dataset_twelve_final_25012020.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check for NAs\n",
    "\"\"\"\n",
    "movie_tables_list=[data_one, data_two, data_three, data_four, data_five, data_six, data_seven, data_eight, data_nine, data_ten, data_eleven, data_twelve]\n",
    "for i,j in enumerate(movie_tables_list):\n",
    "    print(\"{0}/{1}\".format(i+1, len(movie_tables_list)))\n",
    "    print(j[j.isna().any(axis=1)])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Concatenate the 12-batch datasets together to form the final extracted movies table\n",
    "\"\"\"\n",
    "final_dataset = pd.concat([data_one, data_two, data_three, data_four, data_five, data_six, \n",
    "                           data_seven, data_eight, data_nine, data_ten, data_eleven, data_twelve], \n",
    "                          ignore_index=True, sort=False)\n",
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove the data with empty list of reviews\n",
    "\"\"\"\n",
    "final_dataset = final_dataset[final_dataset.astype(str)['reviews'] != '[]']\n",
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data with genre tags like \"(no genres listed)\" should be removed. Although, since those genres were not included by Grouplens, \n",
    "we decied to download them.\n",
    "\"\"\"\n",
    "final_dataset.genres.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The final dataset has 2849 movies with the following tag on the column genre \"(no genres listed)\".\n",
    "Thus, we isolated those 2849 movies and we downloaded their genres\n",
    "\"\"\"\n",
    "final_dataset[final_dataset.astype(str)['genres'] == \"['(no genres listed)']\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_no_genres = final_dataset[final_dataset.astype(str)['genres'] == \"['(no genres listed)']\"].reset_index(drop=True)\n",
    "movies_no_genres.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------- --------------------------------------\n",
    "Download the movies with no listed genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download the movie genres of those 2849 movies.\n",
    "\"\"\"\n",
    "genres_links = movies_no_genres.imdb_url\n",
    "genres_url_list = []\n",
    "for i in tqdm_notebook(genres_links):\n",
    "    \n",
    "    genres_url_list.append(requests.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pickle the downloaded list of HTML documents, because this task takes time to complete at every notebook execution\n",
    "\"\"\"\n",
    "# with open(os.path.join(os.getcwd(),'58,000 movies\\\\movies_with_no_genres_25012020.pkl'), 'wb') as f:\n",
    "#     pickle.dump(genres_url_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the above pickled file\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(),'58,000 movies\\\\movies_with_no_genres_25012020.pkl'), 'rb') as f:\n",
    "    genres_url_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the genres using Beautiful Soup\n",
    "\"\"\"\n",
    "souplist = []\n",
    "\n",
    "for i in tqdm_notebook(genres_url_list):\n",
    "    souplist.append(BeautifulSoup(i.text))\n",
    "print(len(souplist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfield_genres = []\n",
    "myfield_genres_final = []\n",
    "genres = []\n",
    "\n",
    "for i in tqdm_notebook(souplist):\n",
    "    myfield_genres.append(i.find_all('div', {'class':'see-more inline canwrap'}))\n",
    "\n",
    "myfield_genres_final = []\n",
    "\n",
    "for item in myfield_genres:\n",
    "    if len(item) == 2:\n",
    "        myfield_genres_final.append([item[1]])\n",
    "    elif len(item) == 1:\n",
    "        myfield_genres_final.append([item[0]])\n",
    "\n",
    "r_genres = re.compile(\"(?=genres)(.*)\")\n",
    "\n",
    "for i in tqdm_notebook(myfield_genres_final):\n",
    "    for j in i:\n",
    "        genres.append(j.find_all('a', {'href':r_genres}))\n",
    "\n",
    "genres_final = []\n",
    "for i in genres:\n",
    "    genres_final.append(list(map(lambda x: x.text.strip(' ').replace('\\n', ''), i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6 genre lists less, probably those 6 links never had a written genre.\n",
    "\"\"\"\n",
    "len(genres_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The 6 index movies with no genres should be removed.\n",
    "\"\"\"\n",
    "index_to_remove_no_genres = [i for i,x in enumerate(myfield_genres) if not x]\n",
    "index_to_remove_no_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_no_genres = movies_no_genres[~movies_no_genres.index.isin(index_to_remove_no_genres)]\n",
    "movies_no_genres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_no_genres.genres = genres_final\n",
    "movies_no_genres.genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if any of the genres download have an empty list.\n",
    "\"\"\"\n",
    "movies_no_genres[movies_no_genres.astype(str)['genres'] == \"['(no genres listed)']\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This step trims the movies with more than three genres from the \"movies_no_genres\" table.\n",
    "This is important to remember because the whole classification takes into account up to 3 movie genres for a single movie.\n",
    "Even though a particular movie could belong to 4, 5, 6 and  more genres.\n",
    "\"\"\"\n",
    "for i in tqdm_notebook(range(len(movies_no_genres['genres']))):\n",
    "    if len(movies_no_genres['genres'].iloc[i]) > 3:\n",
    "        movies_no_genres['genres'].iloc[i] = movies_no_genres['genres'].iloc[i][0:3]\n",
    "    elif len(movies_no_genres['genres'].iloc[i]) <= 3:\n",
    "        movies_no_genres['genres'].iloc[i] = movies_no_genres['genres'].iloc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------- --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have to append final_dataset with the movie_no_genres dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1st movie's table - The one created earlier out of the 12 mini tables\n",
    "\"\"\"\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2nd movie's table - The one that downloaded the sequences of genres for movies that had the tag \"(no genres listed)\"\n",
    "\"\"\"\n",
    "movies_no_genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Concatenate the final_dataset with the movies_no_genres dataset.In the latter, we include the genres of the movies with no genre listed, belonging to the former dataset.\n",
    "\"\"\"\n",
    "final_dataset_test = pd.concat([final_dataset, movies_no_genres], ignore_index=True, sort=False)\n",
    "final_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove movies with untagged genres\n",
    "\"\"\"\n",
    "final_dataset_test = final_dataset_test[final_dataset_test.astype(str)['genres'] != \"['(no genres listed)']\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The final dataset contain no movie untagged of genres.\n",
    "\"\"\"\n",
    "final_dataset_test[final_dataset_test.astype(str)['genres'] == \"['(no genres listed)']\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The shape of the final dataset\n",
    "\"\"\"\n",
    "final_dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_test.to_pickle(os.path.join(os.getcwd(),\"58,000 movies\\\\final_dataset_49393_movies_25012020.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of Part 1 - Update, clean & transfrom the dataset of movies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
