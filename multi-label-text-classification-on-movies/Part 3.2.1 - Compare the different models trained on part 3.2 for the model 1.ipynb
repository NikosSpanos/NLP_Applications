{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2.1 - Compare the different models trained on part 3.2 for the model 1\n",
    "\n",
    "The purpose of the notebook is to select the best model among the 16 models trained with the multi-input keras approach.\n",
    "The final model selected will be then compared to the rest of the models trained.\n",
    "\n",
    "To select the best model we used the following guidelines:\n",
    "\n",
    "* 1) The model with the lowest hamming loss & zero one loss\n",
    "* 2) The model with the lowest test score and the highest test accuracy values\n",
    "* 3) The model with the most accurate predictions among the 17 labels. It is of high importance the best model to identify correctly the most of the genre tags. Models that cannot identify more than 2 genre tags will not be prefered.\n",
    "* 4) Compare model predictions on movie never seen before.\n",
    "* 5) Training-Validation metrics comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the version data control parameter (to save the outputs of this notebook at their latest date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_data_control=\"16072020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "try:\n",
    "    collectionsAbc = collections.abc\n",
    "except AttributeError:\n",
    "    collectionsAbc = collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "import unidecode\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from packaging import version\n",
    "from humanfriendly import format_timespan\n",
    "from sklearn.metrics import confusion_matrix, classification_report, hamming_loss, zero_one_loss, f1_score, roc_auc_score\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "date_format='%Y-%m-%d %H-%M-%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improt visualization libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_docs as tfdocs #!pip install git+https://github.com/tensorflow/docs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "\n",
    "from tensorflow.keras import layers, regularizers, models\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.models import load_model, model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data already tokenized and transformed from Part 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80-20 split - Non-balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_version_data_control=\"13072020\"\n",
    "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
    "\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\words_tokenized_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control)), 'rb') as handle:\n",
    "    words_tokenized = pickle.load(handle)\n",
    "    \n",
    "words_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the saved tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMport the tokenizers of each input, fitted on part 3.1\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\actors_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\plot_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\features_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\reviews_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\title_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])),'rb') as f:\n",
    "    title_tokenizer = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    assert len(actors_tokenizer.word_index)==words_tokenized['actors_tokenized']\n",
    "    assert len(plot_tokenizer.word_index)==words_tokenized['plot_words_tokenized']\n",
    "    assert len(features_tokenizer.word_index)==words_tokenized['features_words_tokenized']\n",
    "    assert len(reviews_tokenizer.word_index)==words_tokenized['reviews_words_tokenized']\n",
    "    assert len(title_tokenizer.word_index)==words_tokenized['title_words_tokenized']\n",
    "except AssertionError:\n",
    "    print(\"ERROR: The vocabulary length for some of the tokenizers, is not equal to 20000. Please verify their lengths by running the following: len(actors_tokenizer.word_index)\")\n",
    "\n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_version_data_control=\"13072020\"\n",
    "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
    "\n",
    "X_train_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_actors_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])))\n",
    "X_train_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_plot_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])))\n",
    "X_train_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_features_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])))\n",
    "X_train_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_reviews_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])))\n",
    "X_train_seq_title=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_title_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])))\n",
    "\n",
    "print(\"X_train data inputs have been loaded!\\n\")\n",
    "\n",
    "X_test_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_actors_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])))\n",
    "X_test_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_plot_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])))\n",
    "X_test_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_features_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])))\n",
    "X_test_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_reviews_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])))\n",
    "X_test_seq_title=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_title_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])))\n",
    "\n",
    "print(\"X_test data inputs have been loaded!\\n\")\n",
    "\n",
    "y_train=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\y_train_80-20_non-balanced_{1}.npy\".format(tokenization_history_folder, saved_version_data_control)))\n",
    "y_test=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\y_test_80-20_non-balanced_{1}.npy\".format(tokenization_history_folder, saved_version_data_control)))\n",
    "\n",
    "print(\"y_train & y_test have been loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import genres\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\genres_list_06032020.pkl'.format(tokenization_history_folder, saved_version_data_control)),'rb') as f:\n",
    "    genres_list = pickle.load(f)\n",
    "genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_actors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise some predefined values first:\n",
    "\n",
    "* Set Optimization function\n",
    "* Model loss\n",
    "* Model metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize name variables\n",
    "\n",
    "saved_model_name=\"multi_input_keras_model\"\n",
    "folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n",
    "saved_df_scored_metric_name=\"df_metrics_multi_input_keras_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_parameters={}\n",
    "optimizer_parameters={}\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Model Compilation\n",
    "neural_network_parameters['model_loss'] = tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy') #'binary_crossentropy'\n",
    "neural_network_parameters['model_metric'] = [tfa.metrics.HammingLoss(mode=\"multilabel\", name=\"hamming_loss\"),\n",
    "                                             tfa.metrics.F1Score(y_train.shape[-1], average=\"micro\", name=\"f1_score_micro\"), \n",
    "                                             tfa.metrics.F1Score(y_train.shape[-1], average=None, name=\"f1_score_none\"),\n",
    "                                             tfa.metrics.F1Score(y_train.shape[-1], average=\"macro\", name=\"f1_score_macro\")]\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Function 1 - Optimizers\n",
    "\n",
    "# Optimizer: ADAM (Learning scheduler with Inverse Time Decay)\n",
    "\n",
    "optimizer_parameters['lr_scheduler_decay_rate'] = 0.1\n",
    "optimizer_parameters['staircase'] = False\n",
    "optimizer_parameters['validation_split_ratio']=0.7\n",
    "\n",
    "def optimizer_adam_v2(hparams):\n",
    "\n",
    "    return keras.optimizers.Adam(tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate=hparams[HP_LEARNING_RATE],\n",
    "        decay_steps=int(np.ceil((X_train_seq_actors.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS]))*hparams[HP_DECAY_STEPS_MULTIPLIER],\n",
    "        decay_rate=optimizer_parameters['lr_scheduler_decay_rate'],\n",
    "        staircase=optimizer_parameters['staircase']))\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Optimizer: SDG (version 1)\n",
    "\n",
    "optimizer_parameters['SGD_momentum'] = 0.2 #default 0.0\n",
    "optimizer_parameters['SGD_nesterov'] = True #default False\n",
    "\n",
    "def optimizer_sgd_v1(haparms, mode):\n",
    "\n",
    "    if mode==\"step decay\":\n",
    "\n",
    "        return keras.optimizers.SGD(lr=0.0, #Notice that we set the learning rate in the SGD class to 0 to clearly indicate that it is not used.\n",
    "                                    momentum=0.9 #Use a large momentum. Using a larger momentum value will help the optimization algorithm to continue to make updates in the right direction when your learning rate shrinks to small values.\n",
    "                                   )\n",
    "    else:\n",
    "        return keras.optimizers.SGD(lr=hparams[HP_LEARNING_RATE],\n",
    "                              momentum=optimizer_parameters['SGD_momentum'],\n",
    "                              nesterov=optimizer_parameters['SGD_nesterov'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Optimizer: RMSprop (version 1)\n",
    "\n",
    "optimizer_parameters['RMSprop_momentum'] = 0.5\n",
    "optimizer_parameters['RMSprop_centered'] = True\n",
    "\n",
    "def optimizer_rmsprop_v1(haparms):\n",
    "\n",
    "    return keras.optimizers.RMSprop(lr=hparams[HP_LEARNING_RATE],\n",
    "                                    momentum=optimizer_parameters['RMSprop_momentum'],\n",
    "                                    centered=optimizer_parameters['RMSprop_centered'])\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Function 2\n",
    "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
    "    if mode not in ['multiclass', 'multilabel']:\n",
    "        raise TypeError('mode must be: [multiclass, multilabel])')\n",
    "\n",
    "    if mode == 'multiclass':\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        print(nonzero)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), tf.float32)\n",
    "        return nonzero / y_true.shape[-1]\n",
    "\n",
    "class HammingLoss(tfa.metrics.MeanMetricWrapper):\n",
    "    def __init__(self, name='hamming_loss', dtype=None, mode='multilabel'):\n",
    "        super(HammingLoss, self).__init__(hamming_loss, name, dtype=dtype, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1\n",
    "\n",
    "def import_trained_keras_model(method, decay_steps_mode, optimizer_name, hparams):\n",
    "    \"\"\"\n",
    "    Load the weights of the model saved with EarlyStopping\n",
    "    \"\"\"\n",
    "    if method == \"import custom trained model\":\n",
    "        \n",
    "        if decay_steps_mode==\"on\":\n",
    "            \n",
    "            with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.json\".format(folder_path_model_saved,\n",
    "                                                                                                                        saved_model_name,\n",
    "                                                                                                                        str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                        str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                        str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                        str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                        version_data_control)),'r') as f:\n",
    "                model_json = json.load(f)\n",
    "\n",
    "            model_imported = model_from_json(model_json)\n",
    "\n",
    "            model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                                        saved_model_name,\n",
    "                                                                                                                                        str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                        str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                        str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                        str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                        version_data_control)))\n",
    "        else:\n",
    "            with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.json\".format(folder_path_model_saved,\n",
    "                                                                                                     saved_model_name,\n",
    "                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                     version_data_control)),'r') as f:\n",
    "                model_json = json.load(f)\n",
    "\n",
    "            model_imported = model_from_json(model_json)\n",
    "\n",
    "            model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                     saved_model_name,\n",
    "                                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                     version_data_control)))\n",
    "        if optimizer_name==\"adam\":\n",
    "            optimizer = optimizer_adam_v2(hparams)\n",
    "        \n",
    "        elif optimizer_name==\"sgd\":\n",
    "            optimizer = optimizer_sgd_v1(hparams, \"step decay\")\n",
    "            \n",
    "        else:\n",
    "            optimizer = optimizer_rmsprop_v1(hparams)\n",
    "            \n",
    "        model_imported.compile(optimizer=optimizer,\n",
    "                               loss=neural_network_parameters['model_loss'],\n",
    "                               metrics=neural_network_parameters['model_metric'])\n",
    "        print(type(model_imported))\n",
    "        print(\"\\nModel is loaded successfully\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.json\".format(folder_path_model_saved,\n",
    "                                                                                                                    saved_model_name,\n",
    "                                                                                                                    str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                    str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                    str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                    str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                    version_data_control)),'r') as f:\n",
    "            model_json = json.load(f)\n",
    "\n",
    "        model_imported = model_from_json(model_json, custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "        model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                                    saved_model_name,\n",
    "                                                                                                                                    str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                    str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                    str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                    str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                    version_data_control)))\n",
    "\n",
    "        optimizer = optimizer_adam_v2(hparams[HP_LEARNING_RATE], hparams[HP_DECAY_STEPS_MULTIPLIER], partial_x_train_actors_array.shape[0], optimizer_parameters['validation_split_ratio'], hparams[HP_HIDDEN_UNITS])\n",
    "\n",
    "        model_imported.compile(optimizer=optimizer,\n",
    "                               loss=neural_network_parameters['model_loss'],\n",
    "                               metrics=neural_network_parameters['model_metric'])\n",
    "        print(type(model_imported))\n",
    "        print(\"\\nModel is loaded successfully\\n\")\n",
    "    \n",
    "    return model_imported\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Function 2\n",
    "\n",
    "def create_df_scoring_table(method, decay_steps_mode, model_tag, hparams, model):\n",
    "    \"\"\"\n",
    "    Create a scoring dictionary to select the best out of the four models\n",
    "    \"\"\"\n",
    "    if method == \"import custom trained model\":\n",
    "        \n",
    "        model_evaluation = model.evaluate([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title],\n",
    "                                          y_test,\n",
    "                                          batch_size=hparams[HP_HIDDEN_UNITS],\n",
    "                                          verbose=2)\n",
    "\n",
    "        y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "        variance = np.var(y_test_predictions)\n",
    "        sse = np.mean((np.mean(y_test_predictions) - y_test)**2)\n",
    "        bias = sse - variance\n",
    "\n",
    "        hamming_loss_value = HammingLoss(mode='multilabel')\n",
    "        hamming_loss_value.update_state(y_test, y_test_predictions)\n",
    "        \n",
    "        if decay_steps_mode==\"on\":\n",
    "            \n",
    "            df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
    "                                    'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n",
    "                                    'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
    "                                    'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
    "                                    'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
    "                                    'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                                    'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                                    'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
    "                                    'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                                    'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
    "                                    'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
    "                                    'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                    'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                    'Bias':pd.Series([bias], dtype='float'),\n",
    "                                    'Variance':pd.Series([variance], dtype='float')\n",
    "                                   })\n",
    "\n",
    "            df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                                 saved_df_scored_metric_name,\n",
    "                                                                                                                                 str(hparams[HP_EMBEDDING_DIM]),\n",
    "                                                                                                                                 str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                                 str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                                 str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                 version_data_control)))\n",
    "        else:\n",
    "            \n",
    "            df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
    "                                    'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n",
    "                                    'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
    "                                    'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
    "                                    'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                                    'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                                    'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
    "                                    'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                                    'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
    "                                    'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
    "                                    'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                    'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                    'Bias':pd.Series([bias], dtype='float'),\n",
    "                                    'Variance':pd.Series([variance], dtype='float')\n",
    "                                   })\n",
    "\n",
    "            df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                              saved_df_scored_metric_name,\n",
    "                                                                                                              str(hparams[HP_EMBEDDING_DIM]),\n",
    "                                                                                                              str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                              str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                              version_data_control)))\n",
    "    else:\n",
    "        \n",
    "        model_evaluation = model.evaluate([test_bytes_list_plot, test_bytes_list_features, test_bytes_list_reviews, test_bytes_list_title],\n",
    "                                          test_label,\n",
    "                                          batch_size=hparams[HP_HIDDEN_UNITS],\n",
    "                                          verbose=2)\n",
    "\n",
    "        y_test_pred_probs = model.predict([test_bytes_list_plot, test_bytes_list_features, test_bytes_list_reviews, test_bytes_list_title])\n",
    "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "        variance = np.var(y_test_predictions)\n",
    "        sse = np.mean((np.mean(y_test_predictions) - test_label)**2)\n",
    "        bias = sse - variance\n",
    "\n",
    "        hamming_loss_value = HammingLoss(mode='multilabel')\n",
    "        hamming_loss_value.update_state(test_label, y_test_predictions)\n",
    "\n",
    "        df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
    "                                'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
    "                                'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
    "                                'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
    "                                'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                                'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                                'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
    "                                'Zero_one Loss':pd.Series([zero_one_loss(test_label, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                                'F1_score':pd.Series([f1_score(test_label, y_test_predictions, average=\"micro\")], dtype='float'),\n",
    "                                'F1_score_samples':pd.Series([f1_score(test_label, y_test_predictions, average=\"samples\")], dtype='float'),\n",
    "                                'ROC_score':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                'ROC_score_samples':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                'Bias':pd.Series([bias], dtype='float'),\n",
    "                                'Variance':pd.Series([variance], dtype='float')\n",
    "                               })\n",
    "\n",
    "        df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                             saved_df_scored_metric_name,\n",
    "                                                                                                                             str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                             str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                             str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                             str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                             version_data_control)))\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the models per Optimizer & Create a scoring dataframe for each model**\n",
    "\n",
    "Step 1 of the selection plan (based on the written thesis documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_method_creation=\"adam\"\n",
    "\n",
    "# First run the 36 models with batch 32, 64 and then the other 18 model of 128 batch size if your Ram is 16GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 36 models (0-36)\n",
    "\n",
    "list_models=[]\n",
    "list_df=[]\n",
    "\n",
    "if model_method_creation==\"adam\":\n",
    "\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10, 20]))\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_HIDDEN_UNITS: batch_size,\n",
    "                        HP_EMBEDDING_DIM: embedding_dim,\n",
    "                        HP_LEARNING_RATE: learning_rate,\n",
    "                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                      }\n",
    "                    print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n",
    "                    model_object=import_trained_keras_model(\"import custom trained model\", \"on\", model_method_creation, hparams)\n",
    "                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                    list_models.append(model_object)\n",
    "                    list_df.append(df_object)\n",
    "\n",
    "else:\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate\n",
    "                  }\n",
    "                print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n",
    "                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n",
    "                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                list_models.append(model_object)\n",
    "                list_df.append(df_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the first 36 models trained by the Adam Optimizer - Keras custom neural network\n",
    "\n",
    "Run this cell only if model_method_creation=\"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n",
    "model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen, model_nineteen, model_twenty, model_twenty_one, model_twenty_two, model_twenty_three, model_twenty_four=list_models[12:24]\n",
    "model_twenty_five, model_twenty_six, model_twenty_seven, model_twenty_eight, model_twenty_nine, model_twenty_thirty, model_thirty_one, model_thirty_two, model_thirty_three, model_thirty_four, model_thirty_five, model_thirty_six=list_models[24:36]\n",
    "\n",
    "df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n",
    "df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four=list_df[12:24]\n",
    "df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven, df_scores_twenty_eight, df_scores_twenty_nine, df_scores_thirty, df_scores_thirty_one, df_scores_thirty_two, df_scores_thirty_three, df_scores_thirty_four, df_scores_thirty_five, df_scores_thirty_six=list_df[24:36]\n",
    "\n",
    "frames_adam_one=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n",
    "                 df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four,\n",
    "                 df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven, df_scores_twenty_eight, df_scores_twenty_nine, df_scores_thirty, df_scores_thirty_one, df_scores_thirty_two, df_scores_thirty_three, df_scores_thirty_four, df_scores_thirty_five, df_scores_thirty_six]\n",
    "\n",
    "result_adam_one=pd.concat(frames_adam_one)\n",
    "result_adam_one=result_adam_one.reset_index(drop=True)\n",
    "result_adam_one.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_adam_one.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_adam_one.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                           \"results_table_adam_one\",\n",
    "                                                                                                                           str(100), #Embedding size of the the best model estimator\n",
    "                                                                                                                           str(32), #Batch size of the the best model estimator\n",
    "                                                                                                                           str(0.001), #Learning rate of the the best model estimator\n",
    "                                                                                                                           str(10),  #Decay Steps Multiplayer of the the best model estimator\n",
    "                                                                                                                           version_data_control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model of 36 presented above is the model 7 with:\n",
    "* Embedding size: 100\n",
    "* Batch size: 32\n",
    "* Learning rate: 0.001\n",
    "* Decay Steps Multiplier: 10\n",
    "* Hamming loss & Zeron-one loss: 0.003620 - 566.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest 18 models (37-54)\n",
    "\n",
    "list_models=[]\n",
    "list_df=[]\n",
    "\n",
    "if model_method_creation==\"adam\":\n",
    "\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([128]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10, 20]))\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_HIDDEN_UNITS: batch_size,\n",
    "                        HP_EMBEDDING_DIM: embedding_dim,\n",
    "                        HP_LEARNING_RATE: learning_rate,\n",
    "                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                      }\n",
    "                    print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n",
    "                    model_object=import_trained_keras_model(\"import custom trained model\", \"on\", model_method_creation, hparams)\n",
    "                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                    list_models.append(model_object)\n",
    "                    list_df.append(df_object)\n",
    "\n",
    "else:\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate\n",
    "                  }\n",
    "                print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n",
    "                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n",
    "                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                list_models.append(model_object)\n",
    "                list_df.append(df_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the rest 18 models trained by the Adam Optimizer - Keras custom neural network\n",
    "\n",
    "Run this cell only if model_method_creation=\"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n",
    "model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen=list_models[12:18]\n",
    "\n",
    "df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n",
    "df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen=list_df[12:18]\n",
    "\n",
    "frames_adam_two=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n",
    "                 df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen]\n",
    "\n",
    "result_adam_two=pd.concat(frames_adam_two)\n",
    "result_adam_two=result_adam_two.reset_index(drop=True)\n",
    "result_adam_two.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_adam_two.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_adam_two.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                           \"results_table_adam_two\",\n",
    "                                                                                                                           str(100), #Embedding size of the the best model estimator\n",
    "                                                                                                                           str(128), #Batch size of the the best model estimator\n",
    "                                                                                                                           str(0.001), #Learning rate of the the best model estimator\n",
    "                                                                                                                           str(20),  #Decay Steps Multiplayer of the the best model estimator\n",
    "                                                                                                                           version_data_control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model of 36 (0-36) presented above is the model 7 with:\n",
    "* Embedding size: 100\n",
    "* Batch size: 32\n",
    "* Learning rate: 0.001\n",
    "* Decay Steps Multiplier: 10\n",
    "* Hamming loss & Zeron-one loss: 0.003620 - 566.0\n",
    "    \n",
    "Best model of 18 (36-54) presented above is the model 44 with:\n",
    "* Embedding size: 100\n",
    "* Batch size: 128\n",
    "* Learning rate: 0.001\n",
    "* Decay Steps Multiplier: 20\n",
    "* Hamming loss & Zeron-one loss: 0.003986 - 622.0\n",
    "\n",
    "The best out of the two is the model 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the models trained by the SGD Optimizer - Keras custom neural network\n",
    "\n",
    "Run this cell only if model_method_creation=\"sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_name=\"multi_input_keras_model\"\n",
    "folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n",
    "saved_df_scored_metric_name=\"df_metrics_multi_input_keras_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_method_creation=\"sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models=[]\n",
    "list_df=[]\n",
    "\n",
    "if model_method_creation==\"adam\":\n",
    "\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10, 20])) #only used in adam optimizer\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_HIDDEN_UNITS: batch_size,\n",
    "                        HP_EMBEDDING_DIM: embedding_dim,\n",
    "                        HP_LEARNING_RATE: learning_rate,\n",
    "                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                      }\n",
    "                    print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n",
    "                    model_object=import_trained_keras_model(\"import custom trained model\", \"on\", model_method_creation, hparams)\n",
    "                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                    list_models.append(model_object)\n",
    "                    list_df.append(df_object)\n",
    "\n",
    "else:\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate\n",
    "                  }\n",
    "                print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n",
    "                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n",
    "                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                list_models.append(model_object)\n",
    "                list_df.append(df_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n",
    "model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen, model_nineteen, model_twenty, model_twenty_one, model_twenty_two, model_twenty_three, model_twenty_four=list_models[12:24]\n",
    "model_twenty_five, model_twenty_six, model_twenty_seven=list_models[24:27]\n",
    "\n",
    "df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n",
    "df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four=list_df[12:24]\n",
    "df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven=list_df[24:27]\n",
    "\n",
    "frames_glove=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n",
    "              df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four,\n",
    "              df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven]\n",
    "\n",
    "result_sgd=pd.concat(frames_glove)\n",
    "result_sgd=result_sgd.reset_index(drop=True)\n",
    "result_sgd.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_sgd.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sgd.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                   \"results_table_sgd\",\n",
    "                                                                                                   str(100), #Embedding size of the the best model estimator\n",
    "                                                                                                   str(64), #Batch size of the the best model estimator\n",
    "                                                                                                   str(0.1), #Learning rate of the the best model estimator\n",
    "                                                                                                   version_data_control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model of 27 presented above is the model 15 with:\n",
    "* Embedding size: 100\n",
    "* Batch size: 64\n",
    "* Learning rate: 0.1\n",
    "* Hamming loss & Zeron-one loss: 0.009035 - 1363.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the models trained by the RMSprop Optimizer - Keras custom neural network\n",
    "\n",
    "Run this cell only if model_method_creation=\"rmsprop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize name variables\n",
    "\n",
    "saved_model_name=\"multi_input_keras_model\"\n",
    "folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n",
    "saved_df_scored_metric_name=\"df_metrics_multi_input_keras_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_method_creation=\"rmsprop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models=[]\n",
    "list_df=[]\n",
    "\n",
    "if model_method_creation==\"adam\":\n",
    "\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "    HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10])) #only used in adam optimizer\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_HIDDEN_UNITS: batch_size,\n",
    "                        HP_EMBEDDING_DIM: embedding_dim,\n",
    "                        HP_LEARNING_RATE: learning_rate,\n",
    "                        HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                      }\n",
    "                    print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n",
    "                    model_object=import_trained_keras_model(\"import custom trained model\", \"on\", model_method_creation, hparams)\n",
    "                    df_object=create_df_scoring_table(\"import custom trained model\", \"on\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                    list_models.append(model_object)\n",
    "                    list_df.append(df_object)\n",
    "\n",
    "else:\n",
    "    HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
    "    HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "    HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
    "\n",
    "    for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "        for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "            for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate\n",
    "                  }\n",
    "                print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))))\n",
    "                model_object=import_trained_keras_model(\"import custom trained model\", \"off\", model_method_creation, hparams)\n",
    "                df_object=create_df_scoring_table(\"import custom trained model\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
    "                list_models.append(model_object)\n",
    "                list_df.append(df_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve=list_models[0:12]\n",
    "model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen, model_nineteen, model_twenty, model_twenty_one, model_twenty_two, model_twenty_three, model_twenty_four=list_models[12:24]\n",
    "model_twenty_five, model_twenty_six, model_twenty_seven=list_models[24:27]\n",
    "\n",
    "df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve=list_df[0:12]\n",
    "df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four=list_df[12:24]\n",
    "df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven=list_df[24:27]\n",
    "\n",
    "frames_rmsprop=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve,\n",
    "                df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen, df_scores_nineteen, df_scores_twenty, df_scores_twenty_one, df_scores_twenty_two, df_scores_twenty_three, df_scores_twenty_four,\n",
    "                df_scores_twenty_five, df_scores_twenty_six, df_scores_twenty_seven]\n",
    "\n",
    "result_rmsprop=pd.concat(frames_rmsprop)\n",
    "result_rmsprop=result_rmsprop.reset_index(drop=True)\n",
    "result_rmsprop.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_rmsprop.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rmsprop.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                       \"results_table_rmsprop\",\n",
    "                                                                                                       str(150), #Embedding size of the the best model estimator\n",
    "                                                                                                       str(64), #Batch size of the the best model estimator\n",
    "                                                                                                       str(0.001), #Learning rate of the the best model estimator\n",
    "                                                                                                       version_data_control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model of 27 presented above is the model 16 with:\n",
    "* Embedding size: 150\n",
    "* Batch size: 64\n",
    "* Learning rate: 0.001\n",
    "* Hamming loss & Zeron-one loss: 0.004064 - 637.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three final best models, one per optimizer, are:\n",
    "\n",
    "**Adam**\n",
    "* Model 7\n",
    "* Embedding size: 100\n",
    "* Batch size: 32\n",
    "* Learning rate: 0.001\n",
    "* Decay Steps Multiplier: 10\n",
    "* Hamming loss & Zeron-one loss: 0.003620 - 566.0\n",
    "\n",
    "**SGD**\n",
    "* Model 15\n",
    "* Embedding size: 100\n",
    "* Batch size: 64\n",
    "* Learning rate: 0.1\n",
    "* Hamming loss & Zeron-one loss: 0.009035 - 1363.0\n",
    "\n",
    "**RMSprop**\n",
    "* Model 16\n",
    "* Embedding size: 150\n",
    "* Batch size: 64\n",
    "* Learning rate: 0.001\n",
    "* Hamming loss & Zeron-one loss: 0.004064 - 637.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison 2: Create a classification report and a confusion matrix for the two closest models (per optimizer)** <br>\n",
    "Additionally, create a bias-variance tradeoff tample per optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model selected-Adam\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                hparams_adam = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate,\n",
    "                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                  }\n",
    "                folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n",
    "                model_one = import_trained_keras_model(\"import custom trained model\", \"on\", \"adam\", hparams_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model selected-SGD\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.1]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams_sgd = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_EMBEDDING_DIM: embedding_dim,\n",
    "                HP_LEARNING_RATE: learning_rate\n",
    "              }\n",
    "            folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n",
    "            model_two = import_trained_keras_model(\"import custom trained model\", \"off\", \"sgd\", hparams_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model selected-RMSprop\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([150]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_EMBEDDING_DIM: embedding_dim,\n",
    "                HP_LEARNING_RATE: learning_rate\n",
    "              }\n",
    "            folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n",
    "            model_three = import_trained_keras_model(\"import custom trained model\", \"off\", \"rmsprop\", hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_version_control=\"20072020\"\n",
    "\n",
    "history_dataframe_one=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_{0}\\\\metrics_histogram_multi_input_keras_{1}dim_{2}batchsize_{3}lr_{4}decaymultiplier_16072020.pkl\".format(saved_version_control, str(100), str(32), str(0.001), str(10))))\n",
    "history_dataframe_two=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\sgd_models_{0}\\\\metrics_histogram_multi_input_keras_{1}dim_{2}batchsize_{3}lr_16072020.pkl\".format(saved_version_control, str(100), str(64), str(0.1))))\n",
    "history_dataframe_three=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\rmsprop_models_{0}\\\\metrics_histogram_multi_input_keras_{1}dim_{2}batchsize_{3}lr_16072020.pkl\".format(saved_version_control, str(150), str(64), str(0.001))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_table(model):\n",
    "    \n",
    "    y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "    y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "    \n",
    "    variance = np.var(y_test_predictions)\n",
    "    sse = np.mean((np.mean(y_test_predictions) - y_test)**2)\n",
    "    bias = sse - variance\n",
    "\n",
    "    classification_table = classification_report(y_true=y_test, y_pred=y_test_predictions)\n",
    "    \n",
    "    return classification_table, variance, bias\n",
    "\n",
    "def create_confusion_matrix(mode, decay_steps_mode, embedding_dim_mode,  model, hparams):\n",
    "\n",
    "    if mode == \"custom trained model\":\n",
    "        \n",
    "        if decay_steps_mode==\"on\":\n",
    "            \n",
    "            if embedding_dim_mode==\"on\":\n",
    "        \n",
    "                y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "                y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "                conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "                conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                         columns=genres_list,\n",
    "                                         index=genres_list)\n",
    "\n",
    "                conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                                       \"confusion_matrix\",\n",
    "                                                                                                                                       str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                       str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                       str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                       str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                       version_data_control)))\n",
    "            else:\n",
    "                \n",
    "                y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "                y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "                conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "                conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                         columns=genres_list,\n",
    "                                         index=genres_list)\n",
    "\n",
    "                conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                                \"confusion_matrix\",\n",
    "                                                                                                                                str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                                str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                                str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                version_data_control)))\n",
    "        else:\n",
    "            y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "            y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "            conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "            conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                     columns=genres_list,\n",
    "                                     index=genres_list)\n",
    "\n",
    "            conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                \"confusion_matrix\",\n",
    "                                                                                                                str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                version_data_control)))\n",
    "    else:\n",
    "        \n",
    "        y_test_pred_probs = model.predict([test_bytes_list_plot, test_bytes_list_features, test_bytes_list_reviews, test_bytes_list_title])\n",
    "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "        conf_mat=confusion_matrix(test_label.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "        conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                 columns=genres_list,\n",
    "                                 index=genres_list)\n",
    "\n",
    "        conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                        \"confusion_matrix\",\n",
    "                                                                                                                        str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                        str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                        str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                        version_data_control)))\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ADAM Optimizer\n",
    "\n",
    "folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n",
    "\n",
    "classification_table_one, variance_adam, bias_adam=create_classification_table(model_one)\n",
    "print(\"Classification report for the best model estimator of the Adam optimizaion function:\\n\\n\" + str(classification_table_one))\n",
    "\n",
    "bias_variance_tradeoff_adam=pd.DataFrame({'Tag Name':pd.Series(\"model seven adam\", dtype='str'),\n",
    "                                          'Bias': pd.Series(bias_adam, dtype='str'),\n",
    "                                          'Variance': pd.Series(variance_adam, dtype='str'), \n",
    "                                          'Average Training loss': pd.Series(np.mean(history_dataframe_one.loss), dtype='str'),\n",
    "                                          'Average Validation loss': pd.Series(np.mean(history_dataframe_one.val_loss), dtype='str')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ---------------------------------------------------------------------------#ADAM Optimizer\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                hparams_adam = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate,\n",
    "                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                  }\n",
    "                folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n",
    "                confusion_matrix_one=create_confusion_matrix(\"custom trained model\", \"on\", \"on\", model_one, hparams_adam)\n",
    "confusion_matrix_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_one.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Optimizer\n",
    "\n",
    "folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n",
    "\n",
    "classification_table_two, variance_sgd, bias_sgd=create_classification_table(model_two)\n",
    "print(\"Classification report for the best model estimator of the SGD optimizaion function:\\n\\n\" + str(classification_table_two))\n",
    "\n",
    "bias_variance_tradeoff_sgd=pd.DataFrame({'Tag Name':pd.Series(\"model fifteen sgd\", dtype='str'),\n",
    "                                         'Bias': pd.Series(bias_sgd, dtype='str'),\n",
    "                                         'Variance': pd.Series(variance_sgd, dtype='str'),\n",
    "                                         'Average Training loss': pd.Series(np.mean(history_dataframe_two.loss), dtype='str'),\n",
    "                                         'Average Validation loss': pd.Series(np.mean(history_dataframe_two.val_loss), dtype='str')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SGD Optimizer\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.1]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams_sgd = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_EMBEDDING_DIM: embedding_dim,\n",
    "                HP_LEARNING_RATE: learning_rate\n",
    "              }\n",
    "            folder_path_model_saved=\"model_one\\\\sgd_models_20072020\"\n",
    "            confusion_matrix_two=create_confusion_matrix(\"custom trained model\", \"off\", \"on\", model_two, hparams_sgd)\n",
    "confusion_matrix_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_two.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop Optimizer\n",
    "\n",
    "folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n",
    "\n",
    "classification_table_three, variance_rmsprop, bias_rmsprop=create_classification_table(model_three)\n",
    "print(\"Classification report for the best model estimator of the SGD optimizaion function:\\n\\n\" + str(classification_table_three))\n",
    "\n",
    "bias_variance_tradeoff_rmsprop=pd.DataFrame({'Tag Name':pd.Series(\"model sixteen rmsprop\", dtype='str'),\n",
    "                                             'Bias': pd.Series(bias_rmsprop, dtype='str'),\n",
    "                                             'Variance': pd.Series(variance_rmsprop, dtype='str'),\n",
    "                                             'Average Training loss': pd.Series(np.mean(history_dataframe_three.loss), dtype='str'),\n",
    "                                             'Average Validation loss': pd.Series(np.mean(history_dataframe_three.val_loss), dtype='str')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop Optimizer\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([150]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams_rmsprop = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_EMBEDDING_DIM: embedding_dim,\n",
    "                HP_LEARNING_RATE: learning_rate\n",
    "              }\n",
    "            folder_path_model_saved=\"model_one\\\\rmsprop_models_20072020\"\n",
    "            confusion_matrix_three=create_confusion_matrix(\"custom trained model\", \"off\", \"on\", model_three, hparams_rmsprop)\n",
    "confusion_matrix_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_three.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_variance_frames = [bias_variance_tradeoff_adam, bias_variance_tradeoff_sgd, bias_variance_tradeoff_rmsprop]\n",
    "bias_variance_result = pd.concat(bias_variance_frames)\n",
    "bias_variance_result = bias_variance_result.reset_index(drop=True)\n",
    "bias_variance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bias_variance_result.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison 3: Test Accuracy - Test Score/Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the df_metric_score dataframes of the three models under review.\n",
    "\n",
    "df_score_one=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_20072020\\\\df_metrics_multi_input_keras_model_100dim_32batchsize_0.001lr_10decaymultiplier_16072020.pkl\"))\n",
    "df_score_two=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\sgd_models_20072020\\\\df_metrics_multi_input_keras_model_100dim_64batchsize_0.1lr_16072020.pkl\"))\n",
    "df_score_three=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\rmsprop_models_20072020\\\\df_metrics_multi_input_keras_model_150dim_64batchsize_0.001lr_16072020.pkl\"))\n",
    "\n",
    "result=pd.concat([df_score_one, df_score_two, df_score_three])\n",
    "result=result.reset_index(drop=True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormin = 'red'\n",
    "colorother = 'black'\n",
    "clrs_acc = [colormin if result['Test Hamming Loss'].iloc[row]== result['Test Hamming Loss'].min() else colorother for row in range(len(result['Test Hamming Loss']))]\n",
    "clrs_loss = [colormin if result['Test Loss'].iloc[row]== result['Test Loss'].min() else colorother for row in range(len(result['Test Loss']))]\n",
    "\n",
    "x=result['Tag Name'].values.tolist()\n",
    "y=result['Test Hamming Loss'].values.tolist()\n",
    "fig5 = go.Figure()\n",
    "fig5.add_trace(go.Scatter(x=x, y=y,\n",
    "                          mode='markers',\n",
    "                          marker=dict(color=clrs_acc)\n",
    "                        ))\n",
    "fig5.update_layout(title=\"Hamming Loss on test set per model\",\n",
    "                   xaxis_title=\"Model number\",\n",
    "                   yaxis_title=\"Hamming Loss value/model\")\n",
    "fig5.show()\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "fig6 = go.Figure()\n",
    "fig6.add_trace(go.Scatter(x=result['Tag Name'].values.tolist(), \n",
    "                          y=result['Test Loss'].values.tolist(),\n",
    "                          mode='markers',\n",
    "                          marker=dict(color=clrs_loss)\n",
    "                         ))\n",
    "\n",
    "fig6.update_layout(title=\"Loss score on test set per model\",\n",
    "                  xaxis_title=\"Model number\",\n",
    "                  yaxis_title=\"Test loss/model\")\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison 4: Predicted vs Actual Genre Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.read_pickle(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\text_tokenization_padded_sequences_13072020\\\\x_test_13072020.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre_tags(indx, model, genres_list):\n",
    "        \n",
    "    test_sequence_actors = X_test_seq_actors[indx:indx+1]\n",
    "    \n",
    "    test_sequence_plot = X_test_seq_plot[indx:indx+1]\n",
    "    \n",
    "    test_sequence_features = X_test_seq_features[indx:indx+1]\n",
    "    \n",
    "    test_sequence_reviews = X_test_seq_reviews[indx:indx+1]\n",
    "    \n",
    "    test_sequence_title = X_test_seq_title[indx:indx+1]\n",
    "    \n",
    "    text_prediction = model.predict([test_sequence_actors, test_sequence_plot, test_sequence_features, test_sequence_reviews, test_sequence_title])\n",
    "    \n",
    "    [float(i) for i in text_prediction[0]]\n",
    "    \n",
    "    genres_length=len(X_test['reduced_genres'].iloc[indx])\n",
    "    \n",
    "    tag_probabilities = text_prediction[0][np.argsort(text_prediction[0])[-genres_length:]]\n",
    "    \n",
    "    indexes = np.argsort(text_prediction[0])[::-1][:genres_length]\n",
    "    \n",
    "    indexes = np.sort(indexes)\n",
    "    \n",
    "    predicted_tags = []\n",
    "    \n",
    "    predicted_tags = [genres_list[i] for i in indexes]\n",
    "    \n",
    "    return predicted_tags\n",
    "\n",
    "def create_predictions_df(model, random_numbers_list, file_name, optimizer_name,  hparams):\n",
    "    \n",
    "    if optimizer_name==\"adam\":\n",
    "    \n",
    "        df_predictions = pd.DataFrame({'Movie Title': pd.Series([X_test['title'].iloc[random_numbers_list[0]]], dtype='str'),\n",
    "                                       'Predicted Genre tags (top 3)': pd.Series([predict_genre_tags(random_numbers_list[0], model, genres_list)], dtype='str'),\n",
    "                                       'Real Genre tags': pd.Series([X_test['reduced_genres'].iloc[random_numbers_list[0]]], dtype='str')})\n",
    "\n",
    "        for i in range(len(random_numbers_list)):\n",
    "\n",
    "            df_predictions = df_predictions.append({'Movie Title': X_test['title'].iloc[random_numbers_list[i]], \n",
    "                                                    'Predicted Genre tags (top 3)': predict_genre_tags(random_numbers_list[i], model, genres_list),\n",
    "                                                    'Real Genre tags': X_test['reduced_genres'].iloc[random_numbers_list[i]]} , ignore_index=True)\n",
    "\n",
    "        df_predictions = df_predictions.drop(df_predictions.index[0])\n",
    "        df_predictions.to_pickle(\"model_one\\\\{0}\\\\{1}_df_predictions_{2}dim_{3}batchsize_{4}lr_{5}decatmultiplier_{6}.pkl\".format(file_name, \n",
    "                                                                                                                                  optimizer_name, \n",
    "                                                                                                                                  str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                  str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                  str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                  str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                  version_data_control))\n",
    "    else:\n",
    "        \n",
    "        df_predictions = pd.DataFrame({'Movie Title': pd.Series([X_test['title'].iloc[random_numbers_list[0]]], dtype='str'),\n",
    "                                       'Predicted Genre tags (top 3)': pd.Series([predict_genre_tags(random_numbers_list[0], model, genres_list)], dtype='str'),\n",
    "                                       'Real Genre tags': pd.Series([X_test['reduced_genres'].iloc[random_numbers_list[0]]], dtype='str')})\n",
    "\n",
    "        for i in range(len(random_numbers_list)):\n",
    "\n",
    "            df_predictions = df_predictions.append({'Movie Title': X_test['title'].iloc[random_numbers_list[i]], \n",
    "                                                    'Predicted Genre tags (top 3)': predict_genre_tags(random_numbers_list[i], model, genres_list),\n",
    "                                                    'Real Genre tags': X_test['reduced_genres'].iloc[random_numbers_list[i]]} , ignore_index=True)\n",
    "\n",
    "        df_predictions = df_predictions.drop(df_predictions.index[0])\n",
    "        df_predictions.to_pickle(\"model_one\\\\{0}\\\\{1}_df_predictions_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(file_name, \n",
    "                                                                                                               optimizer_name, \n",
    "                                                                                                               str(hparams[HP_EMBEDDING_DIM]),\n",
    "                                                                                                               str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                               str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                               version_data_control))\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_numbers = random.sample(range(1, y_test.shape[0]), 20)\n",
    "\n",
    "save_index_of_numbers = random_numbers\n",
    "\n",
    "print(\"Randomly saved numbers to make predictions: {}\".format(save_index_of_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                hparams_adam = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate,\n",
    "                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                  }\n",
    "                predictions_dataframe_one=create_predictions_df(model_one, save_index_of_numbers, \"adam_v2_models_20072020\", \"adam\", hparams_adam)\n",
    "predictions_dataframe_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_dataframe_one.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.1]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams_sgd = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_EMBEDDING_DIM: embedding_dim,\n",
    "                HP_LEARNING_RATE: learning_rate\n",
    "              }\n",
    "            predictions_dataframe_two=create_predictions_df(model_two, save_index_of_numbers, \"sgd_models_20072020\", \"sgd\", hparams_sgd)\n",
    "predictions_dataframe_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_dataframe_two.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams_rmsprop = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_EMBEDDING_DIM: embedding_dim,\n",
    "                HP_LEARNING_RATE: learning_rate\n",
    "              }\n",
    "            predictions_dataframe_three=create_predictions_df(model_three, save_index_of_numbers, \"rmsprop_models_20072020\", \"rmsprop\", hparams_rmsprop)\n",
    "predictions_dataframe_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_dataframe_three.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison 5: Training and Validation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the learning curves for each of the three best model estimators, some examples of underfitting and overfitting learning curves are presents with random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1-Underfitting Learning Curve part 1\n",
    "\n",
    "fig1=go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[0.005, 0.003, 0.001, 0.0009, 0.0007,  0.0005],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Train\",\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[0.05, 0.055, 0.065, 0.068, 0.07, 0.075],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Validation\",\n",
    "                          line=dict(color='rgb(252, 141, 98)')))\n",
    "\n",
    "fig1.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss - Underfitting learning curve\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2-Underfitting Learning Curve part 2\n",
    "\n",
    "fig1=go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[1.075, 1.071, 1.066, 1.061, 1.057, 1.037],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Train\",\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[1.065, 1.061, 1.056, 1.051, 1.047, 1.027],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Validation\",\n",
    "                          line=dict(color='rgb(252, 141, 98)')))\n",
    "\n",
    "fig1.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss - Underfitting learning curve\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3-Underfitting Learning Curve\n",
    "\n",
    "fig1=go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[1.075, 0.85, 0.45, 0.35, 0.25, 0.05],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Train\",\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[1.005, 0.75, 0.51, 0.28, 0.38, 0.49],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Validation\",\n",
    "                          line=dict(color='rgb(252, 141, 98)')))\n",
    "\n",
    "fig1.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss - Overfitting learning curve\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4-Good Fit Learning Curves\n",
    "\n",
    "fig1=go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[1.075, 0.85, 0.45, 0.35, 0.25, 0.05],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Train\",\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=[0, 20, 40, 60, 80, 100], \n",
    "                          y=[1.069, 0.82, 0.43, 0.32, 0.22, 0.03],\n",
    "                          mode='lines+markers',\n",
    "                          name=\"Validation\",\n",
    "                          line=dict(color='rgb(252, 141, 98)')))\n",
    "\n",
    "fig1.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss - Good fit learning curve\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of the pre-final step of the selection plan | Training-Validation Accuracy/Loss Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hamming Loss performance models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormin = 'black'\n",
    "colormax = 'black'\n",
    "colorother = 'rgb(252, 141, 98)'\n",
    "\n",
    "clrs_acc_model_adam = [colormax if history_dataframe_one.val_hamming_loss.iloc[row]==history_dataframe_one.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_one.val_hamming_loss))]\n",
    "clrs_acc_model_sgd = [colormax if history_dataframe_two.val_hamming_loss.iloc[row]==history_dataframe_two.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_two.val_hamming_loss))]\n",
    "clrs_acc_model_rmsprop = [colormax if history_dataframe_three.val_hamming_loss.iloc[row]==history_dataframe_three.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_three.val_hamming_loss))]\n",
    "\n",
    "#Hamming Loss of Adam optimizer model\n",
    "fig1=go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_adam)))\n",
    "\n",
    "fig1.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the Adam Optimizer)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig1.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_hamming_loss==history_dataframe_one.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_one.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the highest validation Hamming Loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig1.update_layout(legend_title_text='Training & Validation Hamming Loss points per epoch')\n",
    "\n",
    "fig1.show()\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Hamming Loss of SGD optimizer model\n",
    "\n",
    "fig2=go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n",
    "                          y=history_dataframe_two.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig2.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n",
    "                          y=history_dataframe_two.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_sgd)))\n",
    "\n",
    "fig2.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the SGD Optimizer)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig2.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_two.epoch[history_dataframe_two.val_hamming_loss==history_dataframe_two.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_two.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the highest validation Hamming Loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig2.update_layout(legend_title_text='Training & Validation Hamming Loss points per epoch')\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Hamming Loss of RMSprop optimizer model\n",
    "\n",
    "fig3=go.Figure()\n",
    "\n",
    "fig3.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(),\n",
    "                          y=history_dataframe_three.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig3.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n",
    "                          y=history_dataframe_three.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_rmsprop)))\n",
    "\n",
    "fig3.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the RMSprop Optimizer)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig3.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_three.epoch[history_dataframe_three.val_hamming_loss==history_dataframe_three.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_three.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the highest validation Hamming Loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig3.update_layout(legend_title_text='Training & Validation Hamming Loss points per epoch')\n",
    "\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colormin = 'black'\n",
    "colorother = 'rgb(252, 141, 98)'\n",
    "\n",
    "clrs_loss_model_adam=[colormin if history_dataframe_one.val_loss.iloc[row]==history_dataframe_one.val_loss.min() else colorother for row in range(len(history_dataframe_one.val_loss))]\n",
    "clrs_loss_model_sgd=[colormin if history_dataframe_two.val_loss.iloc[row]==history_dataframe_two.val_loss.min() else colorother for row in range(len(history_dataframe_two.val_loss))]\n",
    "clrs_loss_model_rmsprop=[colormin if history_dataframe_three.val_loss.iloc[row]==history_dataframe_three.val_loss.min() else colorother for row in range(len(history_dataframe_three.val_loss))]\n",
    "\n",
    "#Loss of Adam optimizer model\n",
    "\n",
    "fig3=go.Figure()\n",
    "\n",
    "fig3.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig3.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_adam)))\n",
    "\n",
    "fig3.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the Adam Optimizer)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig3.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_loss==history_dataframe_one.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_one.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-40)])\n",
    "fig3.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Loss of SGD optimizer model\n",
    "\n",
    "fig4=go.Figure()\n",
    "\n",
    "fig4.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n",
    "                          y=history_dataframe_two.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig4.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n",
    "                          y=history_dataframe_two.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_sgd)))\n",
    "\n",
    "fig4.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the SGD Optimizer)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig4.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_two.epoch[history_dataframe_two.val_loss==history_dataframe_two.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_two.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-65)])\n",
    "fig4.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Loss of RMSprop optimizer model\n",
    "\n",
    "fig5=go.Figure()\n",
    "\n",
    "fig5.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n",
    "                          y=history_dataframe_three.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig5.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n",
    "                          y=history_dataframe_three.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_rmsprop)))\n",
    "\n",
    "fig5.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the RMSprop Optimizer)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig5.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_three.epoch[history_dataframe_three.val_loss==history_dataframe_three.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_three.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-65)])\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65,).create(prog='dot', format='svg'))\n",
    "visualize_model(model_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "model_one,\n",
    "to_file=\"model.png\",\n",
    "show_shapes=True,\n",
    "show_layer_names=True,\n",
    "rankdir=\"TB\",\n",
    "expand_nested=False,\n",
    "dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model estimator trained on binary accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_version_data_control=\"22042020\"\n",
    "\n",
    "X_train_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_actors_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "X_train_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_plot_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "X_train_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_features_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "X_train_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_train_seq_reviews_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "\n",
    "print(\"X_train data inputs have been loaded!\\n\")\n",
    "\n",
    "X_test_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_actors_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "X_test_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_plot_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "X_test_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_features_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "X_test_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\x_test_seq_reviews_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "\n",
    "print(\"X_test data inputs have been loaded!\\n\")\n",
    "\n",
    "y_train=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\y_train_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "y_test=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\y_test_80-20_non-balanced_20000_{0}.npy\".format(saved_version_data_control)))\n",
    "\n",
    "print(\"y_train & y_test have been loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the tokenizers of each input, fitted on part 3.1\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\actors_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\plot_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\features_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\20000_max_features_cleaned_numbers\\\\reviews_tokenizer_20000_{0}.pkl'.format(saved_version_data_control)),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    assert len(actors_tokenizer.word_index)==20000\n",
    "    assert len(plot_tokenizer.word_index)==20000\n",
    "    assert len(features_tokenizer.word_index)==20000\n",
    "    assert len(reviews_tokenizer.word_index)==20000\n",
    "except AssertionError:\n",
    "    print(\"ERROR: The vocabulary length for some of the tokenizers, is not equal to 20000. Please verify their lengths by running the following: len(actors_tokenizer.word_index)\")\n",
    "\n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the X_train, X_test, y_train & y_test data pickled from dataset part 3.1\n",
    "\"\"\"\n",
    "saved_version_data_control=\"22042020\"\n",
    "\n",
    "X_train=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\X_train_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "X_test=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\X_test_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "y_train=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\y_train_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "y_test=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\version_{0}\\\\y_test_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "\n",
    "assert X_train.shape[0]==y_train.shape[0]\n",
    "assert X_test.shape[0]==y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_parameters={}\n",
    "optimizer_parameters={}\n",
    "\n",
    "neural_network_parameters['model_loss'] = 'binary_crossentropy'\n",
    "neural_network_parameters['model_metric'] = 'accuracy'\n",
    "validation_split_ratio=0.8\n",
    "\n",
    "def optimizer_adam_v2_accuracy(batch_size_value):\n",
    "\n",
    "    optimizer_parameters['steps_per_epoch'] = int(np.ceil((X_train_seq_features.shape[0]*validation_split_ratio)//batch_size_value))\n",
    "    optimizer_parameters['lr_schedule_learning_rate'] = 0.01\n",
    "    optimizer_parameters['lr_schedule_decay_steps'] = optimizer_parameters['steps_per_epoch']*1000\n",
    "    optimizer_parameters['lr_schedule_decay_rate'] = 1\n",
    "    optimizer_parameters['staircase'] = False\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        optimizer_parameters['lr_schedule_learning_rate'],\n",
    "        decay_steps=optimizer_parameters['lr_schedule_decay_steps'],\n",
    "        decay_rate=optimizer_parameters['lr_schedule_decay_rate'],\n",
    "        staircase=optimizer_parameters['staircase'])\n",
    "    \n",
    "    return keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 16 - 300 embedding dimenstion & 128 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), 'model_one\\\\adam_v2_models_22042020\\\\multi_input_keras_model_{0}dim_{1}batchsize_{2}lr_{3}decaymultiplier_22042020.json'.format(str(300), str(128), str(0.01), str(1000))),'r') as f:\n",
    "    model_json = json.load(f)\n",
    "\n",
    "model_sixteen = model_from_json(model_json)\n",
    "\n",
    "model_sixteen.load_weights(os.path.join(os.getcwd(), 'model_one\\\\adam_v2_models_22042020\\\\multi_input_keras_model_{0}dim_{1}batchsize_{2}lr_{3}decaymultiplier_22042020.h5'.format(str(300), str(128), str(0.01), str(1000))))\n",
    "\n",
    "model_sixteen.compile(optimizer=optimizer_adam_v2_accuracy(128),\n",
    "                      loss=neural_network_parameters['model_loss'],\n",
    "                      metrics=[neural_network_parameters['model_metric']])\n",
    "\n",
    "print(type(model_sixteen))\n",
    "print(\"\\nModel is loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_adam_accuracy=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_22042020\\\\df_metrics_multy_input_keras_300dim_128batchsize_26052020.pkl\"))\n",
    "df_scores_adam_accuracy['Bias'], df_scores_adam_accuracy['Variance']=[-0.00047,0.096426]\n",
    "df_scores_adam_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scores_adam_accuracy.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dataframe_one=pd.read_pickle(os.path.join(os.getcwd(), \"model_one\\\\adam_v2_models_22042020\\\\metrics_histogram_multi_input_keras_300dim_128batchsize_22042020.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormax = 'black'\n",
    "colorother = 'rgb(252, 141, 98)'\n",
    "clrs_acc_model = [colormax if history_dataframe_one.val_accuracy.iloc[row]==history_dataframe_one.val_accuracy.max() else colorother for row in range(len(history_dataframe_one.val_accuracy))]\n",
    "\n",
    "clrs_loss=[colormin if history_dataframe_one.val_loss.iloc[row]==history_dataframe_one.val_loss.min() else colorother for row in range(len(history_dataframe_one.val_loss))]\n",
    "\n",
    "#Accuracy of model four\n",
    "fig11=go.Figure()\n",
    "\n",
    "fig11.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.accuracy.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Accuracy',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig11.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.val_accuracy.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Accuracy',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model)))\n",
    "\n",
    "fig11.update_layout(template=\"simple_white\",\n",
    "                   title=\"Accuracy score on train & validation sets (adam model accuracy)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Accuracy/epoch\")\n",
    "\n",
    "fig11.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_accuracy==history_dataframe_one.val_accuracy.max()].tolist()[0],\n",
    "                                     y=history_dataframe_one.val_accuracy.max(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the highest validation accuracy\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "fig11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormin = 'black'\n",
    "colorother = 'rgb(252, 141, 98)'\n",
    "\n",
    "clrs_loss_model=[colormin if history_dataframe_one.val_loss.iloc[row]==history_dataframe_one.val_loss.min() else colorother for row in range(len(history_dataframe_one.val_loss))]\n",
    "\n",
    "#Loss of model four\n",
    "\n",
    "fig12=go.Figure()\n",
    "\n",
    "fig12.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig12.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model)))\n",
    "\n",
    "fig12.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (adam model trained on accuracy)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig12.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_loss==history_dataframe_one.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_one.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-40)])\n",
    "fig12.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
