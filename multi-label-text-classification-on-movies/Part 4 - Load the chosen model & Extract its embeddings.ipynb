{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Load the chosen model & Extract its embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the version data control parameter (to save the outputs of this notebook at their latest date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_data_control=\"14082020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "try:\n",
    "    collectionsAbc = collections.abc\n",
    "except AttributeError:\n",
    "    collectionsAbc = collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "import unidecode\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from packaging import version\n",
    "from humanfriendly import format_timespan\n",
    "from sklearn.metrics import confusion_matrix, classification_report, hamming_loss, zero_one_loss, f1_score, roc_auc_score\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "date_format='%Y-%m-%d %H-%M-%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_docs as tfdocs #!pip install git+https://github.com/tensorflow/docs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "\n",
    "from tensorflow.keras import layers, regularizers, models\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.models import load_model, model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_docs as tfdocs #!pip install git+https://github.com/tensorflow/docs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "\n",
    "from tensorflow.keras import layers, regularizers, models\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.models import load_model, model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset from part 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\dataset_part_3.1_13072020.pkl\"))\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_actors'].iloc[0].split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the already trained chosen model\n",
    "This is the model that perfomed better than those trained on part 3. <br>\n",
    "<i> The chosen model is the \"Multi-input keras model\". <i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n",
    "saved_model_name=\"multi_input_keras_model\"\n",
    "saved_model_date=\"16072020\"\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the weights of the best model selected from previous part 3.3\n",
    "\"\"\"\n",
    "def import_weights_keras_model(method, optimizer, hparams):\n",
    "    \"\"\"\n",
    "    Load the weights of the model saved with EarlyStopping\n",
    "    \"\"\"\n",
    "    if method == \"import custom trained model\":\n",
    "        \n",
    "        if optimizer == \"adam\":\n",
    "        \n",
    "            with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.json\".format(folder_path_model_saved,\n",
    "                                                                                                                        saved_model_name,\n",
    "                                                                                                                        str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                        str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                        str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                        str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                        saved_model_date)),'r') as f:\n",
    "                model_json = json.load(f)\n",
    "\n",
    "            model_imported = model_from_json(model_json)\n",
    "\n",
    "            model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                                        saved_model_name,\n",
    "                                                                                                                                        str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                        str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                        str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                        str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                        saved_model_date)))\n",
    "            print(type(model_imported))\n",
    "            print(\"\\nModel's weights have been loaded successfully\\n\")\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.json\".format(folder_path_model_saved,\n",
    "                                                                                                     saved_model_name,\n",
    "                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                     saved_model_date)),'r') as f:\n",
    "                model_json = json.load(f)\n",
    "\n",
    "            model_imported = model_from_json(model_json)\n",
    "\n",
    "            model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                     saved_model_name,\n",
    "                                                                                                                     str(hparams[HP_EMBEDDING_DIM]),\n",
    "                                                                                                                     str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                     saved_model_date)))\n",
    "            print(type(model_imported))\n",
    "            print(\"\\nModel's weights have been loaded successfully\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.json\".format(folder_path_model_saved,\n",
    "                                                                                                                    saved_model_name,\n",
    "                                                                                                                    str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                    str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                    str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                    str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                    saved_model_date)),'r') as f:\n",
    "            model_json = json.load(f)\n",
    "\n",
    "        model_imported = model_from_json(model_json, custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "        model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                                    saved_model_name,\n",
    "                                                                                                                                    str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                    str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                    str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                    str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                    saved_model_date)))\n",
    "        print(type(model_imported))\n",
    "        print(\"\\nModel is loaded successfully\\n\")\n",
    "    \n",
    "    return model_imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                hparams = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate,\n",
    "                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                  }\n",
    "                model = import_weights_keras_model(\"import custom trained model\", \"adam\", hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the saved tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_version_data_control=\"13072020\"\n",
    "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
    "\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\words_tokenized_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control)), 'rb') as handle:\n",
    "    words_tokenized = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMport the tokenizers of each input, fitted on part 3.1\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\actors_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\plot_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\features_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\reviews_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\title_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])),'rb') as f:\n",
    "    title_tokenizer = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    assert len(actors_tokenizer.word_index)==words_tokenized['actors_tokenized']\n",
    "    assert len(plot_tokenizer.word_index)==words_tokenized['plot_words_tokenized']\n",
    "    assert len(features_tokenizer.word_index)==words_tokenized['features_words_tokenized']\n",
    "    assert len(reviews_tokenizer.word_index)==words_tokenized['reviews_words_tokenized']\n",
    "    assert len(title_tokenizer.word_index)==words_tokenized['title_words_tokenized']\n",
    "except AssertionError:\n",
    "    print(\"ERROR: The vocabulary length for some of the tokenizers, is not equal to 20000. Please verify their lengths by running the following: len(actors_tokenizer.word_index)\")\n",
    "\n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two Fucntions have been assembled to complete the word embeddings extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embeddings(variable, model, tokenizer):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "        \n",
    "        embeddings_layer = model.layers[5].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        \n",
    "        print(\"\\nActor's word embeddings length: {}\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "        \n",
    "        embeddings_layer = model.layers[6].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        print(\"Plot Summary's word embeddings length: {}\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "        \n",
    "        embeddings_layer = model.layers[7].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        print(\"Movie's Features word embeddings length: {}\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"reviews\":\n",
    "        \n",
    "        embeddings_layer = model.layers[8].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        print(\"Movie's Reviews word embeddings length: {}\".format(embeddings_layer.shape))\n",
    "        \n",
    "    elif variable == \"title\":\n",
    "        \n",
    "        embeddings_layer = model.layers[9].get_weights()[0]\n",
    "\n",
    "        word_embeddings = {w:embeddings_layer[idx] for w, idx in tokenizer.word_index.items() if idx <= len(tokenizer.word_index)+1}\n",
    "        print(\"Movie's title word embeddings length: {}\".format(embeddings_layer.shape))\n",
    "        \n",
    "    return embeddings_layer, word_embeddings\n",
    "\n",
    "def assign_word_embeddings(variable, dataset, word_embeddings):\n",
    "    \n",
    "    if variable == \"actors\":\n",
    "    \n",
    "        average_vector_list_cast = []\n",
    "\n",
    "        min_vector_list_cast = []\n",
    "\n",
    "        max_vector_list_cast = []\n",
    "\n",
    "        actors_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            actors = dataset.loc[:, \"clean_actors\"].iloc[i].split(\",\")\n",
    "    \n",
    "            assert [word.islower() for word in actors] # assert that all actors are present in lower case\n",
    "    \n",
    "            actors_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in actors])\n",
    "    \n",
    "        dataset.loc[:, 'actors_embeddings_list'] = actors_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            actor_embeddings = dataset[\"actors_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in actor_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in actor_embeddings], axis=0)\n",
    "            average = np.mean([element for element in actor_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_cast.append(minimum)\n",
    "            max_vector_list_cast.append(maximum)\n",
    "            average_vector_list_cast.append(average)\n",
    "\n",
    "        dataset['minimum_cast_vectors'] = min_vector_list_cast\n",
    "        dataset['maximum_cast_vectors'] = max_vector_list_cast\n",
    "        dataset['average_cast_vectors'] = average_vector_list_cast\n",
    "        \n",
    "    elif variable == \"plot\":\n",
    "    \n",
    "        average_vector_list_plot = []\n",
    "\n",
    "        min_vector_list_plot = []\n",
    "\n",
    "        max_vector_list_plot = []\n",
    "\n",
    "        plot_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            plot = dataset[\"clean_plot_summary\"].iloc[i]\n",
    "    \n",
    "            plot_split = plot.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in plot_split]\n",
    "    \n",
    "            plot_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in plot_split])\n",
    "    \n",
    "        dataset['plot_embeddings_list'] = plot_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            plot_embeddings = dataset[\"plot_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in plot_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in plot_embeddings], axis=0)\n",
    "            average = np.mean([element for element in plot_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_plot.append(minimum)\n",
    "            max_vector_list_plot.append(maximum)\n",
    "            average_vector_list_plot.append(average)\n",
    "\n",
    "        dataset['minimum_plot_vectors'] = min_vector_list_plot\n",
    "        dataset['maximum_plot_vectors'] = max_vector_list_plot\n",
    "        dataset['average_plot_vectors'] = average_vector_list_plot\n",
    "        \n",
    "    elif variable == \"features\":\n",
    "    \n",
    "        average_vector_list_combined_features = []\n",
    "\n",
    "        min_vector_list_combined_features = []\n",
    "\n",
    "        max_vector_list_combined_features = []\n",
    "\n",
    "        combined_features_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "    \n",
    "            combined_features = dataset[\"clean_combined_features\"].iloc[i]\n",
    "    \n",
    "            combined_features_split = combined_features.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in combined_features_split]\n",
    "    \n",
    "            combined_features_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in combined_features_split])\n",
    "    \n",
    "        dataset['combined_features_embeddings_list'] = combined_features_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            combined_features_embeddings = dataset[\"combined_features_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in combined_features_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in combined_features_embeddings], axis=0)\n",
    "            average = np.mean([element for element in combined_features_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_combined_features.append(minimum)\n",
    "            max_vector_list_combined_features.append(maximum)\n",
    "            average_vector_list_combined_features.append(average)\n",
    "\n",
    "        dataset['minimum_combined_features_vectors'] = min_vector_list_combined_features\n",
    "        dataset['maximum_combined_features_vectors'] = max_vector_list_combined_features\n",
    "        dataset['average_combined_features_vectors'] = average_vector_list_combined_features\n",
    "\n",
    "    elif variable == \"reviews\":\n",
    "    \n",
    "        average_vector_list_reviews = []\n",
    "\n",
    "        min_vector_list_reviews = []\n",
    "\n",
    "        max_vector_list_reviews = []\n",
    "\n",
    "        reviews_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "    \n",
    "            reviews = dataset[\"clean_reviews\"].iloc[i]\n",
    "    \n",
    "            reviews_split = reviews.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in reviews_split]\n",
    "\n",
    "            reviews_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in reviews_split])\n",
    "\n",
    "        dataset['reviews_embeddings_list'] = reviews_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            reviews_embeddings = dataset[\"reviews_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in reviews_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in reviews_embeddings], axis=0)\n",
    "            average = np.mean([element for element in reviews_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_reviews.append(minimum)\n",
    "            max_vector_list_reviews.append(maximum)\n",
    "            average_vector_list_reviews.append(average)\n",
    "\n",
    "        dataset['minimum_reviews_vectors'] = min_vector_list_reviews\n",
    "        dataset['maximum_reviews_vectors'] = max_vector_list_reviews\n",
    "        dataset['average_reviews_vectors'] = average_vector_list_reviews\n",
    "        \n",
    "    elif variable == \"title\":\n",
    "    \n",
    "        average_vector_list_title = []\n",
    "\n",
    "        min_vector_list_title = []\n",
    "\n",
    "        max_vector_list_title = []\n",
    "\n",
    "        title_embeddings_list = []\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "    \n",
    "            title = dataset[\"clean_movie_title\"].iloc[i]\n",
    "    \n",
    "            title_split = title.split(' ')\n",
    "    \n",
    "            assert [word.islower() for word in title_split]\n",
    "\n",
    "            title_embeddings_list.append([word_embeddings[word] if word in word_embeddings else word_embeddings['<OOV>'] for word in title_split])\n",
    "\n",
    "        dataset['title_embeddings_list'] = title_embeddings_list\n",
    "\n",
    "        for i in range(len(dataset.index)):\n",
    "            \n",
    "            title_embeddings = dataset[\"title_embeddings_list\"].iloc[i]\n",
    "    \n",
    "            minimum = np.min([element for element in title_embeddings], axis=0)\n",
    "            maximum = np.max([element for element in title_embeddings], axis=0)\n",
    "            average = np.mean([element for element in title_embeddings], axis=0)\n",
    "    \n",
    "            min_vector_list_title.append(minimum)\n",
    "            max_vector_list_title.append(maximum)\n",
    "            average_vector_list_title.append(average)\n",
    "\n",
    "        dataset['minimum_title_vectors'] = min_vector_list_title\n",
    "        dataset['maximum_title_vectors'] = max_vector_list_title\n",
    "        dataset['average_title_vectors'] = average_vector_list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the word embeddings\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nExtract the word embeddings\")\n",
    "begin_time=time.time()\n",
    "\n",
    "actors_embedding_layer, word_embeddings_actors = extract_word_embeddings(\"actors\", model, actors_tokenizer)\n",
    "print(\"Word embeddings for actors extracted\\n\")\n",
    "\n",
    "plot_embedding_layer, word_embeddings_plot = extract_word_embeddings(\"plot\", model, plot_tokenizer)\n",
    "print(\"Word embeddings for plot summary extracted\\n\")\n",
    "\n",
    "features_embedding_layer, word_embeddings_features = extract_word_embeddings(\"features\", model, features_tokenizer)\n",
    "print(\"Word embeddings for movie features extracted\\n\")\n",
    "\n",
    "reviews_embedding_layer, word_embeddings_reviews = extract_word_embeddings(\"reviews\", model, reviews_tokenizer)\n",
    "print(\"Word embeddings for movie reviews extracted\\n\")\n",
    "\n",
    "title_embedding_layer, word_embeddings_title = extract_word_embeddings(\"title\", model, title_tokenizer)\n",
    "print(\"Word embeddings for movie reviews extracted\\n\")\n",
    "\n",
    "print(\"Finished word embeddings extraction in: {0}\".format(format_timespan(time.time() - begin_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Assign the word embeddings to each different actor\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nAssign the word embeddings to each different word\\n\")\n",
    "begin_time=time.time()\n",
    "\n",
    "print(\"Assign word embeddings to actors\")\n",
    "assign_word_embeddings(\"actors\", dataset, word_embeddings_actors)\n",
    "\n",
    "print(\"Assign word embeddings to plot summary\")\n",
    "assign_word_embeddings(\"plot\", dataset, word_embeddings_plot)\n",
    "\n",
    "print(\"Assign word embeddings to movie features\")\n",
    "assign_word_embeddings(\"features\", dataset, word_embeddings_features)\n",
    "\n",
    "print(\"Assign word embeddings to movie reviews\")\n",
    "assign_word_embeddings(\"reviews\", dataset, word_embeddings_reviews)\n",
    "\n",
    "print(\"Assign word embeddings to movie titles\")\n",
    "assign_word_embeddings(\"title\", dataset, word_embeddings_title)\n",
    "\n",
    "print(\"\\nFinished word embeddings extraction in: {0}\".format(format_timespan(time.time() - begin_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nThe word embedding vector of the actor 'tobey maguire' is:\\n\\n\", word_embeddings_actors['tobey maguire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time=time.time()\n",
    "keras_embeddings_array_cast = np.hstack([dataset['average_cast_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['minimum_cast_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['maximum_cast_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_plot = np.hstack([dataset['average_plot_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['minimum_plot_vectors'].apply(pd.Series).values,\n",
    "                                         dataset['maximum_plot_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_combined_features = np.hstack([dataset['average_combined_features_vectors'].apply(pd.Series).values,\n",
    "                                                      dataset['minimum_combined_features_vectors'].apply(pd.Series).values,\n",
    "                                                      dataset['maximum_combined_features_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_reviews = np.hstack([dataset['average_reviews_vectors'].apply(pd.Series).values,\n",
    "                                            dataset['minimum_reviews_vectors'].apply(pd.Series).values,\n",
    "                                            dataset['maximum_reviews_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_title = np.hstack([dataset['average_title_vectors'].apply(pd.Series).values,\n",
    "                                          dataset['minimum_title_vectors'].apply(pd.Series).values,\n",
    "                                          dataset['maximum_title_vectors'].apply(pd.Series).values])\n",
    "\n",
    "keras_embeddings_array_cast_plot_combined_features_reviews_title = np.hstack([keras_embeddings_array_cast, \n",
    "                                                                              keras_embeddings_array_plot, \n",
    "                                                                              keras_embeddings_array_combined_features,\n",
    "                                                                              keras_embeddings_array_reviews,\n",
    "                                                                              keras_embeddings_array_title])\n",
    "\n",
    "print(\"Shape of the Actors embeddings: {}\".format(keras_embeddings_array_cast.shape))\n",
    "print(\"\\nShape of the Plot Summary embeddings: {}\".format(keras_embeddings_array_plot.shape))\n",
    "print(\"\\nShape of the Combined Features embeddings: {}\".format(keras_embeddings_array_combined_features.shape))\n",
    "print(\"\\nShape of the Reviews embeddings: {}\".format(keras_embeddings_array_reviews.shape))\n",
    "print(\"\\nShape of the Title embeddings: {}\".format(keras_embeddings_array_title.shape))\n",
    "\n",
    "print(\"\\nShape of the concatenated embeddings(cast, plot, combined features, reviews, title): {}\".format(keras_embeddings_array_cast_plot_combined_features_reviews_title.shape))\n",
    "\n",
    "print(\"\\nFinished word embeddings extraction in: {0}\".format(format_timespan(time.time() - begin_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since the chosen model is the \"Model_1: Multi-Input Keras Model\", we saved the relevant word embeddings to the folder \"model_one\"\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), 'final_word_embeddings_conversational_agent\\\\keras_embeddings_array_cast_{0}.pkl'.format(version_data_control)), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_cast, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'final_word_embeddings_conversational_agent\\\\keras_embeddings_array_plot_{0}.pkl'.format(version_data_control)), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_plot, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'final_word_embeddings_conversational_agent\\\\keras_embeddings_array_combined_features_{0}.pkl'.format(version_data_control)), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_combined_features, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'final_word_embeddings_conversational_agent\\\\keras_embeddings_array_reviews_{0}.pkl'.format(version_data_control)), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_reviews, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'final_word_embeddings_conversational_agent\\\\keras_embeddings_array_title_{0}.pkl'.format(version_data_control)), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_title, f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'final_word_embeddings_conversational_agent\\\\keras_embeddings_array_concatenated_{0}.pkl'.format(version_data_control)), 'wb') as f:\n",
    "    pickle.dump(keras_embeddings_array_cast_plot_combined_features_reviews_title, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_embeddings_array_cast_plot_combined_features_reviews_title[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle the dataset with the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_part_5=dataset[[\"title\", \"actors\", \"clean_combined_features\", \"clean_reviews\", \"imdb_rating\", \"sentiment_value\", \"rating\", \"imdb_url\", \"reduced_genres\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_part_5.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_part_5.to_pickle(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\dataset_part_4_{0}.pkl'.format(version_data_control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF Part 4 - Load the chosen model & Extract Word Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
