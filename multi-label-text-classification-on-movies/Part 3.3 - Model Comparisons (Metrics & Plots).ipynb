{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3 - Model Comparisons (Metrics & Plots)\n",
    "\n",
    "The purpose of the notebook is to select the best model among the 5 models trained. 1 custom neural network and 4 neural network trained on pre-trained word-embeddings.\n",
    "The final model selected will be then to proceed on the next part, the creation of the interactive conversational agent.\n",
    "\n",
    "To select the best model we used the following guidelines:\n",
    "\n",
    "* 1) The model with the lowest hamming loss & zero one loss.\n",
    "* 2) The model with the lowest test score and the highest test accuracy values.\n",
    "* 3) Compare each model's confusion matrix.\n",
    "* 4) Training-Validation learning curves.\n",
    "* 5) Bias-Variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the version data control parameter (to save the outputs of this notebook at their latest date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_data_control=\"31072020\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "try:\n",
    "    collectionsAbc = collections.abc\n",
    "except AttributeError:\n",
    "    collectionsAbc = collections\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "import unidecode\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from packaging import version\n",
    "from humanfriendly import format_timespan\n",
    "from sklearn.metrics import confusion_matrix, classification_report, hamming_loss, zero_one_loss, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "date_format='%Y-%m-%d %H-%M-%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improt visualization libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_docs as tfdocs #!pip install git+https://github.com/tensorflow/docs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "\n",
    "from tensorflow.keras import layers, regularizers, models\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.models import load_model, model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Tensorflow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data already tokenized and transformed from Part 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80-20 split - Non-balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_version_data_control=\"13072020\"\n",
    "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
    "\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\words_tokenized_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control)), 'rb') as handle:\n",
    "    words_tokenized = pickle.load(handle)\n",
    "words_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMport the tokenizers of each input, fitted on part 3.1\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\actors_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\plot_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\features_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\reviews_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), '80-20 split_non-balanced\\\\{0}_{1}\\\\title_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])),'rb') as f:\n",
    "    title_tokenizer = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    assert len(actors_tokenizer.word_index)==words_tokenized['actors_tokenized']\n",
    "    assert len(plot_tokenizer.word_index)==words_tokenized['plot_words_tokenized']\n",
    "    assert len(features_tokenizer.word_index)==words_tokenized['features_words_tokenized']\n",
    "    assert len(reviews_tokenizer.word_index)==words_tokenized['reviews_words_tokenized']\n",
    "    assert len(title_tokenizer.word_index)==words_tokenized['title_words_tokenized']\n",
    "except AssertionError:\n",
    "    print(\"ERROR: The vocabulary length for some of the tokenizers, is not equal to 20000. Please verify their lengths by running the following: len(actors_tokenizer.word_index)\")\n",
    "\n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_version_data_control=\"13072020\"\n",
    "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
    "\n",
    "X_train_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_actors_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])))\n",
    "X_train_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_plot_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])))\n",
    "X_train_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_features_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])))\n",
    "X_train_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_reviews_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])))\n",
    "X_train_seq_title=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_train_seq_title_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])))\n",
    "\n",
    "print(\"X_train data inputs have been loaded!\\n\")\n",
    "\n",
    "X_test_seq_actors=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_actors_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])))\n",
    "X_test_seq_plot=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_plot_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])))\n",
    "X_test_seq_features=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_features_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])))\n",
    "X_test_seq_reviews=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_reviews_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])))\n",
    "X_test_seq_title=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\x_test_seq_title_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])))\n",
    "\n",
    "print(\"X_test data inputs have been loaded!\\n\")\n",
    "\n",
    "y_train=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\y_train_80-20_non-balanced_{1}.npy\".format(tokenization_history_folder, saved_version_data_control)))\n",
    "y_test=np.load(os.path.join(os.getcwd(), \"80-20 split_non-balanced\\\\{0}_{1}\\\\y_test_80-20_non-balanced_{1}.npy\".format(tokenization_history_folder, saved_version_data_control)))\n",
    "\n",
    "print(\"y_train & y_test have been loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the data that have been fiited on the pre-trained saved neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the X_train, X_test, y_train & y_test data pickled from dataset part 3.1\n",
    "\"\"\"\n",
    "saved_version_data_control=\"13072020\"\n",
    "\n",
    "X_train=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\X_train_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "X_test=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\X_test_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "y_train_tf_hub=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\y_train_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "y_test_tf_hub=pd.read_pickle(os.path.join(os.getcwd(), \"pickled_data_per_part\\\\y_test_all_inputs_{0}.pkl\".format(saved_version_data_control)))\n",
    "\n",
    "assert X_train.shape[0]==y_train.shape[0]\n",
    "assert X_test.shape[0]==y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import genres\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'pickled_data_per_part\\\\genres_list_06032020.pkl'.format(tokenization_history_folder, version_data_control)),'rb') as f:\n",
    "    genres_list = pickle.load(f)\n",
    "genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î¤he creation of the data below and their fit on the model estimator will yield an overfitted model estimator\n",
    "# numpy nd.arrays\n",
    "\n",
    "train_text_features = X_train['clean_combined_features'].tolist() #input 1\n",
    "test_text_features = X_test['clean_combined_features'].tolist()\n",
    "\n",
    "train_text_plot = X_train['clean_plot_summary'].tolist() #input 2\n",
    "test_text_plot = X_test['clean_plot_summary'].tolist()\n",
    "\n",
    "train_text_actors = X_train['clean_actors'].tolist() #input 3\n",
    "test_text_actors = X_test['clean_actors'].tolist()\n",
    "\n",
    "train_text_reviews = X_train['clean_reviews'].tolist() #input 4\n",
    "test_text_reviews = X_test['clean_reviews'].tolist()\n",
    "\n",
    "train_text_title = X_train['clean_movie_title'].tolist() #input 5\n",
    "test_text_title = X_test['clean_movie_title'].tolist()\n",
    "\n",
    "train_label = y_train_tf_hub.values\n",
    "test_label = y_test_tf_hub.values\n",
    "\n",
    "train_bytes_list_features = []\n",
    "train_bytes_list_plot = []\n",
    "train_bytes_list_actors = []\n",
    "train_bytes_list_reviews = []\n",
    "train_bytes_list_title = []\n",
    "\n",
    "test_bytes_list_features = []\n",
    "test_bytes_list_plot = []\n",
    "test_bytes_list_actors = []\n",
    "test_bytes_list_reviews = []\n",
    "test_bytes_list_title = []\n",
    "\n",
    "train_bytes_list_features=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_features])\n",
    "train_bytes_list_plot=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_plot])\n",
    "train_bytes_list_actors=np.array([list(map(lambda x: str.encode(unidecode.unidecode(x)), i.split(','))) for i in train_text_actors])\n",
    "# train_bytes_list_actors=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_actors])\n",
    "train_bytes_list_reviews=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_reviews])\n",
    "train_bytes_list_title=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_title])\n",
    "\n",
    "test_bytes_list_features=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_features])\n",
    "test_bytes_list_plot=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_plot])\n",
    "test_bytes_list_actors=np.array([list(map(lambda x: str.encode(unidecode.unidecode(x)), i.split(','))) for i in test_text_actors])\n",
    "# test_bytes_list_actors=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_actors])\n",
    "test_bytes_list_reviews=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_reviews])\n",
    "test_bytes_list_title=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_title])\n",
    "\n",
    "partial_x_train_features, x_val_features, partial_y_train, y_val = train_test_split(train_bytes_list_features, train_label, test_size=0.20, random_state=42)\n",
    "partial_x_train_plot, x_val_plot, partial_y_train, y_val = train_test_split(train_bytes_list_plot, train_label, test_size=0.20, random_state=42)\n",
    "partial_x_train_actors, x_val_actors, partial_y_train, y_val = train_test_split(train_bytes_list_actors, train_label, test_size=0.20, random_state=42)\n",
    "partial_x_train_reviews, x_val_reviews, partial_y_train, y_val = train_test_split(train_bytes_list_reviews, train_label, test_size=0.20, random_state=42)\n",
    "partial_x_train_title, x_val_title, partial_y_train, y_val = train_test_split(train_bytes_list_title, train_label, test_size=0.20, random_state=42)\n",
    "\n",
    "assert partial_x_train_actors.shape[0]==partial_x_train_plot.shape[0]==partial_x_train_features.shape[0]==partial_x_train_reviews.shape[0]==partial_x_train_title.shape[0]==partial_y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Fit the 5 inputs as expected by the TF Hub Model.\n",
    "\n",
    "partial_x_train_features_option1=partial_x_train_features.astype(np.object)\n",
    "partial_x_train_plot_option1=partial_x_train_plot.astype(np.object)\n",
    "partial_x_train_reviews_option1=partial_x_train_reviews.astype(np.object)\n",
    "partial_x_train_title_option1=partial_x_train_title.astype(np.object)\n",
    "partial_x_train_actors_option1=np.asarray([b\" \".join(i) for i in partial_x_train_actors]).astype(np.object)\n",
    "\n",
    "test_bytes_list_features_option1=test_bytes_list_features.astype(np.object)\n",
    "test_bytes_list_plot_option1=test_bytes_list_plot.astype(np.object)\n",
    "test_bytes_list_reviews_option1=test_bytes_list_reviews.astype(np.object)\n",
    "test_bytes_list_title_option1=test_bytes_list_title.astype(np.object)\n",
    "test_bytes_list_actors_option1=np.asarray([b\" \".join(i) for i in test_bytes_list_actors]).astype(np.object)\n",
    "\n",
    "x_val_features_option1=partial_x_train_features.astype(np.object)\n",
    "x_val_plot_option1=x_val_plot.astype(np.object)\n",
    "x_val_reviews_option1=x_val_reviews.astype(np.object)\n",
    "x_val_title_title_option1=x_val_title.astype(np.object)\n",
    "x_val_actors_option1=np.asarray([b\" \".join(i) for i in x_val_actors]).astype(np.object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the functions to be used across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_parameters={}\n",
    "optimizer_parameters={}\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Model Compilation\n",
    "neural_network_parameters['model_loss'] = tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy') #'binary_crossentropy'\n",
    "neural_network_parameters['model_metric'] = [tfa.metrics.HammingLoss(mode=\"multilabel\", name=\"hamming_loss\"),\n",
    "                                             tfa.metrics.F1Score(y_train.shape[-1], average=\"micro\", name=\"f1_score_micro\"), \n",
    "                                             tfa.metrics.F1Score(y_train.shape[-1], average=None, name=\"f1_score_none\"),\n",
    "                                             tfa.metrics.F1Score(y_train.shape[-1], average=\"macro\", name=\"f1_score_macro\")]\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Function 1 - Optimizers\n",
    "\n",
    "# Optimizer: ADAM (Learning scheduler with Inverse Time Decay)\n",
    "\n",
    "optimizer_parameters['lr_scheduler_decay_rate'] = 0.1\n",
    "optimizer_parameters['staircase'] = False\n",
    "optimizer_parameters['validation_split_ratio']=0.7\n",
    "\n",
    "def optimizer_adam_v2(hparams):\n",
    "\n",
    "    return keras.optimizers.Adam(tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate=hparams[HP_LEARNING_RATE],\n",
    "        decay_steps=int(np.ceil((X_train.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS]))*hparams[HP_DECAY_STEPS_MULTIPLIER],\n",
    "        decay_rate=optimizer_parameters['lr_scheduler_decay_rate'],\n",
    "        staircase=optimizer_parameters['staircase']))\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Function 2\n",
    "\n",
    "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
    "    if mode not in ['multiclass', 'multilabel']:\n",
    "        raise TypeError('mode must be: [multiclass, multilabel]')\n",
    "\n",
    "    if mode == 'multiclass':\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), tf.float32)\n",
    "        return nonzero / y_true.shape[-1]\n",
    "\n",
    "class HammingLoss(tfa.metrics.MeanMetricWrapper):\n",
    "    def __init__(self, name='hamming_loss', dtype=None, mode='multilabel'):\n",
    "        super(HammingLoss, self).__init__(\n",
    "                hamming_loss, name, dtype=dtype, mode=mode)\n",
    "        \n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Function 3\n",
    "\n",
    "def import_trained_keras_model(method, decay_steps_mode, embedding_dim_mode, optimizer_name, hparams):\n",
    "    \"\"\"\n",
    "    Load the weights of the model saved with EarlyStopping\n",
    "    \"\"\"\n",
    "    if method == \"import custom trained model\":\n",
    "        \n",
    "        if decay_steps_mode==\"on\":\n",
    "            \n",
    "            if embedding_dim_mode==\"on\":\n",
    "            \n",
    "                with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.json\".format(folder_path_model_saved,\n",
    "                                                                                                                            saved_model_name,\n",
    "                                                                                                                            str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                            str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                            str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                            str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                            saved_model_version)),'r') as f:\n",
    "                    model_json = json.load(f)\n",
    "\n",
    "                model_imported = model_from_json(model_json)\n",
    "\n",
    "                model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                                            saved_model_name,\n",
    "                                                                                                                                            str(hparams[HP_EMBEDDING_DIM]),\n",
    "                                                                                                                                            str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                            str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                            str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                            saved_model_version)))\n",
    "            else:\n",
    "                \n",
    "                with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.json\".format(folder_path_model_saved,\n",
    "                                                                                                                     saved_model_name,\n",
    "                                                                                                                     str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                     str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                     saved_model_version)),'r') as f:\n",
    "                    model_json = json.load(f)\n",
    "\n",
    "                model_imported = model_from_json(model_json)\n",
    "\n",
    "                model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                                            saved_model_name,\n",
    "                                                                                                                                            str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                                            str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                            str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                            saved_model_version)))\n",
    "        else:\n",
    "            \n",
    "            with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.json\".format(folder_path_model_saved,\n",
    "                                                                                                     saved_model_name,\n",
    "                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                     saved_model_version)),'r') as f:\n",
    "                model_json = json.load(f)\n",
    "\n",
    "            model_imported = model_from_json(model_json)\n",
    "\n",
    "            model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                     saved_model_name,\n",
    "                                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                     saved_model_version))) \n",
    "        if optimizer_name==\"adam\":\n",
    "            optimizer = optimizer_adam_v2(hparams)\n",
    "        \n",
    "        elif optimizer_name==\"sgd\":\n",
    "            optimizer = optimizer_sgd_v1(hparams, \"step decay\")\n",
    "            \n",
    "        else:\n",
    "            optimizer = optimizer_rmsprop_v1(hparams)\n",
    "            \n",
    "        model_imported.compile(optimizer=optimizer,\n",
    "                               loss=neural_network_parameters['model_loss'],\n",
    "                               metrics=neural_network_parameters['model_metric'])\n",
    "        print(\"\\nModel is loaded successfully\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        with open(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.json\".format(folder_path_model_saved,\n",
    "                                                                                                             saved_model_name,\n",
    "                                                                                                             str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                             str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                             str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                             saved_model_version)),'r') as f:\n",
    "            model_json = json.load(f)\n",
    "\n",
    "        model_imported = model_from_json(model_json, custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "\n",
    "        model_imported.load_weights(os.path.join(os.getcwd(), '{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.h5'.format(folder_path_model_saved,\n",
    "                                                                                                                             saved_model_name,\n",
    "                                                                                                                             str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                             str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                             str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                             saved_model_version)))\n",
    "\n",
    "        optimizer = optimizer_adam_v2(hparams)\n",
    "\n",
    "        model_imported.compile(optimizer=optimizer,\n",
    "                               loss=neural_network_parameters['model_loss'],\n",
    "                               metrics=neural_network_parameters['model_metric'])\n",
    "        print(\"\\nModel is loaded successfully\\n\")\n",
    "    \n",
    "    return model_imported\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Function 2\n",
    "\n",
    "def create_df_scoring_table(method, decay_steps_mode, embedding_dim_mode, model_tag, hparams, model):\n",
    "    \"\"\"\n",
    "    Create a scoring dictionary to select the best out of the four models\n",
    "    \"\"\"\n",
    "    if method == \"import custom trained model\":\n",
    "        \n",
    "        model_evaluation = model.evaluate([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title],\n",
    "                                          y_test,\n",
    "                                          batch_size=hparams[HP_HIDDEN_UNITS],\n",
    "                                          verbose=2)\n",
    "\n",
    "        y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "        variance = np.var(y_test_predictions)\n",
    "        sse = np.mean((np.mean(y_test_predictions) - y_test)**2)\n",
    "        bias = sse - variance\n",
    "\n",
    "        hamming_loss_value = HammingLoss(mode='multilabel')\n",
    "        hamming_loss_value.update_state(y_test, y_test_predictions)\n",
    "        \n",
    "        if decay_steps_mode==\"on\":\n",
    "            \n",
    "            if embedding_dim_mode==\"on\":\n",
    "            \n",
    "                df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
    "                                        'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n",
    "                                        'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
    "                                        'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
    "                                        'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
    "                                        'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                                        'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                                        'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
    "                                        'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                                        'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
    "                                        'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
    "                                        'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                        'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                        'Bias':pd.Series([bias], dtype='float'),\n",
    "                                        'Variance':pd.Series([variance], dtype='float')\n",
    "                                       })\n",
    "\n",
    "                df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                                     saved_df_scored_metric_name,\n",
    "                                                                                                                                     str(hparams[HP_EMBEDDING_DIM]),\n",
    "                                                                                                                                     str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                                     str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                     saved_model_version)))\n",
    "            else:\n",
    "                \n",
    "                df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
    "                                        'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
    "                                        'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
    "                                        'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
    "                                        'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                                        'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                                        'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
    "                                        'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                                        'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
    "                                        'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
    "                                        'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                        'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                        'Bias':pd.Series([bias], dtype='float'),\n",
    "                                        'Variance':pd.Series([variance], dtype='float')\n",
    "                                       })\n",
    "\n",
    "                df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                              saved_df_scored_metric_name,\n",
    "                                                                                                                              str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                              str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                              str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                              saved_model_version)))    \n",
    "        else:\n",
    "            \n",
    "            df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
    "                                    'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n",
    "                                    'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
    "                                    'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
    "                                    'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                                    'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                                    'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
    "                                    'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                                    'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
    "                                    'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
    "                                    'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                    'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                    'Bias':pd.Series([bias], dtype='float'),\n",
    "                                    'Variance':pd.Series([variance], dtype='float')\n",
    "                                   })\n",
    "\n",
    "            df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                              saved_df_scored_metric_name,\n",
    "                                                                                                              str(hparams[HP_EMBEDDING_DIM]),\n",
    "                                                                                                              str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                              str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                              saved_model_version)))\n",
    "    else:\n",
    "        \n",
    "        model_evaluation = model.evaluate([test_bytes_list_plot_option1, test_bytes_list_features_option1, test_bytes_list_reviews_option1, test_bytes_list_title_option1],\n",
    "                                          test_label,\n",
    "                                          batch_size=hparams[HP_HIDDEN_UNITS],\n",
    "                                          verbose=2)\n",
    "\n",
    "        y_test_pred_probs = model.predict([test_bytes_list_plot_option1, test_bytes_list_features_option1, test_bytes_list_reviews_option1, test_bytes_list_title_option1])\n",
    "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "        variance = np.var(y_test_predictions)\n",
    "        sse = np.mean((np.mean(y_test_predictions) - test_label)**2)\n",
    "        bias = sse - variance\n",
    "\n",
    "        hamming_loss_value = HammingLoss(mode='multilabel')\n",
    "        hamming_loss_value.update_state(test_label, y_test_predictions)\n",
    "\n",
    "        df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
    "                                'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
    "                                'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
    "                                'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
    "                                'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
    "                                'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
    "                                'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
    "                                'Zero_one Loss':pd.Series([zero_one_loss(test_label, y_test_predictions, normalize=False)], dtype='float'),\n",
    "                                'F1_score':pd.Series([f1_score(test_label, y_test_predictions, average=\"micro\")], dtype='float'),\n",
    "                                'F1_score_samples':pd.Series([f1_score(test_label, y_test_predictions, average=\"samples\")], dtype='float'),\n",
    "                                'ROC_score':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                'ROC_score_samples':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
    "                                'Bias':pd.Series([bias], dtype='float'),\n",
    "                                'Variance':pd.Series([variance], dtype='float')\n",
    "                               })\n",
    "\n",
    "        df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                      saved_df_scored_metric_name,\n",
    "                                                                                                                      str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                      str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                      str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                      saved_model_version)))\n",
    "        \n",
    "    \n",
    "    return df_scores\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Function 5\n",
    "\n",
    "def create_confusion_matrix(mode, decay_steps_mode, embedding_dim_mode,  model, hparams):\n",
    "\n",
    "    if mode == \"custom trained model\":\n",
    "        \n",
    "        if decay_steps_mode==\"on\":\n",
    "            \n",
    "            if embedding_dim_mode==\"on\":\n",
    "        \n",
    "                y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "                y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "                conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "                conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                         columns=genres_list,\n",
    "                                         index=genres_list)\n",
    "\n",
    "                conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                                       \"confusion_matrix\",\n",
    "                                                                                                                                       str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                       str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                                       str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                                       str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                       saved_model_version)))\n",
    "            else:\n",
    "                \n",
    "                y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "                y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "                conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "                conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                         columns=genres_list,\n",
    "                                         index=genres_list)\n",
    "\n",
    "                conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                                \"confusion_matrix\",\n",
    "                                                                                                                                str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                                str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                                str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                                saved_model_version)))\n",
    "        else:\n",
    "            y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
    "            y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "            conf_mat=confusion_matrix(y_test.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "            conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                     columns=genres_list,\n",
    "                                     index=genres_list)\n",
    "\n",
    "            conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                \"confusion_matrix\",\n",
    "                                                                                                                str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                saved_model_version)))\n",
    "    else:\n",
    "        \n",
    "        y_test_pred_probs = model.predict([test_bytes_list_plot_option1, test_bytes_list_features_option1, test_bytes_list_reviews_option1, test_bytes_list_title_option1])\n",
    "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
    "\n",
    "        conf_mat=confusion_matrix(test_label.argmax(axis=1), y_test_predictions.argmax(axis=1))\n",
    "\n",
    "        conf_matrix=pd.DataFrame(conf_mat,\n",
    "                                 columns=genres_list,\n",
    "                                 index=genres_list)\n",
    "\n",
    "        conf_matrix.to_pickle(os.path.join(os.getcwd(), \"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
    "                                                                                                                        \"confusion_matrix\",\n",
    "                                                                                                                        str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                                                                                        str(hparams[HP_LEARNING_RATE]), \n",
    "                                                                                                                        str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
    "                                                                                                                        saved_model_version)))\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - Multi-Input Keras neural network (32 dim - 100 hidden units per hidden layer - 0.001 lr - 10 decay steps multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Neural Network\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([100]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "                hparams_adam = {\n",
    "                    HP_HIDDEN_UNITS: batch_size,\n",
    "                    HP_EMBEDDING_DIM: embedding_dim,\n",
    "                    HP_LEARNING_RATE: learning_rate,\n",
    "                    HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "                  }\n",
    "                saved_model_version=\"16072020\"\n",
    "                folder_path_model_saved=\"model_one\\\\adam_v2_models_20072020\"\n",
    "                saved_model_name=\"multi_input_keras_model\"\n",
    "                saved_df_scored_metric_name=\"df_metrics_multi_input_keras_model\"\n",
    "                model_one = import_trained_keras_model(\"import custom trained model\", \"on\", \"on\", \"adam\", hparams_adam)\n",
    "                df_one=pd.read_pickle(\"{0}\\\\{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved, saved_df_scored_metric_name, embedding_dim, batch_size, learning_rate, decay_steps_multiplier, saved_model_version))\n",
    "                confusion_matrix_one=create_confusion_matrix(\"custom trained model\", \"on\", \"on\", model_one, hparams_adam)\n",
    "                history_dataframe_one=pd.read_pickle(os.path.join(os.getcwd(), \"{0}\\\\metrics_histogram_multi_input_keras_{1}dim_{2}batchsize_{3}lr_{4}decaymultiplier_16072020.pkl\".format(folder_path_model_saved, str(embedding_dim), str(batch_size), str(learning_rate), str(decay_steps_multiplier))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Neural network trained with GloVe embeddings (64 hidden units per hidden layer - 0.001lr - 20 decay step multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe embeddings\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([64]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([20]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "        for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "            hparams_glove = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "                HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "              }\n",
    "            saved_model_version=\"18072020\"\n",
    "            folder_path_model_saved=\"model_two\\\\glove_models_20072020\"\n",
    "            saved_model_name=\"glove_embeddings\"\n",
    "            saved_df_scored_metric_name=\"df_metrics_glove\"\n",
    "            model_two = import_trained_keras_model(\"import custom trained model\", \"on\", \"off\", \"adam\", hparams_glove)\n",
    "            df_two=pd.read_pickle(\"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved, saved_df_scored_metric_name, batch_size, learning_rate, decay_steps_multiplier, saved_model_version))\n",
    "            confusion_matrix_two=create_confusion_matrix(\"custom trained model\", \"on\", \"off\", model_two, hparams_glove)\n",
    "            history_dataframe_two=pd.read_pickle(os.path.join(os.getcwd(), \"{0}\\\\metrics_histogram_glove_embeddings_{1}batchsize_{2}lr_{3}decaymultiplier_18072020.pkl\".format(folder_path_model_saved, str(batch_size), str(learning_rate), str(decay_steps_multiplier))))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 - English Google News 130gb corpus without OOV token (32 hidden units per hidden layer - 0.01lr - 10 decay step multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# English_Google_News_130GB_20dim_without_OOV\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.01]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "        for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "            hparams_three = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "                HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "              }\n",
    "            saved_model_version=\"22072020\"\n",
    "            folder_path_model_saved=\"model_three\\\\english_google_news_20dim_no_OOV\"\n",
    "            saved_model_name=\"English_Google_News_130GB_20dim_without_OOV\"\n",
    "            saved_df_scored_metric_name=\"df_metrics_English_Google_News_130GB_20dim_without_OOV\"\n",
    "            model_three = import_trained_keras_model(\"import pre-trained model\", \"on\", \"off\", \"adam\", hparams_three)\n",
    "            df_three=pd.read_pickle(\"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved, saved_df_scored_metric_name, batch_size, learning_rate, decay_steps_multiplier, saved_model_version))\n",
    "            confusion_matrix_three=create_confusion_matrix(\"custom pre-trained model\", \"on\", \"off\", model_three, hparams_three)\n",
    "            history_dataframe_three=pd.read_pickle(os.path.join(os.getcwd(), \"{0}\\\\metrics_histogram_English_Google_News_130GB_20dim_without_OOV_{1}batchsize_{2}lr_{3}decaymultiplier_22072020.pkl\".format(folder_path_model_saved, str(batch_size), str(learning_rate), str(decay_steps_multiplier))))                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4 - English Google News 130gb corpus with OOV token (16 hidden units per hidden layer - 0.001lr - 20 decay steps multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English_google_news_130GB_20dim_with_oov\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([16]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([20]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "        for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "            hparams_four = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "                HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "              }\n",
    "            saved_model_version=\"22072020\"\n",
    "            folder_path_model_saved=\"model_four\\\\english_google_news_20dim_with_OOV\"\n",
    "            saved_model_name=\"model_english_google_news_130GB_20dim_with_oov\"\n",
    "            saved_df_scored_metric_name=\"df_metrics_English_Google_News_130GB_20dim_with_OOV\"\n",
    "            model_four = import_trained_keras_model(\"import pre-trained model\", \"on\", \"off\", \"adam\", hparams_four)\n",
    "            df_four=pd.read_pickle(\"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved, saved_df_scored_metric_name, batch_size, learning_rate, decay_steps_multiplier, saved_model_version))\n",
    "            confusion_matrix_four=create_confusion_matrix(\"custom pre-trained model\", \"on\", \"off\", model_four, hparams_four)\n",
    "            history_dataframe_four=pd.read_pickle(os.path.join(os.getcwd(), \"{0}\\\\metrics_histogram_english_google_news_130GB_20dim_with_oov_{1}batchsize_{2}lr_{3}decaymultiplier_22072020.pkl\".format(folder_path_model_saved, str(batch_size), str(learning_rate), str(decay_steps_multiplier))))                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5 - English Google News 7B corpus with OOV token (32 hidden units per hidden layer - 0.01lr - 20 decay steps multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English_google_news_7B_50dim\n",
    "\n",
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.01]))\n",
    "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([20]))\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "        for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
    "            hparams_five = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "                HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
    "              }\n",
    "            saved_model_version=\"22072020\"\n",
    "            folder_path_model_saved=\"model_five\\\\english_google_news_7B_50dim\"\n",
    "            saved_model_name=\"model_english_google_news_7B_50dim\"\n",
    "            saved_df_scored_metric_name=\"df_metrics_english_google_news_7B_50dim_with_OOV\"\n",
    "            model_five = import_trained_keras_model(\"import pre-trained model\", \"on\", \"off\", \"adam\", hparams_five)\n",
    "            df_five=pd.read_pickle(\"{0}\\\\{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved, saved_df_scored_metric_name, batch_size, learning_rate, decay_steps_multiplier, saved_model_version))\n",
    "            confusion_matrix_five=create_confusion_matrix(\"custom pre-trained model\", \"on\", \"off\", model_five, hparams_five)\n",
    "            history_dataframe_five=pd.read_pickle(os.path.join(os.getcwd(), \"{0}\\\\metrics_histogram_english_google_news_7B_50dim_{1}batchsize_{2}lr_{3}decaymultiplier_22072020.pkl\".format(folder_path_model_saved, str(batch_size), str(learning_rate), str(decay_steps_multiplier))))                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison 1: Create a scoring dataframe for each model & comment on the resutls**\n",
    "\n",
    "Step 1 of the selection plan (based on the written thesis documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All frames together\n",
    "frames_df = [df_one, df_two, df_three, df_four, df_five]\n",
    "results_df= pd.concat(frames_df)\n",
    "results_df=results_df.reset_index(drop=True)\n",
    "results_df.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** From the correlation dataframe above, we spotted that Hamming Loss and Test Accuracy have a perfect negative correlation.\n",
    "Thus, we can conclude that the model with the lowest Hamming Loss will also be the model with the highest Test Accuracy.\n",
    "To validate this we checked the dataframe with the model results, and indeed model four what the highest accuracy on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison 2: Create the confusion matrix of each model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_one.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_two.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_three.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_four.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix_five.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison 3: Training and Validation learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metric: Hamming Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colormin = 'black'\n",
    "colormax = 'black'\n",
    "colorother = 'rgb(252, 141, 98)'\n",
    "\n",
    "clrs_acc_model_one = [colormax if history_dataframe_one.val_hamming_loss.iloc[row]==history_dataframe_one.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_one.val_hamming_loss))]\n",
    "clrs_acc_model_two = [colormax if history_dataframe_two.val_hamming_loss.iloc[row]==history_dataframe_two.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_two.val_hamming_loss))]\n",
    "clrs_acc_model_three = [colormax if history_dataframe_three.val_hamming_loss.iloc[row]==history_dataframe_three.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_three.val_hamming_loss))]\n",
    "clrs_acc_model_four = [colormax if history_dataframe_four.val_hamming_loss.iloc[row]==history_dataframe_four.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_four.val_hamming_loss))]\n",
    "clrs_acc_model_five= [colormax if history_dataframe_five.val_hamming_loss.iloc[row]==history_dataframe_five.val_hamming_loss.min() else colorother for row in range(len(history_dataframe_five.val_hamming_loss))]\n",
    "\n",
    "#Hamming Loss of the Custom trained Neural Netowrk\n",
    "fig1=go.Figure()\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig1.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(), \n",
    "                          y=history_dataframe_one.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_one)))\n",
    "\n",
    "fig1.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the Custom trained Neural Netowrk)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig1.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_hamming_loss==history_dataframe_one.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_one.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation hamming loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig1.update_layout(legend_title_text='Training & Validation hamming loss points per epoch')\n",
    "\n",
    "fig1.show()\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Hamming loss of the GloVe embeddings model\n",
    "fig2=go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n",
    "                          y=history_dataframe_two.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig2.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(), \n",
    "                          y=history_dataframe_two.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_two)))\n",
    "\n",
    "fig2.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the GloVe embeddings Neural Network)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig2.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_two.epoch[history_dataframe_two.val_hamming_loss==history_dataframe_two.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_two.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation hamming loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig2.update_layout(legend_title_text='Training & Validation hamming loss points per epoch')\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Hamming Loss of the pre-trained model English Google News 130GB 20dim without OOV token\n",
    "\n",
    "fig3=go.Figure()\n",
    "\n",
    "fig3.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n",
    "                          y=history_dataframe_three.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig3.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(), \n",
    "                          y=history_dataframe_three.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_three)))\n",
    "\n",
    "fig3.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the model English Google News 130GB 20dim without OOV token)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig3.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_three.epoch[history_dataframe_three.val_hamming_loss==history_dataframe_three.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_three.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation hamming loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig3.update_layout(legend_title_text='Training & Validation hamming loss points per epoch')\n",
    "\n",
    "fig3.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Hamming Loss of the pre-trained model English Google News 130GB 20dim with OOV token\n",
    "\n",
    "fig4=go.Figure()\n",
    "\n",
    "fig4.add_trace(go.Scatter(x=history_dataframe_four.epoch.tolist(), \n",
    "                          y=history_dataframe_four.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig4.add_trace(go.Scatter(x=history_dataframe_four.epoch.tolist(), \n",
    "                          y=history_dataframe_four.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_four)))\n",
    "\n",
    "fig4.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the model English Google News 130GB 20dim with OOV token)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig4.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_four.epoch[history_dataframe_four.val_hamming_loss==history_dataframe_four.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_four.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation hamming loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig4.update_layout(legend_title_text='Training & Validation hamming loss points per epoch')\n",
    "\n",
    "fig4.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Hamming Loss of the pre-trained model English Google News 7B 50dim with OOV token\n",
    "\n",
    "fig5=go.Figure()\n",
    "\n",
    "fig5.add_trace(go.Scatter(x=history_dataframe_five.epoch.tolist(), \n",
    "                          y=history_dataframe_five.hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Hamming Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig5.add_trace(go.Scatter(x=history_dataframe_five.epoch.tolist(), \n",
    "                          y=history_dataframe_five.val_hamming_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Hamming Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_acc_model_five)))\n",
    "\n",
    "fig5.update_layout(template=\"simple_white\",\n",
    "                   title=\"Hamming Loss score on train & validation sets (Model estimator of the model English Google News 7B 50dim with OOV token)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Hamming Loss/epoch\")\n",
    "\n",
    "fig5.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_five.epoch[history_dataframe_five.val_hamming_loss==history_dataframe_five.val_hamming_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_five.val_hamming_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation hamming loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=40)])\n",
    "\n",
    "fig5.update_layout(legend_title_text='Training & Validation hamming loss points per epoch')\n",
    "\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Metric: Binary Cross Entropy or Sigmoid Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colormin = 'black'\n",
    "colorother = 'rgb(252, 141, 98)'\n",
    "\n",
    "clrs_loss_model_one=[colormin if history_dataframe_one.val_loss.iloc[row]==history_dataframe_one.val_loss.min() else colorother for row in range(len(history_dataframe_one.val_loss))]\n",
    "clrs_loss_model_two=[colormin if history_dataframe_two.val_loss.iloc[row]==history_dataframe_two.val_loss.min() else colorother for row in range(len(history_dataframe_two.val_loss))]\n",
    "clrs_loss_model_three=[colormin if history_dataframe_three.val_loss.iloc[row]==history_dataframe_three.val_loss.min() else colorother for row in range(len(history_dataframe_three.val_loss))]\n",
    "clrs_loss_model_four=[colormin if history_dataframe_four.val_loss.iloc[row]==history_dataframe_four.val_loss.min() else colorother for row in range(len(history_dataframe_four.val_loss))]\n",
    "clrs_loss_model_five=[colormin if history_dataframe_five.val_loss.iloc[row]==history_dataframe_five.val_loss.min() else colorother for row in range(len(history_dataframe_five.val_loss))]\n",
    "\n",
    "\n",
    "# Loss of the Custom trained Neural Netowrk\n",
    "fig6=go.Figure()\n",
    "\n",
    "fig6.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(),\n",
    "                          y=history_dataframe_one.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig6.add_trace(go.Scatter(x=history_dataframe_one.epoch.tolist(),\n",
    "                          y=history_dataframe_one.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_one)))\n",
    "\n",
    "fig6.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the Custom trained Neural Netowrk)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig6.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_one.epoch[history_dataframe_one.val_loss==history_dataframe_one.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_one.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-40)])\n",
    "fig6.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Loss of SGD optimizer model\n",
    "fig7=go.Figure()\n",
    "\n",
    "fig7.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(),\n",
    "                          y=history_dataframe_two.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig7.add_trace(go.Scatter(x=history_dataframe_two.epoch.tolist(),\n",
    "                          y=history_dataframe_two.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_two)))\n",
    "\n",
    "fig7.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the GloVe embeddings Neural Network)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig7.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_two.epoch[history_dataframe_two.val_loss==history_dataframe_two.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_two.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-40)])\n",
    "fig7.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Loss of the model English Google News 130GB 20dim without OOV token\n",
    "fig8=go.Figure()\n",
    "\n",
    "fig8.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(),\n",
    "                          y=history_dataframe_three.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig8.add_trace(go.Scatter(x=history_dataframe_three.epoch.tolist(),\n",
    "                          y=history_dataframe_three.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_three)))\n",
    "\n",
    "fig8.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the model English Google News 130GB 20dim without OOV token)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig8.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_three.epoch[history_dataframe_three.val_loss==history_dataframe_three.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_three.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-40)])\n",
    "fig8.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Loss of the model English Google News 130GB 20dim with OOV token\n",
    "fig9=go.Figure()\n",
    "\n",
    "fig9.add_trace(go.Scatter(x=history_dataframe_four.epoch.tolist(),\n",
    "                          y=history_dataframe_four.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig9.add_trace(go.Scatter(x=history_dataframe_four.epoch.tolist(),\n",
    "                          y=history_dataframe_four.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_four)))\n",
    "\n",
    "fig9.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the model English Google News 130GB 20dim with OOV token)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig9.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_four.epoch[history_dataframe_four.val_loss==history_dataframe_four.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_four.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-40)])\n",
    "fig9.show()\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#Loss of the pre-trained model English Google News 7B 50dim with OOV token\n",
    "fig10=go.Figure()\n",
    "\n",
    "fig10.add_trace(go.Scatter(x=history_dataframe_five.epoch.tolist(),\n",
    "                          y=history_dataframe_five.loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Training Loss',\n",
    "                          line=dict(color='rgb(102, 194, 165)')))\n",
    "\n",
    "fig10.add_trace(go.Scatter(x=history_dataframe_five.epoch.tolist(),\n",
    "                          y=history_dataframe_five.val_loss.tolist(),\n",
    "                          mode='lines+markers',\n",
    "                          name='Validation Loss',\n",
    "                          line=dict(color='rgb(252, 141, 98)'),\n",
    "                          marker=dict(color=clrs_loss_model_five)))\n",
    "\n",
    "fig10.update_layout(template=\"simple_white\",\n",
    "                   title=\"Loss score on train & validation sets (Model estimator of the pre-trained model English Google News 7B 50dim with OOV token)\",\n",
    "                   xaxis_title=\"Number of epochs\",\n",
    "                   yaxis_title=\"Loss/epoch\")\n",
    "\n",
    "fig10.update_layout(showlegend=True,\n",
    "                   annotations=[dict(x=history_dataframe_five.epoch[history_dataframe_five.val_loss==history_dataframe_five.val_loss.min()].tolist()[0],\n",
    "                                     y=history_dataframe_five.val_loss.min(),\n",
    "                                     xref=\"x\",yref=\"y\",\n",
    "                                     text=\"Epoch with the lowest validation loss\",\n",
    "                                     showarrow=True,\n",
    "                                     arrowhead=5,\n",
    "                                     ax=0,ay=-40)])\n",
    "fig10.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
