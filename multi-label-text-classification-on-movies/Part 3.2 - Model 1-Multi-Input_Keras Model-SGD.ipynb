{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgZbKO4uqaBU"
   },
   "source": [
    "### Part 3.2 - Model 1: Multi-Input Keras neural model (Train the model with the SGD Optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyyZyGHmqaBW"
   },
   "source": [
    "Set the version data control parameter (to save the outputs of this notebook at their latest date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQrNzPlZqaBd"
   },
   "outputs": [],
   "source": [
    "version_data_control=\"16072020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2Fdg3ayriaa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADdvr9qsqaCA"
   },
   "outputs": [],
   "source": [
    "# Install those libraries if the notebook is executed on Google Colab\n",
    "\n",
    "!pip install --quiet git+https://github.com/tensorflow/docs\n",
    "!pip install --quiet humanfriendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBom_NSkqaCX"
   },
   "source": [
    "#### Import standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "KQnMD9-RqaCZ",
    "outputId": "3dab70d4-5480-4684-8b95-ac3783cdf381"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from packaging import version\n",
    "from humanfriendly import format_timespan\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "date_format='%Y-%m-%d %H-%M-%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TU0naysuqaCh"
   },
   "source": [
    "#### Improt visualization libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAdS634ZqaCi"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmIkte00qaCs"
   },
   "source": [
    "#### Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqOUFD3LqaCx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_docs as tfdocs #!pip install git+https://github.com/tensorflow/docs\n",
    "import tensorflow_docs.plots as tfplots\n",
    "import tensorflow_docs.modeling as tfmodel\n",
    "\n",
    "from tensorflow.keras import layers, regularizers, models\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.models import load_model, model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pDT8gdjQqaC5",
    "outputId": "bd3da098-7895-4efd-bcea-68d4da00bbc2"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "number_cores=mp.cpu_count()\n",
    "print(\"Number of available cores: {0}\".format(number_cores))\n",
    "tf.config.threading.set_intra_op_parallelism_threads(number_cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z7pPJfnqaC9"
   },
   "source": [
    "#### Import Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM-VVKDCqaC-"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HEOK4vHqaDA"
   },
   "source": [
    "#### Clear any logs from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iln8VPBUqaDB"
   },
   "outputs": [],
   "source": [
    "logging_directory=os.path.join(os.getcwd(), \"model_one/sgd/logs/hparam_tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVqEN8JSqaDD"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(logging_directory, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMGmzuj4qaDG"
   },
   "source": [
    "#### Import Tensorflow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lf9wbR8nqaDH"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXjhsz9xqaDK"
   },
   "source": [
    "#### Import Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VaivodrqaDK"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np19BgHMqaDN"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFZcmKSAqaDP"
   },
   "source": [
    "#### Import the data already tokenized and transformed from Part 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sCby1uEqaDQ"
   },
   "source": [
    "* 80-20 split - Non-balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IDBiRpdqaDQ"
   },
   "outputs": [],
   "source": [
    "saved_version_data_control=\"13072020\"\n",
    "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
    "\n",
    "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/words_tokenized_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control)), 'rb') as handle:\n",
    "    words_tokenized = pickle.load(handle)\n",
    "words_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESvqKw2CqaDW"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the tokenizers of each input\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/actors_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])),'rb') as f:\n",
    "    actors_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/plot_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])),'rb') as f:\n",
    "    plot_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/features_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])),'rb') as f:\n",
    "    features_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/reviews_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])),'rb') as f:\n",
    "    reviews_tokenizer = pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/title_tokenizer_{2}_{1}.pkl'.format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])),'rb') as f:\n",
    "    title_tokenizer = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    assert len(actors_tokenizer.word_index)==words_tokenized['actors_tokenized']\n",
    "    assert len(plot_tokenizer.word_index)==words_tokenized['plot_words_tokenized']\n",
    "    assert len(features_tokenizer.word_index)==words_tokenized['features_words_tokenized']\n",
    "    assert len(reviews_tokenizer.word_index)==words_tokenized['reviews_words_tokenized']\n",
    "    assert len(title_tokenizer.word_index)==words_tokenized['title_words_tokenized']\n",
    "except AssertionError:\n",
    "    print(\"ERROR: The vocabulary length for some of the tokenizers, is not equal to 20000. Please verify their lengths by running the following: len(actors_tokenizer.word_index)\")\n",
    "\n",
    "print(\"Tokenizers are loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tMRgl34qaDb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the cleaned, preprocessed and padded X independent features and the target variable y\n",
    "\"\"\"\n",
    "saved_version_data_control=\"13072020\"\n",
    "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
    "\n",
    "X_train_seq_actors=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_train_seq_actors_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])))\n",
    "X_train_seq_plot=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_train_seq_plot_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])))\n",
    "X_train_seq_features=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_train_seq_features_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])))\n",
    "X_train_seq_reviews=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_train_seq_reviews_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])))\n",
    "X_train_seq_title=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_train_seq_title_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])))\n",
    "\n",
    "print(\"X_train data inputs have been loaded!\\n\")\n",
    "\n",
    "X_test_seq_actors=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_test_seq_actors_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['actors_tokenized'])))\n",
    "X_test_seq_plot=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_test_seq_plot_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['plot_words_tokenized'])))\n",
    "X_test_seq_features=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_test_seq_features_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['features_words_tokenized'])))\n",
    "X_test_seq_reviews=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_test_seq_reviews_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['reviews_words_tokenized'])))\n",
    "X_test_seq_title=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/x_test_seq_title_80-20_non-balanced_{2}_{1}.npy\".format(tokenization_history_folder, saved_version_data_control, words_tokenized['title_words_tokenized'])))\n",
    "\n",
    "print(\"X_test data inputs have been loaded!\\n\")\n",
    "\n",
    "y_train=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/y_train_80-20_non-balanced_{1}.npy\".format(tokenization_history_folder, saved_version_data_control)))\n",
    "y_test=np.load(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/y_test_80-20_non-balanced_{1}.npy\".format(tokenization_history_folder, saved_version_data_control)))\n",
    "\n",
    "print(\"y_train & y_test have been loaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tp9AO6a4hSJ7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import genres\n",
    "\"\"\"\n",
    "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/genres_list_06032020.pkl'.format(tokenization_history_folder, saved_version_data_control)),'rb') as f:\n",
    "    genres_list = pickle.load(f)\n",
    "genres_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJtCPpocqaDq"
   },
   "source": [
    "## <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FBm4UthqaDr"
   },
   "source": [
    "#### Python Cell no.1\n",
    "--------------------------\n",
    "\n",
    "Define the functions that will be used accross the notebook. <br>\n",
    "* visualise_model: This function is used to create a more informative image of the neural network structure.\n",
    "\n",
    "* callback: The callbacks function with Early Stopping, Model checkpoints and Learning rate monitoring.\n",
    "\n",
    "* save_model: Save the model's weights and structure at the end of the training\n",
    "\n",
    "* plot_keras_history: Plot the learning curves of validation and training datasets across the training epochs\n",
    "\n",
    "* hamming_loss: calculate the hamming loss value to evaluate the performance of the algorithm on test data. Not to be confused to the Hamming loss used as the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBUjE6hCqaDr"
   },
   "outputs": [],
   "source": [
    "# Function 1 - Visualize Neural Network structure.\n",
    "\n",
    "def visualize_model(model):\n",
    "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "#------------------------------------------#------------------------------------------\n",
    "\n",
    "# Fucntion 2 - Callback function with early stopping to avod overfit.\n",
    "\n",
    "def callback(mode, folder_path, saved_model_name, patience_value, logdir, hparams):\n",
    "    \n",
    "    # Initialize parameters\n",
    "    monitor_metric = 'val_loss'\n",
    "    minimum_delta = 0.009\n",
    "    patience_limit = patience_value\n",
    "    verbose_value = 1\n",
    "    mode_value = 'min'\n",
    "    weights_fname = os.path.join(os.getcwd(), '{0}/{1}.h5'.format(folder_path, saved_model_name))\n",
    "    \n",
    "    if mode == \"step decay\":\n",
    "        # Initialize callbacks\n",
    "        callbacks = [\n",
    "            \n",
    "            EarlyStopping(monitor=monitor_metric,\n",
    "                          min_delta=minimum_delta,\n",
    "                          patience=patience_limit,\n",
    "                          verbose=verbose_value,\n",
    "                          mode=mode_value,\n",
    "                          restore_best_weights=True),\n",
    "\n",
    "            ModelCheckpoint(filepath=weights_fname,\n",
    "                            monitor=monitor_metric,\n",
    "                            verbose=verbose_value,\n",
    "                            save_best_only=True,\n",
    "                            save_weights_only=True),\n",
    "            \n",
    "            tf.keras.callbacks.TensorBoard(logdir),\n",
    "            \n",
    "            hp.KerasCallback(logdir, hparams),\n",
    "\n",
    "            tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=1)\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        # Initialize callbacks\n",
    "        callbacks = [\n",
    "            \n",
    "            EarlyStopping(monitor=monitor_metric,\n",
    "                          min_delta=minimum_delta,\n",
    "                          patience=patience_limit,\n",
    "                          verbose=verbose_value,\n",
    "                          mode=mode_value,\n",
    "                          restore_best_weights=True),\n",
    "\n",
    "            ModelCheckpoint(filepath=weights_fname,\n",
    "                            monitor=monitor_metric,\n",
    "                            verbose=verbose_value,\n",
    "                            save_best_only=True,\n",
    "                            save_weights_only=True),\n",
    "            \n",
    "            tf.keras.callbacks.TensorBoard(logdir),\n",
    "            \n",
    "            hp.KerasCallback(logdir, hparams)\n",
    "        ]\n",
    "\n",
    "    return callbacks\n",
    "\n",
    "#------------------------------------------#------------------------------------------\n",
    "\n",
    "# Fucntion 3 - Save the neural network's weights and structure\n",
    "\n",
    "def save_model(model, folder_path, model_name):\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "\n",
    "    with open(os.path.join(os.getcwd(), \"{0}/{1}.json\".format(folder_path, model_name)), \"w\") as json_file:\n",
    "        json.dump(model_json, json_file)\n",
    "\n",
    "    model.save_weights(os.path.join(os.getcwd(), \"{0}/{1}.h5\".format(folder_path, model_name)))\n",
    "    \n",
    "    print(\"\\nModel's weights are saved\")\n",
    "\n",
    "#------------------------------------------#------------------------------------------\n",
    "\n",
    "# Fucntion 4 - Plot the learning curves of the neural network\n",
    "\n",
    "def plot_keras_history(history, folder_path, embeddings_dimension, batch_size, lr_value, version_data_control):\n",
    "    \n",
    "    metrics_names = [key for key in history.keys() if not key.startswith('val_')][0:3]\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    fig, axs = plt.subplots(3, figsize=(20,18))\n",
    "    \n",
    "    for i, metric in enumerate(metrics_names):\n",
    "        \n",
    "        # getting the training values\n",
    "        metric_train_values = history.get(metric, [])\n",
    "\n",
    "        # getting the validation values\n",
    "        metric_val_values = history.get(\"val_{0}\".format(metric), [])\n",
    "\n",
    "        # As loss always exists as a metric we use it to find the \n",
    "        epochs = range(1, len(metric_train_values) + 1)\n",
    "        \n",
    "        # leaving extra spaces to allign with the validation text\n",
    "        training_text = \"   Training {}: {:.5f}\".format(metric,\n",
    "                                                        metric_train_values[-1])\n",
    "        print(training_text)\n",
    "\n",
    "        axs[i].plot(epochs, \n",
    "                    metric_train_values,\n",
    "                    'b',\n",
    "                    label=training_text,\n",
    "                    marker=\".\")\n",
    "        \n",
    "        axs[i].yaxis.label.set_color('white')\n",
    "        axs[i].xaxis.label.set_color('white')\n",
    "        \n",
    "        axs[i].tick_params(axis='x', colors='white')\n",
    "        axs[i].tick_params(axis='y', colors='white')\n",
    "        \n",
    "        axs[i].title.set_color('white')\n",
    "        axs[i].spines['left'].set_color('white')\n",
    "        axs[i].spines['bottom'].set_color('white')\n",
    "\n",
    "        # if we validation metric exists, then plot that as well\n",
    "        if metric_val_values:\n",
    "            \n",
    "            validation_text = \"Validation {}: {:.5f}\".format(metric,\n",
    "                                                             metric_val_values[-1])\n",
    "\n",
    "            axs[i].plot(epochs,\n",
    "                        metric_val_values,\n",
    "                        'g',\n",
    "                        label=validation_text,\n",
    "                        marker=\".\")\n",
    "            \n",
    "            axs[i].yaxis.label.set_color('white')\n",
    "            axs[i].xaxis.label.set_color('white')\n",
    "\n",
    "            axs[i].tick_params(axis='x', colors='white')\n",
    "            axs[i].tick_params(axis='y', colors='white')\n",
    "\n",
    "            axs[i].title.set_color('white')\n",
    "            axs[i].spines['left'].set_color('white')\n",
    "            axs[i].spines['bottom'].set_color('white')\n",
    "        \n",
    "        # add title, xlabel, ylabe, and legend\n",
    "        axs[i].set_title('Model Metric: {}'.format(metric))\n",
    "        axs[i].set_xlabel('Epochs')\n",
    "        axs[i].set_ylabel(metric.title())\n",
    "        axs[i].legend()\n",
    "    \n",
    "    fig.savefig(os.path.join(os.getcwd(), '{0}/ploting_training_validation_performance_{1}dim_{2}batchsize_{3}lr_{4}.png'.format(folder_path, str(embeddings_dimension), str(batch_size), str(lr_value), version_data_control)), dpi=100)\n",
    "    plt.show()\n",
    "    plt.draw()\n",
    "    plt.close()\n",
    "            \n",
    "#------------------------------------------#------------------------------------------\n",
    "\n",
    "# Fucntion 5 - Calculate the Hamming loss of a multilabeled dataset\n",
    "\n",
    "#source:https://github.com/tensorflow/addons/issues/305\n",
    "\n",
    "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
    "    if mode not in ['multiclass', 'multilabel']:\n",
    "        raise TypeError('mode must be: [multiclass, multilabel])')\n",
    "\n",
    "    if mode == 'multiclass':\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
    "        print(\"Hamming loss for multi-class classification is: \", 1.0-nonzero)\n",
    "        return 1.0 - nonzero\n",
    "\n",
    "    else:\n",
    "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), tf.float32)\n",
    "        print(\"Hamming loss for multi-label classification is: \", nonzero / y_true.shape[-1] )\n",
    "        return nonzero / y_true.shape[-1]\n",
    "\n",
    "class HammingLoss(tfa.metrics.MeanMetricWrapper):\n",
    "    def __init__(self, name='hamming_loss', dtype=None, mode='multilabel'):\n",
    "        super(HammingLoss, self).__init__(\n",
    "                hamming_loss, name, dtype=dtype, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lz4pDDcDqaDu"
   },
   "source": [
    "#### Python Cell no.2\n",
    "--------------------------\n",
    "\n",
    "Initialize the hyper parameters of the model and the parameters used by the Neural Network not hyperparameter tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JYDnE4-qaEB"
   },
   "source": [
    "**HyperParameters - Initialized**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrDnpSJ-qaEC"
   },
   "outputs": [],
   "source": [
    "hp_logging_directory=os.path.join(os.getcwd(), \"model_one/sgd/logs/hparam_tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQyQkm-rqaEH"
   },
   "outputs": [],
   "source": [
    "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
    "HP_EMBEDDING_DIM = hp.HParam('embedding_dim', hp.Discrete([50, 100, 150]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1])) # Adam default: 0.001, SGD default: 0.01, RMSprop default: 0.001\n",
    "\n",
    "METRIC_ACCURACY = \"hamming_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AT3lT8zOqaEb"
   },
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer(hp_logging_directory).as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_HIDDEN_UNITS, HP_EMBEDDING_DIM, HP_LEARNING_RATE],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='hamming_loss')],\n",
    "  )\n",
    "\n",
    "try:\n",
    "    os.path.exists(hp_logging_directory)\n",
    "    print(\"Directory of hyper parameters logging exists!\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdAh_ztBqaDv"
   },
   "source": [
    "**Initialize the parameters non-tuned and the optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDr-RBoBqaDz"
   },
   "outputs": [],
   "source": [
    "neural_network_parameters = {}\n",
    "fit_parameters = {}\n",
    "optimizer_parameters = {}\n",
    "\n",
    "#======================================================================\n",
    "#           PARAMETERS THAT DEFINE THE NEURAL NETWORK STRUCTURE       =\n",
    "#======================================================================\n",
    "\n",
    "neural_network_parameters['l2_regularization'] = 0.01\n",
    "neural_network_parameters['dropout_rate'] = 0.1\n",
    "neural_network_parameters['dense_activation'] = 'relu'\n",
    "neural_network_parameters['output_activation'] = 'sigmoid'\n",
    "neural_network_parameters['number_target_variables'] = y_train[0].shape[-1]\n",
    "\n",
    "neural_network_parameters['model_loss'] = tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy')\n",
    "\n",
    "neural_network_parameters['model_metric'] = [tfa.metrics.HammingLoss(mode=\"multilabel\", name=\"hamming_loss\"), \n",
    "                                             tfa.metrics.F1Score(y_train[0].shape[-1], average=\"micro\", name=\"f1_score_micro\"), \n",
    "                                             tfa.metrics.F1Score(y_train[0].shape[-1], average=None, name=\"f1_score_none\")]\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "#===================================================\n",
    "#       PARAMETERS THAT DEFINE EACH OPTIMIZER      =\n",
    "#===================================================\n",
    "\n",
    "# Define a function to monitor the learning rate per epoch (Option 1)\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "# learning rate schedule ((Option 2)\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = hparams[HP_LEARNING_RATE]\n",
    "    drop = 0.5\n",
    "    epochs_drop = 20.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# Optimizer: ADAM (Learning scheduler with Inverse Time Decay)\n",
    "\n",
    "optimizer_parameters['lr_scheduler_decay_rate'] = 0.1\n",
    "optimizer_parameters['staircase'] = False\n",
    "optimizer_parameters['validation_split_ratio']=0.7\n",
    "\n",
    "def optimizer_adam_v2(haparms):\n",
    "    \n",
    "    print(\"Decay Steps of Inverse Time Decay: {0}\".format(int(np.ceil((X_train_seq_actors.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS]))*hparams[HP_DECAY_STEPS_MULTIPLIER]))\n",
    "    \n",
    "    return keras.optimizers.Adam(tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate=hparams[HP_LEARNING_RATE],\n",
    "        decay_steps=int(np.ceil((X_train_seq_actors.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS]))*hparams[HP_DECAY_STEPS_MULTIPLIER],\n",
    "        decay_rate=optimizer_parameters['lr_scheduler_decay_rate'],\n",
    "        staircase=optimizer_parameters['staircase']))\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Optimizer: SDG (version 1)\n",
    "\n",
    "optimizer_parameters['SGD_momentum'] = 0.2 #default 0.0\n",
    "optimizer_parameters['SGD_nesterov'] = True #default False\n",
    "\n",
    "def optimizer_sgd_v1(haparms, mode):\n",
    "\n",
    "    if mode==\"step decay\":\n",
    "\n",
    "        return keras.optimizers.SGD(lr=0.0, #Notice that we set the learning rate in the SGD class to 0 to clearly indicate that it is not used.\n",
    "                                    momentum=0.9 #Use a large momentum. Using a larger momentum value will help the optimization algorithm to continue to make updates in the right direction when your learning rate shrinks to small values.\n",
    "                                   )\n",
    "    else:\n",
    "        return keras.optimizers.SGD(lr=hparams[HP_LEARNING_RATE],\n",
    "                                    momentum=optimizer_parameters['SGD_momentum'],\n",
    "                                    nesterov=optimizer_parameters['SGD_nesterov'])\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# Optimizer: RMSprop (version 1)\n",
    "\n",
    "optimizer_parameters['RMSprop_momentum'] = 0.5\n",
    "optimizer_parameters['RMSprop_centered'] = True\n",
    "\n",
    "def optimizer_rmsprop_v1(haparms):\n",
    "    \n",
    "    return keras.optimizers.RMSprop(lr=hparams[HP_LEARNING_RATE],\n",
    "                                    momentum=optimizer_parameters['RMSprop_momentum'],\n",
    "                                    centered=optimizer_parameters['RMSprop_centered'])\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "#==================================================\n",
    "#       PARAMETERS THAT DEFINE THE MODEL FIT      =\n",
    "#==================================================\n",
    "\n",
    "fit_parameters[\"epoch\"] = 150\n",
    "fit_parameters[\"patience_value\"] = 10\n",
    "fit_parameters[\"verbose_fit\"] = 1\n",
    "fit_parameters['validation_data_ratio']=1-optimizer_parameters['validation_split_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0njG-SnqaEe"
   },
   "source": [
    "#### Python Cell no.3\n",
    "--------------------------\n",
    "\n",
    "The function creates the neural network structure, and fits the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8B6_EeRKqaEf"
   },
   "outputs": [],
   "source": [
    "# Initialize variables specific to the training of the model\n",
    "\n",
    "sequential_model_name=\"MultyInput_Keras_Classification_model\"\n",
    "network_structure_file_name=\"network_structure_multy_input_keras\"\n",
    "folder_path_model_saved=\"drive/My Drive/model_one/sgd_{0}\".format(version_data_control)\n",
    "saved_model_name=\"multi_input_keras_model\"\n",
    "saved_metrics_dataframe_name=\"metrics_histogram_multi_input_keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WuhWZ8vqaEh"
   },
   "outputs": [],
   "source": [
    "def create_fit_keras_model(hparams,\n",
    "                           version_data_control,\n",
    "                           optimizer_name,\n",
    "                           validation_method,\n",
    "                           callbacks,\n",
    "                           optimizer_version = None):\n",
    "\n",
    "    sentenceLength_actors = X_train_seq_actors.shape[1]\n",
    "    vocab_size_frequent_words_actors = len(actors_tokenizer.word_index)\n",
    "\n",
    "    sentenceLength_plot = X_train_seq_plot.shape[1]\n",
    "    vocab_size_frequent_words_plot = len(plot_tokenizer.word_index)\n",
    "\n",
    "    sentenceLength_features = X_train_seq_features.shape[1]\n",
    "    vocab_size_frequent_words_features = len(features_tokenizer.word_index)\n",
    "\n",
    "    sentenceLength_reviews = X_train_seq_reviews.shape[1]\n",
    "    vocab_size_frequent_words_reviews = len(reviews_tokenizer.word_index)\n",
    "\n",
    "    sentenceLength_title = X_train_seq_title.shape[1]\n",
    "    vocab_size_frequent_words_title = len(title_tokenizer.word_index)\n",
    "\n",
    "    model = keras.Sequential(name='{0}_{1}dim_{2}batchsize_{3}lr_{4}'.format(sequential_model_name, \n",
    "                                                                             str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                             str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                             str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                             version_data_control))\n",
    "    actors = keras.Input(shape=(sentenceLength_actors,), name='actors_input')\n",
    "    plot = keras.Input(shape=(sentenceLength_plot,), name='plot_input')\n",
    "    features = keras.Input(shape=(sentenceLength_features,), name='features_input')\n",
    "    reviews = keras.Input(shape=(sentenceLength_reviews,), name='reviews_input')\n",
    "    title = keras.Input(shape=(sentenceLength_title,), name='title_input')\n",
    "\n",
    "    emb1 = layers.Embedding(input_dim = vocab_size_frequent_words_actors + 2,\n",
    "                            output_dim = 16,\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_actors,\n",
    "                            name=\"actors_embedding_layer\")(actors)\n",
    "    \n",
    "    encoded_layer1 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling_actors_layer\")(emb1)\n",
    "    \n",
    "    emb2 = layers.Embedding(input_dim = vocab_size_frequent_words_plot + 2,\n",
    "                            output_dim = hparams[HP_EMBEDDING_DIM],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_plot,\n",
    "                            name=\"plot_embedding_layer\")(plot)\n",
    "\n",
    "    encoded_layer2 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling_plot_summary_Layer\")(emb2)\n",
    "\n",
    "    emb3 = layers.Embedding(input_dim = vocab_size_frequent_words_features + 2,\n",
    "                            output_dim = hparams[HP_EMBEDDING_DIM],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_features,\n",
    "                            name=\"features_embedding_layer\")(features)\n",
    "    \n",
    "    encoded_layer3 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling_movie_features_layer\")(emb3)\n",
    "    \n",
    "    emb4 = layers.Embedding(input_dim = vocab_size_frequent_words_reviews + 2,\n",
    "                            output_dim = hparams[HP_EMBEDDING_DIM],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_reviews,\n",
    "                            name=\"reviews_embedding_layer\")(reviews)\n",
    "    \n",
    "    encoded_layer4 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling_user_reviews_layer\")(emb4)\n",
    "\n",
    "    emb5 = layers.Embedding(input_dim = vocab_size_frequent_words_title + 2,\n",
    "                            output_dim = hparams[HP_EMBEDDING_DIM],\n",
    "                            embeddings_initializer = 'uniform',\n",
    "                            mask_zero = True,\n",
    "                            input_length = sentenceLength_title,\n",
    "                            name=\"title_embedding_layer\")(title)\n",
    "    \n",
    "    encoded_layer5 = layers.GlobalMaxPooling1D(name=\"globalmaxpooling_movie_title_layer\")(emb5)\n",
    "\n",
    "    merged = layers.concatenate([encoded_layer1, encoded_layer2, encoded_layer3, encoded_layer4, encoded_layer5], axis=-1)\n",
    "\n",
    "    dense_layer_1 = layers.Dense(hparams[HP_HIDDEN_UNITS],\n",
    "                                 kernel_regularizer=regularizers.l2(neural_network_parameters['l2_regularization']),\n",
    "                                 activation=neural_network_parameters['dense_activation'],\n",
    "                                 name=\"1st_dense_hidden_layer_concatenated_inputs\")(merged)\n",
    "    \n",
    "    layers.Dropout(neural_network_parameters['dropout_rate'])(dense_layer_1)\n",
    "    \n",
    "    output_layer = layers.Dense(neural_network_parameters['number_target_variables'],\n",
    "                                activation=neural_network_parameters['output_activation'],\n",
    "                                name='output_layer')(dense_layer_1)\n",
    "\n",
    "    model = keras.Model(inputs=[actors, plot, features, reviews, title], outputs=output_layer, name='{0}_{1}dim_{2}batchsize_{3}lr_{4}'.format(sequential_model_name, \n",
    "                                                                                                                                               str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                                                               str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                                                               str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                                                               version_data_control))\n",
    "    print(model.summary())\n",
    "    \n",
    "    if optimizer_name==\"adam\" and optimizer_version is None:\n",
    "        \n",
    "        optimizer = optimizer_adam_v2(hparams)\n",
    "        \n",
    "    elif optimizer_name==\"sgd\" and optimizer_version is None:\n",
    "        \n",
    "        optimizer = optimizer_sgd_v1(hparams, \"step decay\")\n",
    "        \n",
    "    elif optimizer_name==\"rmsprop\" and optimizer_version is None:\n",
    "        \n",
    "        optimizer = optimizer_rmsprop_v1(hparams)\n",
    "\n",
    "    print(\"Type of optimizer LR: {0}\".format(optimizer.lr.dtype))\n",
    "\n",
    "    lr_metric = [get_lr_metric(optimizer)]\n",
    "    \n",
    "    if optimizer.lr.dtype == np.float32:\n",
    "\n",
    "        print(\"Learning Rate's type is Float or Integer\")\n",
    "\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=neural_network_parameters['model_loss'],\n",
    "                      metrics=neural_network_parameters['model_metric'] + lr_metric, )\n",
    "    else:\n",
    "        print(\"Learning Rate's type is not Float or Integer, but rather {0}\".format(type(optimizer.lr)))\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=neural_network_parameters['model_loss'],\n",
    "                      metrics=neural_network_parameters['model_metric'])\n",
    "    \n",
    "    #plot model's structure\n",
    "    plot_model(model, to_file=os.path.join(os.getcwd(), '{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}.png'.format(folder_path_model_saved, \n",
    "                                                                                                           network_structure_file_name,\n",
    "                                                                                                           str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                                                                           str(hparams[HP_HIDDEN_UNITS]),\n",
    "                                                                                                           str(hparams[HP_LEARNING_RATE]),\n",
    "                                                                                                           version_data_control)))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    steps_per_epoch=int(np.ceil((X_train_seq_actors.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS]))\n",
    "    \n",
    "    print(\"\\nSteps per epoch on current run: {0}\".format(steps_per_epoch))\n",
    "    \n",
    "    if validation_method==\"validation_split\":\n",
    "\n",
    "        fitted_model=model.fit([X_train_seq_actors, X_train_seq_plot, X_train_seq_features, X_train_seq_reviews, X_train_seq_title],\n",
    "                                y_train,\n",
    "                                steps_per_epoch=int(np.ceil((X_train_seq_actors.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS])),\n",
    "                                epochs=fit_parameters[\"epoch\"],\n",
    "                                batch_size=hparams[HP_HIDDEN_UNITS],\n",
    "                                validation_split=fit_parameters['validation_data_ratio'],\n",
    "                                callbacks=callbacks,\n",
    "                                use_multiprocessing=True\n",
    "                              )\n",
    "\n",
    "    elif validation_method==\"validation_data\":\n",
    "        \n",
    "        fitted_model=model.fit([X_train_seq_actors, X_train_seq_plot, X_train_seq_features, X_train_seq_reviews, X_train_seq_title], \n",
    "                               y_train,\n",
    "                               steps_per_epoch=int(np.ceil((X_train_seq_actors.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS])),\n",
    "                               epochs=fit_parameters[\"epoch\"],\n",
    "                               verbose=fit_parameters[\"verbose_fit\"],\n",
    "                               batch_size=hparams[HP_HIDDEN_UNITS],\n",
    "                               validation_data=([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title],\n",
    "                                                y_test),\n",
    "                               callbacks=callbacks\n",
    "                              )\n",
    "    #save the model\n",
    "    save_model(model,\n",
    "               folder_path_model_saved,\n",
    "               \"{0}_{1}dim_{2}batchsize_{3}lr_{4}\".format(saved_model_name,\n",
    "                                                          str(hparams[HP_EMBEDDING_DIM]), \n",
    "                                                          str(hparams[HP_HIDDEN_UNITS]), \n",
    "                                                          str(hparams[HP_LEARNING_RATE]), \n",
    "                                                          version_data_control))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\nTraining time of the multi-input keras model has finished. Duration {} secs\".format(format_timespan(elapsed_time)))\n",
    "    \n",
    "    evaluation = model.evaluate([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title], y_test, batch_size=hparams[HP_HIDDEN_UNITS], verbose=2)\n",
    "\n",
    "    loss = evaluation[0] # single number\n",
    "    model_metric = evaluation[1:] # is a list of 3 elements (hamming loss, f1_score_micro, f1_score_none)\n",
    "\n",
    "    hamming_loss_value=evaluation[1]\n",
    "    print('Hamming loss value on test data: {0}'.format(hamming_loss_value))\n",
    "\n",
    "    return hamming_loss_value, model, fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7fBaiw-zqaEj"
   },
   "source": [
    "#### Python Cell no.4\n",
    "--------------------------\n",
    "\n",
    "*run* is a method that call the above function *create_fit_keras_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDCOTvgPqaEj"
   },
   "outputs": [],
   "source": [
    "def run(run_dir, hparams, version_data_control, optimizer_name, validation_method, callbacks):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        hamming_loss_value, model, fitted_model = create_fit_keras_model(hparams, version_data_control, optimizer_name, validation_method, callbacks)\n",
    "        \n",
    "        tf.summary.scalar(METRIC_ACCURACY, hamming_loss_value, step=2)\n",
    "    \n",
    "    return model, fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXOSUC75qaEl"
   },
   "source": [
    "#### Python Cell no.5\n",
    "--------------------------\n",
    "\n",
    "#### Prior to fitting the model: \n",
    "\n",
    "* X_train, X_test should have the form of an array with sequence of numbers.\n",
    "* y_train, y_test should have the form of a multi-hot encoded dataframe.\n",
    "\n",
    "<b> General observations: </b>\n",
    "\n",
    "* Reducing batch size can produce a better model (I should grid search on batch size).\n",
    "* Reducing the general number of parameters can produce better results.\n",
    "* Removing the second dense layer improved the results.\n",
    "* Removing regularization also affected the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5gkQEdlqaEm"
   },
   "source": [
    "Everything is set. In the next coding cells the training is executed! <br>\n",
    "<b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oc-O1OLDqaEm"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(os.getcwd(), \"{0}\".format(folder_path_model_saved))) is True:\n",
    "    print(\"Folder already exists!\\n\")\n",
    "else:\n",
    "    print(\"Folder not found!\\n\")\n",
    "    os.mkdir(os.path.join(os.getcwd(), \"{0}\".format(folder_path_model_saved)))\n",
    "    print(\"Folder is created!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2mh6VNQqaEp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training\n",
    "\"\"\"\n",
    "begin_time=time.time()\n",
    "print(\"{0}: Start execution of the cell\\n\".format(datetime.utcnow().strftime(date_format)))\n",
    "\n",
    "session_num = 1\n",
    "\n",
    "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
    "    for embedding_dim in HP_EMBEDDING_DIM.domain.values:\n",
    "        for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "            hparams = {\n",
    "                HP_HIDDEN_UNITS: batch_size,\n",
    "                HP_EMBEDDING_DIM: embedding_dim,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "              }\n",
    "            run_name = \"run-id {0}\".format(session_num)\n",
    "            total_number_models=(len(HP_HIDDEN_UNITS.domain.values)*len(HP_EMBEDDING_DIM.domain.values)*len(HP_LEARNING_RATE.domain.values))\n",
    "            print('--- Starting trial: {0}/{1}\\n'.format(run_name, total_number_models))\n",
    "            print({h.name: hparams[h] for h in hparams}, '\\n')\n",
    "\n",
    "            starting_training=time.time()\n",
    "\n",
    "            model_struture, model_history=run('{0}/'.format(hp_logging_directory) + run_name, \n",
    "                                              hparams, \n",
    "                                              version_data_control, \n",
    "                                              \"sgd\", \n",
    "                                              \"validation_split\",\n",
    "                                              callback(\"step decay\",\n",
    "                                                        folder_path_model_saved, \n",
    "                                                        \"{0}_{1}dim_{2}batchsize_{3}lr_{4}\".format(saved_model_name,\n",
    "                                                                                                  str(embedding_dim), \n",
    "                                                                                                  str(batch_size), \n",
    "                                                                                                  str(learning_rate), \n",
    "                                                                                                  version_data_control),\n",
    "                                                        fit_parameters[\"patience_value\"],\n",
    "                                                        \"{0}/\".format(hp_logging_directory) + datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "                                                        hparams))\n",
    "            \n",
    "            print(\"Average time per epoch: {0}\\n\".format(format_timespan((time.time()-starting_training)/len(model_history.epoch))))\n",
    "            \n",
    "            hist = pd.DataFrame(model_history.history)\n",
    "            hist['epoch'] = model_history.epoch\n",
    "            hist['epoch']+= 1\n",
    "            hist.index += 1\n",
    "            print(\"Table of training the {0} text classification model\\n\".format(sequential_model_name))\n",
    "            print(tabulate(hist, headers='keys', tablefmt='psql'))\n",
    "\n",
    "            hist.to_pickle(os.path.join(os.getcwd(), folder_path_model_saved+\"/{0}_{1}dim_{2}batchsize_{3}lr_{4}.pkl\".format(saved_metrics_dataframe_name,\n",
    "                                                                                                                              str(embedding_dim), \n",
    "                                                                                                                              str(batch_size), \n",
    "                                                                                                                              str(learning_rate),\n",
    "                                                                                                                              version_data_control)))\n",
    "\n",
    "            #plot the model's model_metric (Hamming Loss, F1-score) & loss\n",
    "            plot_keras_history(model_history.history, folder_path_model_saved, embedding_dim, batch_size, learning_rate, version_data_control)\n",
    "\n",
    "            #evaluate the model\n",
    "            model_evaluation = model_struture.evaluate([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title], \n",
    "                                                        y_test,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        verbose=2)\n",
    "\n",
    "            print(\"\\nTest Score (evalution of the model's loss/error on the test sequences): {0}\".format(model_evaluation[0]))\n",
    "            print(\"\\nTest model_metric (evalution of the hamming loss on the test sequences): {0}\\n\".format(model_evaluation[1]))\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            \n",
    "            plt.rcParams[\"figure.figsize\"] = (16,13)\n",
    "            \n",
    "            ax.bar(genres_list, model_evaluation[3])\n",
    "            \n",
    "            ax.set_title('F1 score per genre tag')\n",
    "            ax.set_xlabel('Movie Genre')\n",
    "            ax.set_ylabel('F1 score')\n",
    "            ax.set_xticklabels(genres_list)\n",
    "            \n",
    "            ax.spines['left'].set_color('white')\n",
    "            ax.spines['bottom'].set_color('white')\n",
    "\n",
    "            ax.tick_params(axis='x', colors='white')\n",
    "            ax.tick_params(axis='y', colors='white')\n",
    "            \n",
    "            ax.yaxis.label.set_color('white')\n",
    "            ax.xaxis.label.set_color('white')\n",
    "            ax.title.set_color('white')\n",
    "\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "            for i, f1 in enumerate(model_evaluation[3]):\n",
    "                ax.annotate(round(f1, 2), (i, f1), ha='center', va='bottom')\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "            session_num += 1\n",
    "\n",
    "total_time=time.time() - begin_time\n",
    "print(\"{0}: Total cell execution time: {1}\".format(datetime.utcnow().strftime(date_format), format_timespan(total_time)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Part 3.2 - Model 1-Multi-Input_Keras Model-SGD.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
