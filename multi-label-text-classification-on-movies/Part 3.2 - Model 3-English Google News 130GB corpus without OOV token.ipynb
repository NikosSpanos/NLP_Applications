{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Part 3.2 - Model 3-English Google News 130GB corpus without OOV tokens.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2AqOSnK66Vs9",
        "cq8Cn8uS6VtJ"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymbz-SKr6VsQ",
        "colab_type": "text"
      },
      "source": [
        "### Part 3.2 - Model 3-English Google News 130GB corpus without OOV tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGplgxDN6VsR",
        "colab_type": "text"
      },
      "source": [
        "Set the version data control parameter (to save the outputs of this notebook at their latest date)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGgpEKr66VsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "version_data_control=\"22072020\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giPy73t_7Ig3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQLdH__96VsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install those libraries if the notebook is executed on Google Colab\n",
        "\n",
        "!pip install --quiet git+https://github.com/tensorflow/docs\n",
        "!pip install --quiet humanfriendly\n",
        "!pip install --quiet tqdm\n",
        "!pip install --quiet unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sZbHP036VsX",
        "colab_type": "text"
      },
      "source": [
        "#### Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy6hxhRC6VsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import shutil\n",
        "import unidecode\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from tabulate import tabulate\n",
        "from packaging import version\n",
        "from humanfriendly import format_timespan\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "date_format='%Y-%m-%d %H-%M-%S'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6OVpW8K6VsZ",
        "colab_type": "text"
      },
      "source": [
        "#### Improt visualization libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDWvhJ-x6Vsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "\n",
        "import pydot\n",
        "import pydotplus\n",
        "import graphviz\n",
        "\n",
        "from IPython.display import SVG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DtVRNC16Vsc",
        "colab_type": "text"
      },
      "source": [
        "#### Import Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x_xsulf6Vsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_docs as tfdocs #!pip install git+https://github.com/tensorflow/docs\n",
        "import tensorflow_docs.plots as tfplots\n",
        "import tensorflow_docs.modeling as tfmodel\n",
        "\n",
        "from tensorflow.keras import layers, regularizers, models\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.utils import model_to_dot, plot_model\n",
        "from tensorflow.keras.models import load_model, model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql4k5JGO6Vsf",
        "colab_type": "text"
      },
      "source": [
        "#### Clear any logs from previous runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVPMfgvA6Vsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging_directory=os.path.join(os.getcwd(), \"model_three/logs/hparam_tuning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAi6bG_C6Vsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree(logging_directory, ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VltESOD46Vsk",
        "colab_type": "text"
      },
      "source": [
        "#### Import Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDtge6LW6Vsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "from tensorboard.plugins.hparams import api as hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95sZdVb26Vsm",
        "colab_type": "text"
      },
      "source": [
        "#### Import Tensorflow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_X-WGdi6Vsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcrMOdEE6Vso",
        "colab_type": "text"
      },
      "source": [
        "#### Import Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-GcxFro6Vso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxxXhCqN6Vsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
        "    \"This notebook requires TensorFlow 2.0 or above.\"\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l1dtZh66Vss",
        "colab_type": "text"
      },
      "source": [
        "#### Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8dD8eH46Vss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Import the X_train, X_test, y_train & y_test data pickled from dataset part 3.1\n",
        "\"\"\"\n",
        "saved_version_data_control=\"13072020\"\n",
        "tokenization_history_folder=\"text_tokenization_padded_sequences\"\n",
        "\n",
        "X_train=pd.read_pickle(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/X_train_all_inputs_{1}.pkl\".format(tokenization_history_folder, saved_version_data_control)))\n",
        "X_test=pd.read_pickle(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/X_test_all_inputs_{1}.pkl\".format(tokenization_history_folder, saved_version_data_control)))\n",
        "y_train=pd.read_pickle(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/y_train_all_inputs_{1}.pkl\".format(tokenization_history_folder, saved_version_data_control)))\n",
        "y_test=pd.read_pickle(os.path.join(os.getcwd(), \"drive/My Drive/data/{0}_{1}/y_test_all_inputs_{1}.pkl\".format(tokenization_history_folder, saved_version_data_control)))\n",
        "\n",
        "assert X_train.shape[0]==y_train.shape[0]\n",
        "assert X_test.shape[0]==y_test.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvtVJNqgsojk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Import genres\n",
        "\"\"\"\n",
        "with open(os.path.join(os.getcwd(), 'drive/My Drive/data/{0}_{1}/genres_list_06032020.pkl'.format(tokenization_history_folder, saved_version_data_control)),'rb') as f:\n",
        "    genres_list = pickle.load(f)\n",
        "genres_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK2sbHt56Vsu",
        "colab_type": "text"
      },
      "source": [
        "## <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbIQqy1VutKM",
        "colab_type": "text"
      },
      "source": [
        "Create the data to be used by the pre-trained saved model. Each model expects the data of a specific format. The researcher should read the relative documentation to understand the proper data format per pre-trained save model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVVF6KB-6Vsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text_features = X_train['clean_combined_features'].tolist() #input 1\n",
        "test_text_features = X_test['clean_combined_features'].tolist()\n",
        "\n",
        "train_text_plot = X_train['clean_plot_summary'].tolist() #input 2\n",
        "test_text_plot = X_test['clean_plot_summary'].tolist()\n",
        "\n",
        "train_text_actors = X_train['clean_actors'].tolist() #input 3\n",
        "test_text_actors = X_test['clean_actors'].tolist()\n",
        "\n",
        "train_text_reviews = X_train['clean_reviews'].tolist() #input 4\n",
        "test_text_reviews = X_test['clean_reviews'].tolist()\n",
        "\n",
        "train_text_title = X_train['clean_movie_title'].tolist() #input 5\n",
        "test_text_title = X_test['clean_movie_title'].tolist()\n",
        "\n",
        "train_label = y_train.values\n",
        "test_label = y_test.values\n",
        "\n",
        "train_bytes_list_features = []\n",
        "train_bytes_list_plot = []\n",
        "train_bytes_list_actors = []\n",
        "train_bytes_list_reviews = []\n",
        "train_bytes_list_title = []\n",
        "\n",
        "test_bytes_list_features = []\n",
        "test_bytes_list_plot = []\n",
        "test_bytes_list_actors = []\n",
        "test_bytes_list_reviews = []\n",
        "test_bytes_list_title = []\n",
        "\n",
        "train_bytes_list_features=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_features])\n",
        "train_bytes_list_plot=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_plot])\n",
        "train_bytes_list_actors=np.array([list(map(lambda x: str.encode(unidecode.unidecode(x)), i.split(','))) for i in train_text_actors])\n",
        "train_bytes_list_reviews=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_reviews])\n",
        "train_bytes_list_title=np.array([str.encode(unidecode.unidecode(i)) for i in train_text_title])\n",
        "\n",
        "test_bytes_list_features=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_features])\n",
        "test_bytes_list_plot=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_plot])\n",
        "test_bytes_list_actors=np.array([list(map(lambda x: str.encode(unidecode.unidecode(x)), i.split(','))) for i in test_text_actors])\n",
        "test_bytes_list_reviews=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_reviews])\n",
        "test_bytes_list_title=np.array([str.encode(unidecode.unidecode(i)) for i in test_text_title])\n",
        "\n",
        "partial_x_train_features, x_val_features, partial_y_train, y_val = train_test_split(train_bytes_list_features, train_label, test_size=0.20, random_state=42)\n",
        "partial_x_train_plot, x_val_plot, partial_y_train, y_val = train_test_split(train_bytes_list_plot, train_label, test_size=0.20, random_state=42)\n",
        "partial_x_train_actors, x_val_actors, partial_y_train, y_val = train_test_split(train_bytes_list_actors, train_label, test_size=0.20, random_state=42)\n",
        "partial_x_train_reviews, x_val_reviews, partial_y_train, y_val = train_test_split(train_bytes_list_reviews, train_label, test_size=0.20, random_state=42)\n",
        "partial_x_train_title, x_val_title, partial_y_train, y_val = train_test_split(train_bytes_list_title, train_label, test_size=0.20, random_state=42)\n",
        "\n",
        "assert partial_x_train_actors.shape[0]==partial_x_train_plot.shape[0]==partial_x_train_features.shape[0]==partial_x_train_reviews.shape[0]==partial_x_train_title.shape[0]==partial_y_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKbJfH1ygn-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Option 1: Fit the 5 inputs as expected by the TensorFlow Hub Model.\n",
        "\n",
        "partial_x_train_features_option1=partial_x_train_features.astype(np.object)\n",
        "partial_x_train_plot_option1=partial_x_train_plot.astype(np.object)\n",
        "partial_x_train_reviews_option1=partial_x_train_reviews.astype(np.object)\n",
        "partial_x_train_title_option1=partial_x_train_title.astype(np.object)\n",
        "partial_x_train_actors_option1=np.asarray([b\" \".join(i) for i in partial_x_train_actors]).astype(np.object)\n",
        "\n",
        "test_bytes_list_features_option1=test_bytes_list_features.astype(np.object)\n",
        "test_bytes_list_plot_option1=test_bytes_list_plot.astype(np.object)\n",
        "test_bytes_list_reviews_option1=test_bytes_list_reviews.astype(np.object)\n",
        "test_bytes_list_title_option1=test_bytes_list_title.astype(np.object)\n",
        "test_bytes_list_actors_option1=np.asarray([b\" \".join(i) for i in test_bytes_list_actors]).astype(np.object)\n",
        "\n",
        "x_val_features_option1=partial_x_train_features.astype(np.object)\n",
        "x_val_plot_option1=x_val_plot.astype(np.object)\n",
        "x_val_reviews_option1=x_val_reviews.astype(np.object)\n",
        "x_val_title_title_option1=x_val_title.astype(np.object)\n",
        "x_val_actors_option1=np.asarray([b\" \".join(i) for i in x_val_actors]).astype(np.object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_ngvRcgiPbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Features\", partial_x_train_features_option1[0:10], \"\\n\")\n",
        "print(\"Plot Summary\", partial_x_train_plot_option1[0:10], \"\\n\")\n",
        "print(\"Reviews\", partial_x_train_reviews_option1[0:10], \"\\n\")\n",
        "print(\"Movie Title\", partial_x_train_title_option1[0:10], \"\\n\")\n",
        "print(\"Actors\", partial_x_train_actors_option1[0:10], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNrQZD3z6Vs9",
        "colab_type": "text"
      },
      "source": [
        "## <b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxYoWRIt6Vs-",
        "colab_type": "text"
      },
      "source": [
        "#### Python Cell no.1\n",
        "--------------------------\n",
        "\n",
        "Define the functions that will be used accross the notebook. <br>\n",
        "* visualise_model: This function is used to create a more informative image of the neural network structure.\n",
        "\n",
        "* callback: The callbacks function with Early Stopping, Model checkpoints and Learning rate monitoring.\n",
        "\n",
        "* save_model: Save the model's weights and structure at the end of the training\n",
        "\n",
        "* plot_keras_history: Plot the learning curves of validation and training datasets across the training epochs\n",
        "\n",
        "* hamming_loss: calculate the hamming loss value to evaluate the performance of the algorithm on test data. Not to be confused to the Hamming loss used as the performance metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQC29J5o6Vs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function 1 - Visualize Neural Network structure.\n",
        "\n",
        "def visualize_model(model):\n",
        "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
        "\n",
        "#------------------------------------------#------------------------------------------\n",
        "\n",
        "# Fucntion 2 - Callback function with early stopping to avod overfit.\n",
        "\n",
        "def callback(mode, folder_path, saved_model_name, patience_value, logdir, hparams):\n",
        "    \n",
        "    # Initialize parameters\n",
        "    monitor_metric = 'val_loss'\n",
        "    minimum_delta = 0.009\n",
        "    patience_limit = patience_value\n",
        "    verbose_value = 1\n",
        "    mode_value = 'min'\n",
        "    weights_fname = os.path.join(os.getcwd(), '{0}/{1}.h5'.format(folder_path, saved_model_name))\n",
        "    \n",
        "    if mode == \"step decay\":\n",
        "        # Initialize callbacks\n",
        "        callbacks = [\n",
        "            \n",
        "            EarlyStopping(monitor=monitor_metric,\n",
        "                          min_delta=minimum_delta,\n",
        "                          patience=patience_limit,\n",
        "                          verbose=verbose_value,\n",
        "                          mode=mode_value,\n",
        "                          restore_best_weights=True),\n",
        "\n",
        "            ModelCheckpoint(filepath=weights_fname,\n",
        "                            monitor=monitor_metric,\n",
        "                            verbose=verbose_value,\n",
        "                            save_best_only=True,\n",
        "                            save_weights_only=True),\n",
        "            \n",
        "            tf.keras.callbacks.TensorBoard(logdir),\n",
        "            \n",
        "            hp.KerasCallback(logdir, hparams),\n",
        "\n",
        "            tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=1)\n",
        "        ]\n",
        "\n",
        "    else:\n",
        "        # Initialize callbacks\n",
        "        callbacks = [\n",
        "            \n",
        "            EarlyStopping(monitor=monitor_metric,\n",
        "                          min_delta=minimum_delta,\n",
        "                          patience=patience_limit,\n",
        "                          verbose=verbose_value,\n",
        "                          mode=mode_value,\n",
        "                          restore_best_weights=True),\n",
        "\n",
        "            ModelCheckpoint(filepath=weights_fname,\n",
        "                            monitor=monitor_metric,\n",
        "                            verbose=verbose_value,\n",
        "                            save_best_only=True,\n",
        "                            save_weights_only=True),\n",
        "            \n",
        "            tf.keras.callbacks.TensorBoard(logdir),\n",
        "            \n",
        "            hp.KerasCallback(logdir, hparams)\n",
        "        ]\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "#------------------------------------------#------------------------------------------\n",
        "\n",
        "# Fucntion 3 - Save the neural network's weights and structure\n",
        "\n",
        "def save_model(model, folder_path, model_name):\n",
        "    \n",
        "    model_json = model.to_json()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), \"{0}/{1}.json\".format(folder_path, model_name)), \"w\") as json_file:\n",
        "        json.dump(model_json, json_file)\n",
        "\n",
        "    model.save_weights(os.path.join(os.getcwd(), \"{0}/{1}.h5\".format(folder_path, model_name)))\n",
        "    \n",
        "    print(\"\\nModel's weights are saved\")\n",
        "\n",
        "#------------------------------------------#------------------------------------------\n",
        "\n",
        "# Fucntion 4 - Plot the learning curves of the neural network\n",
        "\n",
        "def plot_keras_history(history, folder_path, batch_size, lr_value, decaymiltiplier, version_data_control):\n",
        "    \n",
        "    metrics_names = [key for key in history.keys() if not key.startswith('val_')][0:3]\n",
        "    \n",
        "    fig = plt.gcf()\n",
        "    fig, axs = plt.subplots(3, figsize=(20,18))\n",
        "    \n",
        "    for i, metric in enumerate(metrics_names):\n",
        "        \n",
        "        # getting the training values\n",
        "        metric_train_values = history.get(metric, [])\n",
        "\n",
        "        # getting the validation values\n",
        "        metric_val_values = history.get(\"val_{0}\".format(metric), [])\n",
        "\n",
        "        # As loss always exists as a metric we use it to find the \n",
        "        epochs = range(1, len(metric_train_values) + 1)\n",
        "        \n",
        "        # leaving extra spaces to allign with the validation text\n",
        "        training_text = \"   Training {}: {:.5f}\".format(metric,\n",
        "                                                        metric_train_values[-1])\n",
        "        print(training_text)\n",
        "\n",
        "        axs[i].plot(epochs, \n",
        "                    metric_train_values,\n",
        "                    'b',\n",
        "                    label=training_text,\n",
        "                    marker=\".\")\n",
        "        \n",
        "        axs[i].yaxis.label.set_color('white')\n",
        "        axs[i].xaxis.label.set_color('white')\n",
        "        \n",
        "        axs[i].tick_params(axis='x', colors='white')\n",
        "        axs[i].tick_params(axis='y', colors='white')\n",
        "        \n",
        "        axs[i].title.set_color('white')\n",
        "        axs[i].spines['left'].set_color('white')\n",
        "        axs[i].spines['bottom'].set_color('white')\n",
        "\n",
        "        # if we validation metric exists, then plot that as well\n",
        "        if metric_val_values:\n",
        "            \n",
        "            validation_text = \"Validation {}: {:.5f}\".format(metric,\n",
        "                                                             metric_val_values[-1])\n",
        "\n",
        "            axs[i].plot(epochs,\n",
        "                        metric_val_values,\n",
        "                        'g',\n",
        "                        label=validation_text,\n",
        "                        marker=\".\")\n",
        "            \n",
        "            axs[i].yaxis.label.set_color('white')\n",
        "            axs[i].xaxis.label.set_color('white')\n",
        "\n",
        "            axs[i].tick_params(axis='x', colors='white')\n",
        "            axs[i].tick_params(axis='y', colors='white')\n",
        "\n",
        "            axs[i].title.set_color('white')\n",
        "            axs[i].spines['left'].set_color('white')\n",
        "            axs[i].spines['bottom'].set_color('white')\n",
        "        \n",
        "        # add title, xlabel, ylabe, and legend\n",
        "        axs[i].set_title('Model Metric: {}'.format(metric))\n",
        "        axs[i].set_xlabel('Epochs')\n",
        "        axs[i].set_ylabel(metric.title())\n",
        "        axs[i].legend()\n",
        "    \n",
        "    fig.savefig(os.path.join(os.getcwd(), '{0}/ploting_training_validation_performance_{1}batchsize_{2}lr_{3}decaymultiplier_{4}.png'.format(folder_path, str(batch_size), str(lr_value), str(decaymiltiplier), version_data_control)), dpi=100)\n",
        "    plt.show()\n",
        "    plt.draw()\n",
        "    plt.close()\n",
        "            \n",
        "#------------------------------------------#------------------------------------------\n",
        "\n",
        "# Fucntion 5 - Calculate the Hamming loss of a multilabeled dataset\n",
        "\n",
        "#source:https://github.com/tensorflow/addons/issues/305\n",
        "\n",
        "def hamming_loss(y_true, y_pred, mode='multilabel'):\n",
        "    if mode not in ['multiclass', 'multilabel']:\n",
        "        raise TypeError('mode must be: [multiclass, multilabel])')\n",
        "\n",
        "    if mode == 'multiclass':\n",
        "        nonzero = tf.cast(tf.math.count_nonzero(y_true * y_pred, axis=-1), tf.float32)\n",
        "        print(\"Hamming loss for multi-class classification is: \", 1.0-nonzero)\n",
        "        return 1.0 - nonzero\n",
        "\n",
        "    else:\n",
        "        nonzero = tf.cast(tf.math.count_nonzero(y_true - y_pred, axis=-1), tf.float32)\n",
        "        print(\"Hamming loss for multi-label classification is: \", nonzero / y_true.shape[-1] )\n",
        "        return nonzero / y_true.shape[-1]\n",
        "\n",
        "class HammingLoss(tfa.metrics.MeanMetricWrapper):\n",
        "    def __init__(self, name='hamming_loss', dtype=None, mode='multilabel'):\n",
        "        super(HammingLoss, self).__init__(\n",
        "                hamming_loss, name, dtype=dtype, mode=mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMfcAD-i6VtA",
        "colab_type": "text"
      },
      "source": [
        "#### Python Cell no.2\n",
        "------------------------------\n",
        "\n",
        "In this below python cell I keep track of the model parameters used to:\n",
        "\n",
        "* create the neural network model,\n",
        "* to fit the neural network,\n",
        "* to optimize the neural network.\n",
        "\n",
        "Storing the values of the parameters to a dictionary, I could then change dynamically the value of a parameter, rerun the neural model and then monitor the difference in the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiNttk9Z6VtD",
        "colab_type": "text"
      },
      "source": [
        "**HyperParameters - Initialized**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkEBIt_O6VtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hp_logging_directory=os.path.join(os.getcwd(), \"model_three/logs/hparam_tuning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9RF8-5N6VtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
        "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10, 20]))\n",
        "\n",
        "METRIC_ACCURACY = \"hamming_loss\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcpNz1YX6VtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3729a11c-0bf8-4614-e9a7-7318691fe015"
      },
      "source": [
        "with tf.summary.create_file_writer(hp_logging_directory).as_default():\n",
        "    hp.hparams_config(\n",
        "    hparams=[HP_HIDDEN_UNITS, HP_LEARNING_RATE, HP_DECAY_STEPS_MULTIPLIER],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='hamming_loss')],\n",
        "  )\n",
        "    \n",
        "try:\n",
        "    os.path.exists(hp_logging_directory)\n",
        "    print(\"Directory of hyper parameters logging exists!\")\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"Directory not found!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory of hyper parameters logging exists!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYewt-g_6VtA",
        "colab_type": "text"
      },
      "source": [
        "**Initialize the parameters non-tuned and the optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5KbsYcS6VtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neural_network_parameters = {}\n",
        "fit_parameters = {}\n",
        "optimizer_parameters = {}\n",
        "\n",
        "#======================================================================\n",
        "#           PARAMETERS THAT DEFINE THE NEURAL NETWORK STRUCTURE       =\n",
        "#======================================================================\n",
        "\n",
        "neural_network_parameters['l2_regularization'] = 0.01\n",
        "neural_network_parameters['dropout_rate'] = 0.1\n",
        "neural_network_parameters['dense_activation'] = 'relu'\n",
        "neural_network_parameters['output_activation'] = 'sigmoid'\n",
        "neural_network_parameters['number_target_variables'] = y_train.iloc[0].shape[-1]\n",
        "\n",
        "neural_network_parameters['model_loss'] = tf.keras.losses.BinaryCrossentropy(from_logits=False, name='binary_crossentropy')\n",
        "\n",
        "neural_network_parameters['model_metric'] = [tfa.metrics.HammingLoss(mode=\"multilabel\", name=\"hamming_loss\"), \n",
        "                                             tfa.metrics.F1Score(y_train.iloc[0].shape[-1], average=\"micro\", name=\"f1_score_micro\"), \n",
        "                                             tfa.metrics.F1Score(y_train.iloc[0].shape[-1], average=None, name=\"f1_score_none\"),\n",
        "                                             tfa.metrics.F1Score(y_train.iloc[0].shape[-1], average=\"macro\", name=\"f1_score_macro\")]\n",
        "\n",
        "#---------------------------------------------------------------------------------------\n",
        "\n",
        "#===================================================\n",
        "#       PARAMETERS THAT DEFINE EACH OPTIMIZER      =\n",
        "#===================================================\n",
        "\n",
        "# Define a function to monitor the learning rate per epoch (Option 1)\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer.lr\n",
        "    return lr\n",
        "\n",
        "# learning rate schedule ((Option 2)\n",
        "def step_decay(epoch, hparams):\n",
        "    initial_lrate = hparams[HP_LEARNING_RATE]\n",
        "    drop = 0.5\n",
        "    epochs_drop = 20.0\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "    \n",
        "# Optimizer: ADAM (Learning scheduler with Inverse Time Decay)\n",
        "\n",
        "optimizer_parameters['lr_scheduler_decay_rate'] = 0.1\n",
        "optimizer_parameters['staircase'] = False\n",
        "optimizer_parameters['validation_split_ratio']=0.7\n",
        "\n",
        "def optimizer_adam_v2(haparms):\n",
        "    \n",
        "    print(\"Decay Steps of Inverse Time Decay: {0}\".format(int(np.ceil((X_train.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS]))*hparams[HP_DECAY_STEPS_MULTIPLIER]))\n",
        "    \n",
        "    return keras.optimizers.Adam(tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "        initial_learning_rate=hparams[HP_LEARNING_RATE],\n",
        "        decay_steps=int(np.ceil((X_train.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS]))*hparams[HP_DECAY_STEPS_MULTIPLIER],\n",
        "        decay_rate=optimizer_parameters['lr_scheduler_decay_rate'],\n",
        "        staircase=optimizer_parameters['staircase']))\n",
        "\n",
        "#---------------------------------------------------------------------------------------\n",
        "\n",
        "#==================================================\n",
        "#       PARAMETERS THAT DEFINE THE MODEL FIT      =\n",
        "#==================================================\n",
        "\n",
        "fit_parameters[\"epoch\"] = 150\n",
        "fit_parameters[\"patience_value\"] = 20\n",
        "fit_parameters[\"verbose_fit\"] = 1\n",
        "fit_parameters['validation_data_ratio']=1-optimizer_parameters['validation_split_ratio']\n",
        "fit_parameters[\"cv_enabled\"]=\"0\"\n",
        "fit_parameters[\"option_method\"]=\"option1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnEhDS-M6VtJ",
        "colab_type": "text"
      },
      "source": [
        "#### Python Cell no.3\n",
        "--------------------------\n",
        "\n",
        "The function creates the neural network structure, and fits the model on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1PIO7_k6VtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the pre-trained saved model from the relevant link of Tensorflow HUB\n",
        "\n",
        "saved_model_url = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(saved_model_url, output_shape=[20], input_shape=[], dtype=tf.string, trainable=True)\n",
        "\n",
        "# Initialize the basic folder path names that outputs\n",
        "sequential_model_name=\"English_Google_News_130GB_20dim_without_OOV\"\n",
        "network_structure_file_name=\"network_structure_English_Google_News_130GB_without_OOV\"\n",
        "folder_path_model_saved=\"drive/My Drive/model_three/english_google_news_20dim_no_OOV\"\n",
        "saved_model_name=\"English_Google_News_130GB_20dim_without_OOV\"\n",
        "saved_metrics_dataframe_name=\"metrics_histogram_English_Google_News_130GB_20dim_without_OOV\"\n",
        "saved_df_scored_metric_name=\"df_metrics_english_google_news_130GB_20dim_without_oov\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UcnK5ZH6VtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_fit_tf_hub_model(hparams,\n",
        "                            version_data_control,\n",
        "                            optimizer_name,\n",
        "                            option_method,\n",
        "                            callbacks,\n",
        "                            optimizer_version = None):\n",
        "\n",
        "    model = tf.keras.Sequential(name=\"{0}_{1}batchsize_{2}lr_{3}decaymultiplier_{4}\".format(sequential_model_name,\n",
        "                                                                                            str(hparams[HP_HIDDEN_UNITS]),\n",
        "                                                                                            str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                                            str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                            version_data_control))\n",
        "    model.add(hub_layer)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(hparams[HP_HIDDEN_UNITS],\n",
        "                                    kernel_regularizer=regularizers.l2(neural_network_parameters['l2_regularization']),\n",
        "                                    activation=neural_network_parameters['dense_activation']))\n",
        "    model.add(tf.keras.layers.Dropout(neural_network_parameters['dropout_rate']))\n",
        "    model.add(tf.keras.layers.Dense(y_val.shape[1], activation=neural_network_parameters['output_activation']))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    #instantiate Optimizer\n",
        "    if optimizer_name==\"adam\" and optimizer_version is None:\n",
        "\n",
        "        optimizer = optimizer_adam_v2(hparams)\n",
        "\n",
        "    model.compile(optimizer=optimizer, #else put \"adam\"\n",
        "                  loss=neural_network_parameters['model_loss'],\n",
        "                  metrics=[neural_network_parameters['model_metric']])\n",
        "    \n",
        "    #plot model's structure\n",
        "    plot_model(model, show_shapes=True, dpi=70, to_file=os.path.join(os.getcwd(), '{0}/{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.png'.format(folder_path_model_saved,\n",
        "                                                                                                                                                  network_structure_file_name, \n",
        "                                                                                                                                                  str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                                                                  str(hparams[HP_LEARNING_RATE]), \n",
        "                                                                                                                                                  str(hparams[HP_DECAY_STEPS_MULTIPLIER]), \n",
        "                                                                                                                                                  version_data_control)))\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if option_method==\"option1\": # Use only this option. The preprocessing for options 2.1, 2.2 is deleted\n",
        "        fitted_model=model.fit([partial_x_train_plot_option1, \n",
        "                                partial_x_train_features_option1,\n",
        "                                partial_x_train_reviews_option1,\n",
        "                                partial_x_train_title_option1],\n",
        "                               partial_y_train,\n",
        "                               steps_per_epoch=int(np.ceil((X_train.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS])),\n",
        "                               epochs=fit_parameters[\"epoch\"],\n",
        "                               verbose=fit_parameters[\"verbose_fit\"],\n",
        "                               batch_size=hparams[HP_HIDDEN_UNITS],\n",
        "                               validation_split=fit_parameters['validation_data_ratio'],\n",
        "                               callbacks=callbacks)\n",
        "        evaluation = model.evaluate([test_bytes_list_plot_option1,\n",
        "                                    test_bytes_list_features_option1, \n",
        "                                    test_bytes_list_reviews_option1, \n",
        "                                    test_bytes_list_title_option1], test_label, batch_size=hparams[HP_HIDDEN_UNITS], verbose=2)\n",
        "\n",
        "    elif option_method==\"option2.1\":\n",
        "        fitted_model=model.fit(dataset.repeat().batch(hparams[HP_HIDDEN_UNITS], drop_remainder=True).prefetch(1),\n",
        "                               steps_per_epoch=int(np.ceil(len(partial_x_train_actors_option2)//hparams[HP_HIDDEN_UNITS])),\n",
        "                               epochs=fit_parameters[\"epoch\"],\n",
        "                               verbose=fit_parameters[\"verbose_fit\"],\n",
        "                               batch_size=hparams[HP_HIDDEN_UNITS],\n",
        "                               validation_data=dataset_validation,\n",
        "                               validation_steps=int(np.ceil(len(x_val_features_option2)//hparams[HP_HIDDEN_UNITS])),\n",
        "                               callbacks=callbacks)\n",
        "        evaluation = model.evaluate(dataset_test, batch_size=hparams[HP_HIDDEN_UNITS], verbose=2)\n",
        "\n",
        "    else:\n",
        "        fitted_model=model.fit([partial_x_train_actors_tensor,\n",
        "                                partial_x_train_plot_tensor,\n",
        "                                partial_x_train_features_tensor,\n",
        "                                partial_x_train_reviews_tensor,\n",
        "                                partial_x_train_title_tensor],\n",
        "                               partial_y_train,\n",
        "                               steps_per_epoch=int(np.ceil((X_train.shape[0]*optimizer_parameters['validation_split_ratio'])//hparams[HP_HIDDEN_UNITS])),\n",
        "                               epochs=fit_parameters[\"epoch\"],\n",
        "                               verbose=fit_parameters[\"verbose_fit\"],\n",
        "                               batch_size=hparams[HP_HIDDEN_UNITS],\n",
        "                               validation_data=([x_val_actors_tensor, x_val_plot_tensor, x_val_features_tensor, x_val_reviews_tensor, x_val_title_tensor],\n",
        "                                                y_val),\n",
        "                               callbacks=callbacks)\n",
        "        evaluation = model.evaluate([test_bytes_list_actors_tensor, \n",
        "                                    test_bytes_list_plot_tensor,\n",
        "                                    test_bytes_list_features_tensor, \n",
        "                                    test_bytes_list_reviews_tensor, \n",
        "                                    test_bytes_list_title_tensor], test_label, batch_size=hparams[HP_HIDDEN_UNITS], verbose=2)\n",
        "    #save the model\n",
        "    save_model(model,\n",
        "               folder_path_model_saved,\n",
        "               \"{0}_{1}batchsize_{2}lr_{3}decaymultiplier_{4}\".format(saved_model_name,\n",
        "                                                                      str(hparams[HP_HIDDEN_UNITS]),\n",
        "                                                                      str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                      str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                      version_data_control))\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    print(\"\\nTraining time of the multi-input keras model has finished. Duration {0} secs\".format(format_timespan(elapsed_time)))\n",
        "\n",
        "    loss = evaluation[0] # single number\n",
        "    model_metric = evaluation[1:] # is a list of 3 elements (hamming loss, f1_score_micro, f1_score_none)\n",
        "\n",
        "    hamming_loss_value=evaluation[1]\n",
        "    print('Hamming loss value on test data: {0}'.format(hamming_loss_value))\n",
        "\n",
        "    return hamming_loss_value, model, fitted_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdcdvqhi6VtN",
        "colab_type": "text"
      },
      "source": [
        "#### Python Cell no.4\n",
        "--------------------------\n",
        "\n",
        "*run* is a method that call the above function *create_fit_keras_model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzC1c8Zp6VtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(run_dir, hparams, version_data_control, optimizer_name, option_method, callbacks):\n",
        "    with tf.summary.create_file_writer(run_dir).as_default():\n",
        "        hp.hparams(hparams)\n",
        "        accuracy, model, fitted_model = create_fit_tf_hub_model(hparams, version_data_control, optimizer_name, option_method, callbacks)\n",
        "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
        "    \n",
        "    return model, fitted_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9JiCXL-6VtP",
        "colab_type": "text"
      },
      "source": [
        "Everything is set. In the next coding cells the training is executed! <br>\n",
        "<b>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiE_F1KqpPbD",
        "colab_type": "text"
      },
      "source": [
        "#### Python Cell no.5\n",
        "--------------------------\n",
        "\n",
        "Fit the model and monitor the performance of the classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niPuZsreIFXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists(os.path.join(os.getcwd(), \"{0}\".format(folder_path_model_saved))) is True:\n",
        "    print(\"Folder already exists!\\n\")\n",
        "else:\n",
        "    print(\"Folder not found!\\n\")\n",
        "    os.mkdir(os.path.join(os.getcwd(), \"{0}\".format(folder_path_model_saved)))\n",
        "    print(\"Folder is created!\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWnmrQ8gIQfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Model Training\n",
        "\"\"\"\n",
        "begin_time=time.time()\n",
        "print(\"{0}: Start execution of the cell\\n\".format(datetime.utcnow().strftime(date_format)))\n",
        "\n",
        "session_num = 1\n",
        "\n",
        "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
        "  for learning_rate in HP_LEARNING_RATE.domain.values:\n",
        "      for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
        "          hparams = {\n",
        "              HP_HIDDEN_UNITS: batch_size,\n",
        "              HP_LEARNING_RATE: learning_rate,\n",
        "              HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
        "            }\n",
        "          run_name = \"run-id {0}\".format(session_num)\n",
        "          total_number_models=(len(HP_HIDDEN_UNITS.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))\n",
        "          print('--- Starting trial: {0}/{1}\\n'.format(run_name, total_number_models))\n",
        "          print({h.name: hparams[h] for h in hparams}, '\\n')\n",
        "\n",
        "          starting_training=time.time()\n",
        "\n",
        "          model_struture, model_history=run('{0}/'.format(hp_logging_directory) + run_name,\n",
        "                                            hparams,\n",
        "                                            version_data_control,\n",
        "                                            \"adam\",\n",
        "                                            fit_parameters[\"option_method\"],\n",
        "                                            callback(\"adam-inverse decay\",\n",
        "                                                      folder_path_model_saved,\n",
        "                                                      \"{0}_{1}batchsize_{2}lr_{3}decaymultiplier_{4}\".format(saved_model_name,\n",
        "                                                                                                            str(batch_size),\n",
        "                                                                                                            str(learning_rate),\n",
        "                                                                                                            str(decay_steps_multiplier),\n",
        "                                                                                                            version_data_control),\n",
        "                                                      fit_parameters[\"patience_value\"],\n",
        "                                                      \"{0}/\".format(hp_logging_directory) + datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
        "                                                      hparams))\n",
        "          \n",
        "          print(\"Average time per epoch: {0}\\n\".format(format_timespan((time.time()-starting_training)/len(model_history.epoch))))\n",
        "          \n",
        "          hist = pd.DataFrame(model_history.history)\n",
        "          hist['epoch'] = model_history.epoch\n",
        "          hist['epoch']+= 1\n",
        "          hist.index += 1\n",
        "          print(\"Table of training the {0} text classification model\\n\".format(sequential_model_name))\n",
        "          print(tabulate(hist, headers='keys', tablefmt='psql'))\n",
        "\n",
        "          hist.to_pickle(os.path.join(os.getcwd(), folder_path_model_saved+\"/{0}_{1}batchsize_{2}lr_{3}decaymultiplier_{4}.pkl\".format(saved_metrics_dataframe_name,\n",
        "                                                                                                                                       str(batch_size), \n",
        "                                                                                                                                       str(learning_rate), \n",
        "                                                                                                                                       str(decay_steps_multiplier), \n",
        "                                                                                                                                       version_data_control)))\n",
        "\n",
        "          #plot the model's model_metric (Hamming Loss, F1-score) & loss\n",
        "          plot_keras_history(model_history.history, folder_path_model_saved, batch_size, learning_rate, decay_steps_multiplier, version_data_control)\n",
        "\n",
        "          #evaluate the model\n",
        "          if fit_parameters[\"option_method\"]==\"option1\":\n",
        "              model_evaluation = model_struture.evaluate([test_bytes_list_plot_option1,\n",
        "                                                          test_bytes_list_features_option1,\n",
        "                                                          test_bytes_list_reviews_option1,\n",
        "                                                          test_bytes_list_title_option1],\n",
        "                                                         test_label,\n",
        "                                                         batch_size=batch_size,\n",
        "                                                         verbose=2)\n",
        "              \n",
        "          elif fit_parameters[\"option_method\"]==\"option2.1\":\n",
        "              model_evaluation = model_struture.evaluate(dataset_test, batch_size=hparams[HP_HIDDEN_UNITS], verbose=2)\n",
        "          \n",
        "          else:\n",
        "              model_evaluation = model_struture.evaluate([test_bytes_list_actors_tensor, \n",
        "                                                          test_bytes_list_plot_tensor,\n",
        "                                                          test_bytes_list_features_tensor, \n",
        "                                                          test_bytes_list_reviews_tensor, \n",
        "                                                          test_bytes_list_title_tensor], \n",
        "                                                         test_label, \n",
        "                                                         batch_size=hparams[HP_HIDDEN_UNITS], \n",
        "                                                         verbose=2)\n",
        "\n",
        "          print(\"\\nTest Score (evalution of the model's loss/error on the test sequences): {0}\".format(model_evaluation[0]))\n",
        "          print(\"\\nTest model_metric (evalution of the hamming loss on the test sequences): {0}\\n\".format(model_evaluation[1]))\n",
        "\n",
        "          fig = plt.figure()\n",
        "          ax = fig.add_subplot(111)\n",
        "          \n",
        "          plt.rcParams[\"figure.figsize\"] = (16,13)\n",
        "          \n",
        "          ax.bar(genres_list, model_evaluation[3])\n",
        "          \n",
        "          ax.set_title('F1 score per genre tag')\n",
        "          ax.set_xlabel('Movie Genre')\n",
        "          ax.set_ylabel('F1 score')\n",
        "          ax.set_xticklabels(genres_list)\n",
        "          \n",
        "          ax.spines['left'].set_color('white')\n",
        "          ax.spines['bottom'].set_color('white')\n",
        "\n",
        "          ax.tick_params(axis='x', colors='white')\n",
        "          ax.tick_params(axis='y', colors='white')\n",
        "          \n",
        "          ax.yaxis.label.set_color('white')\n",
        "          ax.xaxis.label.set_color('white')\n",
        "          ax.title.set_color('white')\n",
        "\n",
        "          plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "          for i, f1 in enumerate(model_evaluation[3]):\n",
        "              ax.annotate(round(f1, 2), (i, f1), ha='center', va='bottom')\n",
        "          \n",
        "          plt.show()\n",
        "\n",
        "          session_num += 1\n",
        "\n",
        "total_time=time.time() - begin_time\n",
        "print(\"{0}: Total cell execution time: {1}\".format(datetime.utcnow().strftime(date_format), format_timespan(total_time)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLASjJBW6VtS",
        "colab_type": "text"
      },
      "source": [
        "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyJozhBB6VtT",
        "colab_type": "text"
      },
      "source": [
        "#### Python Cell no.6\n",
        "--------------------------\n",
        "\n",
        "Select the best model classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scco7S_wsqeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_model_name=\"English_Google_News_130GB_20dim_without_OOV\"\n",
        "folder_path_model_saved=\"model_three/english_google_news_20dim_no_OOV\"\n",
        "saved_df_scored_metric_name=\"df_metrics_English_Google_News_130GB_20dim_without_OOV\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWyx2amD6VtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function 1\n",
        "\n",
        "def import_trained_keras_model(method, decay_steps_mode, embedding_dim_mode, optimizer_name, hparams):\n",
        "    \"\"\"\n",
        "    Load the weights of the model saved with EarlyStopping\n",
        "    \"\"\"\n",
        "    if method == \"import custom trained model\":\n",
        "        \n",
        "        if decay_steps_mode==\"on\":\n",
        "            \n",
        "            if embedding_dim_mode==\"on\":\n",
        "            \n",
        "                with open(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.json\".format(folder_path_model_saved,\n",
        "                                                                                                                            saved_model_name,\n",
        "                                                                                                                            str(hparams[HP_EMBEDDING_DIM]), \n",
        "                                                                                                                            str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                                            str(hparams[HP_LEARNING_RATE]), \n",
        "                                                                                                                            str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                            version_data_control)),'r') as f:\n",
        "                    model_json = json.load(f)\n",
        "\n",
        "                model_imported = model_from_json(model_json)\n",
        "\n",
        "                model_imported.load_weights(os.path.join(os.getcwd(), '{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.h5'.format(folder_path_model_saved,\n",
        "                                                                                                                                            saved_model_name,\n",
        "                                                                                                                                            str(hparams[HP_EMBEDDING_DIM]),\n",
        "                                                                                                                                            str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                                                            str(hparams[HP_LEARNING_RATE]), \n",
        "                                                                                                                                            str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                                            version_data_control)))\n",
        "            else:\n",
        "                \n",
        "                with open(os.path.join(os.getcwd(), \"{0}/{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.json\".format(folder_path_model_saved,\n",
        "                                                                                                                     saved_model_name,\n",
        "                                                                                                                     str(hparams[HP_HIDDEN_UNITS]),\n",
        "                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                                                                     str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                     version_data_control)),'r') as f:\n",
        "                    model_json = json.load(f)\n",
        "\n",
        "                model_imported = model_from_json(model_json)\n",
        "\n",
        "                model_imported.load_weights(os.path.join(os.getcwd(), '{0}/{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.h5'.format(folder_path_model_saved,\n",
        "                                                                                                                                            saved_model_name,\n",
        "                                                                                                                                            str(hparams[HP_HIDDEN_UNITS]),\n",
        "                                                                                                                                            str(hparams[HP_LEARNING_RATE]), \n",
        "                                                                                                                                            str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                                            version_data_control)))\n",
        "        else:\n",
        "            \n",
        "            with open(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}.json\".format(folder_path_model_saved,\n",
        "                                                                                                     saved_model_name,\n",
        "                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n",
        "                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                                                     version_data_control)),'r') as f:\n",
        "                model_json = json.load(f)\n",
        "\n",
        "            model_imported = model_from_json(model_json)\n",
        "\n",
        "            model_imported.load_weights(os.path.join(os.getcwd(), '{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}.h5'.format(folder_path_model_saved,\n",
        "                                                                                                                     saved_model_name,\n",
        "                                                                                                                     str(hparams[HP_EMBEDDING_DIM]), \n",
        "                                                                                                                     str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                                                                     version_data_control))) \n",
        "        if optimizer_name==\"adam\":\n",
        "            optimizer = optimizer_adam_v2(hparams)\n",
        "        \n",
        "        elif optimizer_name==\"sgd\":\n",
        "            optimizer = optimizer_sgd_v1(hparams, \"step decay\")\n",
        "            \n",
        "        else:\n",
        "            optimizer = optimizer_rmsprop_v1(hparams)\n",
        "            \n",
        "        model_imported.compile(optimizer=optimizer,\n",
        "                               loss=neural_network_parameters['model_loss'],\n",
        "                               metrics=neural_network_parameters['model_metric'])\n",
        "        print(\"\\nModel is loaded successfully\\n\")\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        with open(os.path.join(os.getcwd(), \"{0}/{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.json\".format(folder_path_model_saved,\n",
        "                                                                                                             saved_model_name,\n",
        "                                                                                                             str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                             str(hparams[HP_LEARNING_RATE]), \n",
        "                                                                                                             str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                             version_data_control)),'r') as f:\n",
        "            model_json = json.load(f)\n",
        "\n",
        "        model_imported = model_from_json(model_json, custom_objects={'KerasLayer':hub.KerasLayer})\n",
        "\n",
        "        model_imported.load_weights(os.path.join(os.getcwd(), '{0}/{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.h5'.format(folder_path_model_saved,\n",
        "                                                                                                                             saved_model_name,\n",
        "                                                                                                                             str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                                             str(hparams[HP_LEARNING_RATE]), \n",
        "                                                                                                                             str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                             version_data_control)))\n",
        "\n",
        "        optimizer = optimizer_adam_v2(hparams)\n",
        "\n",
        "        model_imported.compile(optimizer=optimizer,\n",
        "                               loss=neural_network_parameters['model_loss'],\n",
        "                               metrics=neural_network_parameters['model_metric'])\n",
        "        print(\"\\nModel is loaded successfully\\n\")\n",
        "    \n",
        "    return model_imported\n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "\n",
        "# Function 2\n",
        "\n",
        "def create_df_scoring_table(method, decay_steps_mode, embedding_dim_mode, model_tag, hparams, model):\n",
        "    \"\"\"\n",
        "    Create a scoring dictionary to select the best out of the four models\n",
        "    \"\"\"\n",
        "    if method == \"import custom trained model\":\n",
        "        \n",
        "        model_evaluation = model.evaluate([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title],\n",
        "                                          y_test,\n",
        "                                          batch_size=hparams[HP_HIDDEN_UNITS],\n",
        "                                          verbose=2)\n",
        "\n",
        "        y_test_pred_probs = model.predict([X_test_seq_actors, X_test_seq_plot, X_test_seq_features, X_test_seq_reviews, X_test_seq_title])\n",
        "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
        "\n",
        "        variance = np.var(y_test_predictions)\n",
        "        sse = np.mean((np.mean(y_test_predictions) - y_test)**2)\n",
        "        bias = sse - variance\n",
        "\n",
        "        hamming_loss_value = HammingLoss(mode='multilabel')\n",
        "        hamming_loss_value.update_state(y_test, y_test_predictions)\n",
        "        \n",
        "        if decay_steps_mode==\"on\":\n",
        "            \n",
        "            if embedding_dim_mode==\"on\":\n",
        "            \n",
        "                df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
        "                                        'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n",
        "                                        'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
        "                                        'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
        "                                        'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
        "                                        'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
        "                                        'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
        "                                        'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
        "                                        'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
        "                                        'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
        "                                        'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
        "                                        'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                        'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                        'Bias':pd.Series([bias], dtype='float'),\n",
        "                                        'Variance':pd.Series([variance], dtype='float')\n",
        "                                       })\n",
        "\n",
        "                df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}decaymultiplier_{6}.pkl\".format(folder_path_model_saved,\n",
        "                                                                                                                                     saved_df_scored_metric_name,\n",
        "                                                                                                                                     str(hparams[HP_EMBEDDING_DIM]),\n",
        "                                                                                                                                     str(hparams[HP_HIDDEN_UNITS]),\n",
        "                                                                                                                                     str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                                                                                     str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                                     version_data_control)))\n",
        "            else:\n",
        "                \n",
        "                df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
        "                                        'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
        "                                        'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
        "                                        'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
        "                                        'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
        "                                        'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
        "                                        'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
        "                                        'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
        "                                        'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
        "                                        'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
        "                                        'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                        'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                        'Bias':pd.Series([bias], dtype='float'),\n",
        "                                        'Variance':pd.Series([variance], dtype='float')\n",
        "                                       })\n",
        "\n",
        "                df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}/{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
        "                                                                                                                              saved_df_scored_metric_name,\n",
        "                                                                                                                              str(hparams[HP_HIDDEN_UNITS]),\n",
        "                                                                                                                              str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                                                                              str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                              version_data_control)))    \n",
        "        else:\n",
        "            \n",
        "            df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
        "                                    'Embedding Dimension tag':pd.Series(hparams[HP_EMBEDDING_DIM], dtype='int'),\n",
        "                                    'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
        "                                    'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
        "                                    'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
        "                                    'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
        "                                    'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
        "                                    'Zero_one Loss':pd.Series([zero_one_loss(y_test, y_test_predictions, normalize=False)], dtype='float'),\n",
        "                                    'F1_score':pd.Series([f1_score(y_test, y_test_predictions, average=\"micro\")], dtype='float'),\n",
        "                                    'F1_score_samples':pd.Series([f1_score(y_test, y_test_predictions, average=\"samples\")], dtype='float'),\n",
        "                                    'ROC_score':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                    'ROC_score_samples':pd.Series([roc_auc_score(y_test, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                    'Bias':pd.Series([bias], dtype='float'),\n",
        "                                    'Variance':pd.Series([variance], dtype='float')\n",
        "                                   })\n",
        "\n",
        "            df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}/{1}_{2}dim_{3}batchsize_{4}lr_{5}.pkl\".format(folder_path_model_saved,\n",
        "                                                                                                              saved_df_scored_metric_name,\n",
        "                                                                                                              str(hparams[HP_EMBEDDING_DIM]),\n",
        "                                                                                                              str(hparams[HP_HIDDEN_UNITS]),\n",
        "                                                                                                              str(hparams[HP_LEARNING_RATE]),\n",
        "                                                                                                              version_data_control)))\n",
        "    else:\n",
        "        \n",
        "        model_evaluation = model.evaluate([test_bytes_list_plot_option1, test_bytes_list_features_option1, test_bytes_list_reviews_option1, test_bytes_list_title_option1],\n",
        "                                          test_label,\n",
        "                                          batch_size=hparams[HP_HIDDEN_UNITS],\n",
        "                                          verbose=2)\n",
        "\n",
        "        y_test_pred_probs = model.predict([test_bytes_list_plot_option1, test_bytes_list_features_option1, test_bytes_list_reviews_option1, test_bytes_list_title_option1])\n",
        "        y_test_predictions = (y_test_pred_probs>0.5).astype(int)\n",
        "\n",
        "        variance = np.var(y_test_predictions)\n",
        "        sse = np.mean((np.mean(y_test_predictions) - test_label)**2)\n",
        "        bias = sse - variance\n",
        "\n",
        "        hamming_loss_value = HammingLoss(mode='multilabel')\n",
        "        hamming_loss_value.update_state(test_label, y_test_predictions)\n",
        "\n",
        "        df_scores=pd.DataFrame({'Tag Name':pd.Series(model_tag, dtype='str'),\n",
        "                                'Batch tag':pd.Series(hparams[HP_HIDDEN_UNITS], dtype='int'),\n",
        "                                'Learning Rate tag':pd.Series(hparams[HP_LEARNING_RATE], dtype='float'),\n",
        "                                'Decay Multiplier tag':pd.Series(hparams[HP_DECAY_STEPS_MULTIPLIER], dtype='int'),\n",
        "                                'Test Loss':pd.Series([model_evaluation[0]], dtype='float'),\n",
        "                                'Test Hamming Loss':pd.Series([model_evaluation[1]], dtype='float'),\n",
        "                                'Hamming Loss':pd.Series([hamming_loss_value.result().numpy()], dtype='float'),\n",
        "                                'Zero_one Loss':pd.Series([zero_one_loss(test_label, y_test_predictions, normalize=False)], dtype='float'),\n",
        "                                'F1_score':pd.Series([f1_score(test_label, y_test_predictions, average=\"micro\")], dtype='float'),\n",
        "                                'F1_score_samples':pd.Series([f1_score(test_label, y_test_predictions, average=\"samples\")], dtype='float'),\n",
        "                                'ROC_score':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"micro\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                'ROC_score_samples':pd.Series([roc_auc_score(test_label, y_test_predictions, average=\"samples\", multi_class=\"ovr\")], dtype='float'),\n",
        "                                'Bias':pd.Series([bias], dtype='float'),\n",
        "                                'Variance':pd.Series([variance], dtype='float')\n",
        "                               })\n",
        "\n",
        "        df_scores.to_pickle(os.path.join(os.getcwd(), \"{0}/{1}_{2}batchsize_{3}lr_{4}decaymultiplier_{5}.pkl\".format(folder_path_model_saved,\n",
        "                                                                                                                      saved_df_scored_metric_name,\n",
        "                                                                                                                      str(hparams[HP_HIDDEN_UNITS]), \n",
        "                                                                                                                      str(hparams[HP_LEARNING_RATE]), \n",
        "                                                                                                                      str(hparams[HP_DECAY_STEPS_MULTIPLIER]),\n",
        "                                                                                                                      version_data_control)))\n",
        "    return df_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKp1J7GytulY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_method_creation=\"adam\"\n",
        "\n",
        "list_models=[]\n",
        "list_df=[]\n",
        "\n",
        "HP_HIDDEN_UNITS = hp.HParam('batch_size', hp.Discrete([32, 64, 128]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01, 0.1]))\n",
        "HP_DECAY_STEPS_MULTIPLIER = hp.HParam('decay_steps_multiplier', hp.Discrete([10, 20]))\n",
        "\n",
        "for batch_size in HP_HIDDEN_UNITS.domain.values:\n",
        "    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
        "        for decay_steps_multiplier in HP_DECAY_STEPS_MULTIPLIER.domain.values:\n",
        "            hparams = {\n",
        "                HP_HIDDEN_UNITS: batch_size,\n",
        "                HP_LEARNING_RATE: learning_rate,\n",
        "                HP_DECAY_STEPS_MULTIPLIER: decay_steps_multiplier\n",
        "              }\n",
        "            print(\"{0}/{1}\".format(len(list_models)+1, (len(HP_HIDDEN_UNITS.domain.values)*len(HP_LEARNING_RATE.domain.values)*len(HP_DECAY_STEPS_MULTIPLIER.domain.values))))\n",
        "            model_object=import_trained_keras_model(\"import pre-trained hub model\", \"on\", \"off\", model_method_creation, hparams)\n",
        "            df_object=create_df_scoring_table(\"import pre-trained hub model\", \"on\", \"off\", \"{0}-{1}\".format(saved_model_name, len(list_models)+1), hparams, model_object)\n",
        "            list_models.append(model_object)\n",
        "            list_df.append(df_object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxgkLEurt4mB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_one, model_two, model_three, model_four, model_five, model_six, model_seven, model_eight, model_nine, model_ten, model_eleven, model_twelve, model_thirteen, model_fourteen, model_fifteen, model_sixteen, model_seventeen, model_eighteen=list_models\n",
        "\n",
        "df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve, df_scores_thirteen, df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen=list_df\n",
        "\n",
        "frames_20dim_noOOV=[df_scores_one, df_scores_two, df_scores_three, df_scores_four, df_scores_five, df_scores_six, df_scores_seven, df_scores_eight, df_scores_nine, df_scores_ten, df_scores_eleven, df_scores_twelve, df_scores_thirteen, \n",
        "                    df_scores_fourteen, df_scores_fifteen, df_scores_sixteen, df_scores_seventeen, df_scores_eighteen]\n",
        "\n",
        "result_20dim_noOOV=pd.concat(frames_20dim_noOOV)\n",
        "result_20dim_noOOV=result_20dim_noOOV.reset_index(drop=True)\n",
        "result_20dim_noOOV.sort_values(by=['Hamming Loss', 'Zero_one Loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpuTmwzjt_js",
        "colab_type": "text"
      },
      "source": [
        "Best model of the total 18 models presented above is model 3 with:\n",
        "* Batch size: 32\n",
        "* Learning rate: 0.01\n",
        "* Decay Steps Multiplier: 10\n",
        "* Hamming loss & Zeron-one loss: 0.092434 - 7761.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEQoKaPyt4hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(result_20dim_noOOV.to_latex(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDbOflO6VtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the structure of the model selected\n",
        "def visualize_model(model):\n",
        "    return SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
        "\n",
        "visualize_model(model_three)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}