{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.1 - Data Tokenization & Sequence padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the version data control parameter (to save the outputs of this notebook at their latest date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_data_control=\"22022021\"\n",
    "datasets_path=\"C://Users//spano//Desktop//nlp_github//datasets\" #change this path based on your local folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from humanfriendly import format_timespan\n",
    "import random\n",
    "\n",
    "# Module to serialize the content produced from the execution of the code\n",
    "import joblib\n",
    "\n",
    "# Module to monitor the progress of a python for loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Module to manipulate text in python - NLTK package\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "\n",
    "# Module to compute word vectorizers and compute the cosine distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#Import matplotlib for data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from packaging import version\n",
    "\n",
    "# from IPython.core.display import display,HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Tokenization and Plotting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.4.1\n",
      "Version:  2.4.1\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import the dataset corrected and enriched from part 2.1. This dataset will be tokenized and transformed in order to meet Tensorflow guidelines for NLP applications & research.\"\"\"\n",
    "dataset=joblib.load(f\"{datasets_path}//dataset_part_3.1_22022021_light.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48834, 36)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-hot encoding is a good practice to transform the value y into a data structure appropriate for multi-label text calssification.\n",
    "Basically it creates a single column per genre with 0,1 binary values if the movie has the specific genre tag or not.\n",
    "\"\"\"\n",
    "mlb = MultiLabelBinarizer()\n",
    "dataset_nlp_tokenization=dataset.join(pd.DataFrame(mlb.fit_transform(dataset[\"reduced_genres\"]),\n",
    "                                                                     columns=mlb.classes_,\n",
    "                                                                     index=dataset.index))\n",
    "dataset_nlp_tokenization.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['genres_list_22022021.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create the list of genres and serialize it. The genres of this list will be the dependent variable of the model predictor. The value the model classifier will try to predict.\"\"\"\n",
    "genres_list=dataset_nlp_tokenization[\"reduced_genres\"].explode().value_counts(normalize=True).index.to_list()\n",
    "genres_list.sort()\n",
    "print(genres_list)\n",
    "joblib.dump(genres_list,f\"general_data_samples//genres_list_{version_data_control}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 48834/48834 [00:00<00:00, 428153.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_single_letter_actors(cast_list):\n",
    "    \"\"\"\n",
    "    Purpose: We observed that a portion of actors had names consisting of one, two, or three words. We decided to remove such actors from the cast of each movie. \n",
    "    This action improved a lot the result of the data cleaning and tokenization.\n",
    "    Arguments: Actors list per movie (row) of the dataset.\n",
    "    Output: The actors list with short actor names deleted.\n",
    "    \"\"\"\n",
    "    cleaned_actors=[actor for actor in cast_list if len(actor)>=4]\n",
    "    return cleaned_actors\n",
    "tqdm.pandas()\n",
    "dataset_nlp_tokenization[\"actors_cleaned\"]=dataset_nlp_tokenization[\"actors\"].progress_apply(lambda x: clean_single_letter_actors(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>actors_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, actors_cleaned]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Check the movies with actor names equal to length N, where N=any positive number. For example below, we check the movies with at leat one actor with length 4\"\"\"\n",
    "mask=dataset_nlp_tokenization[\"actors_cleaned\"].explode().str.len().eq(3)\n",
    "res=dataset_nlp_tokenization[['title', 'actors_cleaned']].loc[np.unique(mask.loc[mask].index)]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 48834/48834 [00:00<00:00, 82905.52it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create a function that will unify the actors into a single corpus text\"\"\"\n",
    "def unify_actors(row):\n",
    "    \"\"\"\n",
    "    Purpuse: Join the actor names into a single line\n",
    "    Arguments: The row of the dataset\n",
    "    Output: The list of actors per movie into a single line of text separated by commas. It's important to remember the comma separator because it will be used later in actors tokenization.\n",
    "    \"\"\"\n",
    "    return \",\".join(row[\"actors_cleaned\"]).strip()\n",
    "tqdm.pandas()\n",
    "dataset_nlp_tokenization[\"actors_unified\"]=dataset_nlp_tokenization.progress_apply(unify_actors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions used across the whole notebook.\n",
    "Those functions are explisetely used to pre-process the raw data input of texts\n",
    "\"\"\"\n",
    "from text2digits import text2digits\n",
    "# -------------------------------------------------------------------------------------\n",
    "# Function 1\n",
    "nlp=spacy.load('en_core_web_md')\n",
    "def decontracted(phrase):\n",
    "    \"\"\"\n",
    "    Purpose: Expand contractions and abbreviations in the text. Also replace the text format of a number(i.e million) with its equivallent numeric format (i.e 1000000)\n",
    "    Argument: The text related to movie content such as Plot summary, Reviews\n",
    "    Output: The corpus text cleaned off abbreviations and contractions\n",
    "    \"\"\"\n",
    "    # General contractions\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r'(?<=[.,\"])(?=[^\\s])', \"\", phrase)\n",
    "    phrase = phrase.replace(\"L. A.\", \"Los Angeles\")\\\n",
    "                   .replace(\"U. S.\", \"United States\")\\\n",
    "                   .replace(\"U. K.\", \"United Kindom\")\\\n",
    "                   .replace(\"a.k.a\", \"\")\\\n",
    "                   .replace(\"a. k. a\", \"\")\\\n",
    "                   .replace(\"sci-fi\", \"science fiction\")\\\n",
    "                   .replace(\"U-boat\", \"submarine\")\\\n",
    "                   .replace(\"N-bomb\", \"nuclear bomb\")\\\n",
    "                   .replace(\"S&amp;M\", \"\")\\\n",
    "                   .replace(\"XIX-th\", \"19\")\\\n",
    "                   .replace(\"Twice-orphaned\", \"two times orphaned\")\\\n",
    "                   .replace(\"(wΔz = Cov (w,z) = βwzVz)\", \"\")\\\n",
    "                   .replace(\"(wDz = Cov (w,z) = bwzVz)\", \"\")\n",
    "    return phrase\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 2\n",
    "def correct_abbreviations(phrase):\n",
    "    # Word to numbers and correct abbreviations\n",
    "    phrase=phrase.replace(\"multimillion\", \"multi million\")\\\n",
    "                 .replace(\"multibillion\", \"multi billion\")\\\n",
    "                 .replace(\"multi-million\", \"multi million\")\\\n",
    "                 .replace(\"multi-billion\", \"multi billion\")\\\n",
    "                 .replace(\"trillion\", \"1000000000000\")\\\n",
    "                 .replace(\"billion\", \"1000000000\")\\\n",
    "                 .replace(\"crore\", \"10000000\")\\\n",
    "                 .replace(\"mln\", \"1000000\")\\\n",
    "                 .replace(\"bln\", \"1000000000\")\\\n",
    "                 .replace(\"III\", \"3\")\\\n",
    "                 .replace(\"II\", \"2\")\\\n",
    "                 .replace(\"iii\", \"3\")\\\n",
    "                 .replace(\"world war ii\", \"world war 2\")\\\n",
    "                 .replace(\"world war i\", \"world war 1\")\\\n",
    "                 .replace(\"HEADER\", \"The movie\")\\\n",
    "                 .replace(\"(die fetten jahre sind vorbei)\", \"\")\\\n",
    "                 .replace(\"named V\", \"\")\\\n",
    "                 .replace(\"thiry\", \"Thiry\")\\\n",
    "                 .replace(\"Kirsten deLohr Helland\", \"Kirsten Helland\")\\\n",
    "                 .replace(\"circa\", \"around\")\\\n",
    "                 .replace(\"xXx\", \"\")\\\n",
    "                 .replace(\"XXX\", \"\")\\\n",
    "                 .replace('tomboy \"M\"', \"tomboy\")\\\n",
    "                 .replace(\"slew\", \"slayed\")\\\n",
    "                 .replace(\"Crore\",\"ten million\")\\\n",
    "                 .replace(\"crore\",\"ten million\")\\\n",
    "                 .replace(\"twentyfive\",\"25\")\\\n",
    "                 .replace(\"FLicKeR\", \"the movie\")\\\n",
    "                 .replace(\"IMAX\", \"Space Station 3D\")\n",
    "    return phrase\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 3\n",
    "def preprocess_movie_content(raw_text, process_column, debug_mode=None): #Movie Content aka column name: movie_features\n",
    "    \"\"\"\n",
    "    Purpose: Clean the columns of movie content (plot summary, movie features and reviews) from any textual noise that could spoil the prediction classifier and result. Also apply lemmatization instead of stemming on each word.\n",
    "             This step is from the most important steps of NLP applications because it gives a unique form and meaning in the words of the dataset. Upon those cleaned words the NLP algorithm will learn the schemantics\n",
    "             that categorize a movie to a specific genre tag. Eight(8) steps are applied below. Each step is based on the researcher's intuition and are subjective to the experiment dataset. \n",
    "             Thus, it is advised to take parts from here with caution as they may not be applicable to other datasets.\n",
    "    Arguments: The plot summary or review per movie and an indicator(process_column) whether or not the text is plot or review.\n",
    "    Output: The cleaned plot summary, movie features and reviews.\n",
    "    \"\"\"\n",
    "    # 1.Remove punctuation\n",
    "    raw_text_decontracted=re.sub(\" +\", \" \",unidecode.unidecode(decontracted(raw_text)).translate(str.maketrans(string.punctuation+\"–\", \" \"*len(string.punctuation+\"–\"))))\n",
    "    if debug_mode==True:\n",
    "        print(\"1\",raw_text_decontracted)\n",
    "    \n",
    "    # 2.Expand Contractions and abbreviations\n",
    "    raw_text_no_abbreviations=correct_abbreviations(re.sub(\" +\", \" \",raw_text_decontracted))\n",
    "    if debug_mode==True:\n",
    "        print(\"\\n2\",raw_text_no_abbreviations)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 3.Remove numbers\n",
    "    # 3.1 Strip Dates\n",
    "    stripped_date_string=re.sub(r'\\w*\\d\\w*', \"\", raw_text_no_abbreviations).strip()\n",
    "    \n",
    "    # 3.2 From text to numeric form. Then delete the text in numeric form.\n",
    "    non_numeric_instances=[\"N\",\"m\",\"V\",\"IMAX\",\"IndieFEST\",\"TWICE\",\"InAPPropriate\",\"Twice\",\"zucchinis\"]\n",
    "    doc=nlp(stripped_date_string)\n",
    "    if debug_mode==True:\n",
    "        print([(w.text, w.pos_) for w in doc if w.pos_==\"NUM\"]) #only for rnd when an error is generated\n",
    "    if process_column==\"process_plot\":\n",
    "        tokens=[w2n.word_to_num(token.text) if token.pos_=='NUM' and token.text not in non_numeric_instances else token for token in doc]\n",
    "        stripped=[i.text if not str(i).isnumeric() else str(i) for i in tokens]\n",
    "        tokens_white_space_stripped_again=[w.strip() for w in stripped]\n",
    "    else:\n",
    "        t2d=text2digits.Text2Digits()\n",
    "        tokens=t2d.convert(stripped_date_string)\n",
    "        tokens=tokens.split(' ')\n",
    "        tokens_white_space_stripped_again=[w.strip() for w in tokens]\n",
    "    stripped_no_numbers=[i for i in tokens_white_space_stripped_again if not i.isnumeric()]\n",
    "    stripped_no_numbers=list(filter(None, stripped_no_numbers))\n",
    "    if debug_mode==True:\n",
    "        print(\"\\n3\",stripped_no_numbers)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 4.Remove stop words\n",
    "    stop_words=text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "    no_stopword_text=[word for word in stripped_no_numbers if not word.lower() in stop_words]\n",
    "    no_stopword_text=' '.join(no_stopword_text) #join the text once more because a new lemmatizing approach is implemented below\n",
    "    if debug_mode==True:\n",
    "        print(\"\\n4\",no_stopword_text)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 5.Lemmatization text in its lowercase() format\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmatized_text=list(set([lemmatizer.lemmatize(i.lower(),j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i.lower()) for i,j in pos_tag(word_tokenize(no_stopword_text))]))\n",
    "    if debug_mode==True:\n",
    "        print(\"\\n5\",lemmatized_text)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # 6.Join the words together to create the final text\n",
    "    cleaned_text=' '.join(lemmatized_text)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    return cleaned_text\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 4\n",
    "def transform_columns(column_name_to_clean, column_name_cleaned, dataset, process_column=None, debug_mode=None):\n",
    "    \"\"\"\n",
    "    Purpose: Apply the previous function 'preprocess_movie_content()' on the columns with movie content. Also lower-case every actor and movie tilte in the dataset.\n",
    "    Arguments: column_name_to_clean: Column in the dataset we desire to clean.\n",
    "               column_name_cleaned: New column name with the cleaned rows of the column cleaned.\n",
    "               dataset: Dataset to apply the cleaning.\n",
    "               process_column: In which column to apply the function preprocess_movie_content().\n",
    "               debug_mode: Whethere or not to print some specific statements to find erronious code lines if an exception is raised.\n",
    "    Output: The cleaned column specified in the arguments. Every row of the five input columns with movie content is cleaned using this function. The five movie content columns are: Title, Actors, Plot, Features, Reviews.\n",
    "    \"\"\"\n",
    "    tqdm.pandas()\n",
    "    if column_name_to_clean==\"actors_unified\":\n",
    "        dataset.loc[:, column_name_cleaned]=dataset.loc[:, column_name_to_clean].progress_apply(lambda x: x.lower())\n",
    "    elif column_name_to_clean==\"title\":\n",
    "        dataset.loc[:, column_name_cleaned]=dataset.loc[:, column_name_to_clean].progress_apply(lambda x: re.sub(\" +\", \" \", x.translate(str.maketrans(string.punctuation+\"–\", \" \"*len(string.punctuation+\"–\"))).lower().strip()))\n",
    "    else:\n",
    "        dataset.loc[:, column_name_cleaned]=dataset.loc[:, column_name_to_clean].progress_apply(lambda x: preprocess_movie_content(x, process_column, debug_mode))\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 5\n",
    "def split_dataset(method, labels, dataset, split_ratio):\n",
    "    \"\"\"\n",
    "    Purpose: Stratify split the dataset because of the imbalance of multi-label genre movies. We demonstrated in part 2 that genres were not equally distributed among the movies of the dataset. \n",
    "             This means that some genres are more frequent than others. In order to preserve the same frequency among the training, validation and test samples, it's imperative to apply stratified split.\n",
    "             In case the imbalance in the dataset is fixed in later versions of the notebook, then a simple ramdom split will be applied. However, uptil now the dependent variable(y) has significant imbalance\n",
    "             and thus stratification is applied.\n",
    "    Arguments: method: Stratified or random split\n",
    "               labels: Genre tags which represent the dependent variable y. Recall that the genre tags were extracted and serialized earlier in the notebook.\n",
    "               dataset: Movies dataset with the dependent and independent variables.\n",
    "               split_ratio: Percentage of rows for the training sample and 1-%training sample for the test sample.\n",
    "    Output: Stratified or random splitted training and test samples.           \n",
    "    \"\"\"\n",
    "    X=dataset[[\"title\",\"clean_actors\",\"clean_plot_summary\",\"clean_combined_features\",\"clean_reviews\",\"clean_movie_title\",\"reduced_genres\"]]\n",
    "    y=labels\n",
    "    if method==\"stratified\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio, random_state=123, shuffle=True, stratify=y)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_ratio, random_state=123, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 6\n",
    "def keras_tokenization(variable, maximum_words, dataset, column_name_tokenized, x_train, x_test, x_validation, column_sequence):\n",
    "    \"\"\"\n",
    "    Purpose: The keras tokenization method that will transform a sentence of text to a sequence of tokens, mapping each token to an index. An Out-of-Vocabulary token is also created, to map words not having an integer index.\n",
    "             Data tokenization is a fundamental step of NLP applications, because mathematical algorithms cannot understand text content but only numbers. Thus, every text in the dataset should be represented by a single token.\n",
    "    Arguments: variable: Dependent column to tokenize. At the moment five are the dependent predictors (actors, plot, featurs, reviews, title)\n",
    "               maximum_words: Words to tokenize per column sequence.\n",
    "               dataset: Movies dataset with movie content.\n",
    "               column_name_tokenized: Column of which the rows will be tokenized.The columns the function is applied are the five content inputs.\n",
    "               x_train: Training sample\n",
    "               x_test: Test sample\n",
    "               column_sequence: New column name in x_train, x_test with the tokenized rows per input.\n",
    "    Outputs: vocabulary_sized_frequent_words: Number of words tokenized out of the total corpus of a column. In this experiment we tokized the 95% of the total words per column.\n",
    "             tokenizer: Tensorflow tokenizer that tokenized an input column. Note that as input we refer to the columns used in the Input layer of an NLP classifier (aka dependent variables).\n",
    "    \"\"\"\n",
    "    #The tokenizer class has some main buggs when the word index mapping is created. So the function is assembled based on this GitHub post https://github.com/keras-team/keras/issues/8092#issuecomment-372833486\n",
    "    if variable==\"actors\":\n",
    "        tokenizer=Tokenizer(num_words=maximum_words,  filters=\",\", lower=True, split=\",\", oov_token='<OOV>')\n",
    "    else:\n",
    "        tokenizer=Tokenizer(num_words=maximum_words, filters=\" \", lower=True, split=\" \", oov_token='<OOV>')\n",
    "\n",
    "    tokenizer.fit_on_texts(list(dataset.loc[:, column_name_tokenized]))\n",
    "    unique_tokens=len(tokenizer.word_index)\n",
    "    print(f\"Maximum length of unique tokens is: {unique_tokens}\")\n",
    "\n",
    "    words_to_tokenize=int(round(unique_tokens*0.95,0)) #-1 because OOV token is include. While in the Count Vectorizer the OOV token is not included in the 95% so we should substract one index from the total length\n",
    "    print(f\"Number of words to be tokenized is the 95% of those unique tokens, equal to: {words_to_tokenize}\\nThe rest 5% or {unique_tokens-words_to_tokenize} is not tokenized.\")\n",
    "    \n",
    "    if maximum_words==words_to_tokenize:\n",
    "        print(\"\\nKeras Tokenizer result is equal to Count Vectorizer result!\")\n",
    "    else:\n",
    "        print(\"\\nKeras Tokenizer result is not equal to Count Vectorizer result!\")\n",
    "    tokenizer.word_index={e:i for e,i in tokenizer.word_index.items() if i <= words_to_tokenize}\n",
    "    tokenizer.word_index={x.strip(): v for x, v in tokenizer.word_index.items()}\n",
    "    tokenizer.word_index[tokenizer.oov_token]=words_to_tokenize+1 #this assignes the last index to the OOV token\n",
    "    print(f\"Number of words mapped: {words_to_tokenize-1}. The extra 1 index represents the OVV token, which is not included in the CountVectorizer.\")\n",
    "\n",
    "    x_train.loc[:, column_sequence]=tokenizer.texts_to_sequences(x_train.loc[:, column_name_tokenized])\n",
    "    x_test.loc[:, column_sequence]=tokenizer.texts_to_sequences(x_test.loc[:, column_name_tokenized])\n",
    "    x_validation.loc[:, column_sequence]=tokenizer.texts_to_sequences(x_validation.loc[:, column_name_tokenized])\n",
    "\n",
    "    vocabulary_size_frequent_words=len(tokenizer.word_index)\n",
    "\n",
    "    try:\n",
    "        assert words_to_tokenize==maximum_words #+1 because we added the OOV token in the last index\n",
    "    except AssertionError:\n",
    "        print(f\"ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather is equal to: {len(tokenizer.word_index)}\\nCorrect length: {maximum_words}.\")\n",
    "\n",
    "    return vocabulary_size_frequent_words, tokenizer\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 7\n",
    "def mean(numbers):\n",
    "    \"\"\"\n",
    "    Purpose: Get the maximum length per sequence. Note here that every sequence of indexes that represents a plot summary, a review, a title, a cast or movie features should be of equal length with their peers.\n",
    "             For example, we know that plot summaries have different lengths. Some they have 25 words, others 10 or less. This is not acceptable by tensorflow algorithms.\n",
    "             So every sequence of plot summary of all the movies should have the same sequence length.\n",
    "    \"\"\"\n",
    "    return int(np.ceil(float(sum(numbers)) / max(len(numbers), 1)))\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 8\n",
    "def padding_sequence_length(column_sequences, percentile, x_train, x_test, x_validation):\n",
    "    \"\"\"\n",
    "    Purpose: Find the maximum length of the sequences belonging to a column. The maximum length of the column's sequence is equal to the 95% length of all the sequences. So for example if the dataset has 10,000 sequences and 9,500\n",
    "    of them have length 20 then all the sequences will either be cropped or extended to 20 integers. Then use the maximum_length per column upon every sequence of that column. The result is a batch of sequences with same length.\n",
    "    Arguments: variable: Dependent variable to apply the function. Recall that since five(5) are the inputs of movie content, five are also the dependent variables.\n",
    "               x_train: Training sample\n",
    "               x_test: Test sample\n",
    "               Each sample has five columns. One column per predictor.\n",
    "    Output: The maximum length per input column.\n",
    "    \"\"\"\n",
    "    all_train_lengths=list(x_train[column_sequences].apply(len))\n",
    "    all_test_lengths=list(x_test[column_sequences].apply(len))\n",
    "    all_validation_lenghts=list(x_validation[column_sequences].apply(len))\n",
    "    \n",
    "    maxlen_train=int(np.percentile(all_train_lengths, q=percentile)) #all samples (train, test, validation) should have the same length in sequences.\n",
    "    maxlen_test=int(np.percentile(all_test_lengths, q=percentile))\n",
    "    maxlen_validation=int(np.percentile(all_validation_lenghts, q=percentile))\n",
    "    \n",
    "    if maxlen_train!=maxlen_test!=maxlen_validation:\n",
    "        maxlen_value=mean([maxlen_train, maxlen_test, maxlen_validation]) #maxlen per sequence will be euqal to the mean value between two maxlen value per sample (train, test).\n",
    "    else:\n",
    "        maxlen_value=maxlen_train\n",
    "\n",
    "    print(f\"Max Length of the pad sequence for {column_sequences}: {maxlen_value}\")\n",
    "\n",
    "    return maxlen_value\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Function 9\n",
    "# The input data for a deep learning model must be a single tensor (of shape e.g. (batch_size, 6, vocab_size), samples that are shorter than the longest item need to be padded with some placeholder value.\n",
    "# For more check here: https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "def padding_sequence(column_sequence, x_train, x_test, x_validation, y_train, y_test, y_validation, maxlen):\n",
    "    \"\"\"\n",
    "    Purpose: Apply padding based on the maximum length per input column.\n",
    "    Arguments: column_sequence: Name of the column to apply the padding.\n",
    "               x_train: Training sample with the dependent variables (actors, plot, features, reviews, title).\n",
    "               x_test: Test sample with the dependent variables.\n",
    "               y_train: Training sample with the independent variable (genre tags).\n",
    "               y_test: Test sample with the independent variable (genre tags).\n",
    "               maxlen: Maximum length to be applied in all the sequences of the column. The maximum length is the output of the previous function.\n",
    "    Output: Training and test sequence per dependent predictor.\n",
    "    \"\"\"\n",
    "    x_train_seq=pad_sequences(x_train.loc[:, column_sequence], padding='post', maxlen=maxlen)\n",
    "    x_test_seq=pad_sequences(x_test.loc[:, column_sequence], padding='post', maxlen=maxlen)\n",
    "    x_validation_seq=pad_sequences(x_validation.loc[:, column_sequence], padding='post', maxlen=maxlen)\n",
    "    \n",
    "    assert len(x_train_seq)==len(y_train)\n",
    "    assert len(x_test_seq)==len(y_test)\n",
    "    assert len(x_validation_seq)==len(y_validation)\n",
    "    \n",
    "    return x_train_seq, x_test_seq, x_validation_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies that are assigned to only 1 sequence of genres: 131\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Previously we experinced an error using the stratified sampling. Below we printed the number of genre sequences that are assigned to only one movie.\n",
    "For those 131 movies the stratified sampling is failing to complete. Because those 131 movies have a unique sequence of genre assigned.\n",
    "Thus, we should find their indexes and remove them. The final dataset should contain 48834-131=48703\n",
    "\"\"\"\n",
    "list_of_movies_to_remove=None\n",
    "calculated_rows=len(dataset_nlp_tokenization[\"reduced_genres\"].apply(tuple).value_counts()[dataset_nlp_tokenization[\"reduced_genres\"].apply(tuple).value_counts()==1])\n",
    "print(f\"Number of movies that are assigned to only 1 sequence of genres: {calculated_rows}\")\n",
    "list_of_movies_to_remove=dataset_nlp_tokenization[\"reduced_genres\"].apply(tuple).value_counts()[dataset_nlp_tokenization[\"reduced_genres\"].apply(tuple).value_counts()==1].index.tolist()\n",
    "list_of_movies_to_remove=[list(x) for x in list_of_movies_to_remove]\n",
    "assert type(list_of_movies_to_remove[0]) is list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48703, 38)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Below are the indexes of rows that should be removed from the dataset. In total 131 indexes.\n",
    "With those final 48991 rows of the dataset, the stratified sampling will be successfully completed.\n",
    "\"\"\"\n",
    "indexes_to_remove=dataset_nlp_tokenization['reduced_genres'].map(lambda x: 1 if list(x) in list_of_movies_to_remove else 0)[dataset_nlp_tokenization['reduced_genres'].map(lambda x: 1 if x in list_of_movies_to_remove else 0)==1].index.tolist()\n",
    "dataset_nlp_tokenization=dataset_nlp_tokenization[~dataset_nlp_tokenization.index.isin(indexes_to_remove)]\n",
    "dataset_nlp_tokenization=dataset_nlp_tokenization.reset_index(drop=True)\n",
    "dataset_nlp_tokenization.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Serialize a list of movie's year of release. This list will be used extensively in Part 5\"\"\"\n",
    "year_list=dataset_nlp_tokenization['year'].values.tolist()\n",
    "with open(f\"general_data_samples//year_list_{version_data_control}.pkl\",\"wb\") as f:\n",
    "    joblib.dump(year_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Serialize a list of the movie titles. This list will be used in many of the notebooks to come.\"\"\"\n",
    "movie_title_list=dataset_nlp_tokenization['title'].values.tolist()\n",
    "with open(f\"general_data_samples//movie_title_list_{version_data_control}.pkl\",\"wb\") as f:\n",
    "    joblib.dump(movie_title_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------------------------------------------------------------------------------------------\n",
    "#### R&D section for testing the function 4 - used for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_rnd=dataset_nlp_tokenization.iloc[0:].copy()\n",
    "# dataset_rnd=dataset_rnd.reset_index(drop=True)\n",
    "\n",
    "#transform_columns(\"plot_overview\", \"clean_plot_summary\", dataset_rnd.iloc[46280:46281], \"process_plot\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R&D section for testing the function 4 - used for demo\n",
    "#---------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 48703/48703 [00:00<00:00, 666733.65it/s]\n",
      "  0%|                                                                                        | 0/48703 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Execution\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Transfrom the column of the actors\n",
      "Finished the actors transformation after: 0.08 seconds\n",
      "\n",
      "Transfrom the column of the plot summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 48703/48703 [09:55<00:00, 81.82it/s]\n",
      "  0%|                                                                              | 2/48703 [00:00<1:04:07, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the plot transformation after: 9 minutes and 55.31 seconds\n",
      "\n",
      "Transfrom the column of the movie reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 48703/48703 [1:35:20<00:00,  8.51it/s]\n",
      " 40%|████████████████████████████▊                                           | 19465/48703 [00:00<00:00, 192726.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the reviews_pruned transformation after: 1 hour, 35 minutes and 20.49 seconds\n",
      "\n",
      "Transfrom the column of the movie title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 48703/48703 [00:00<00:00, 191746.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the movie title transformation after: 0.3 seconds\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Finished Execution after: 1 hour, 45 minutes and 16.19 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Apply the function 3. The function executed will extensively clean the corpus text per input variable by calling functions 1 and 2.\n",
    "\"\"\"\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Transfrom the columns:\n",
    "# -> Actors\n",
    "# -> Plot summary\n",
    "# -> Movie Features\n",
    "# -> Reviews\n",
    "# -> Movie Title\n",
    "\n",
    "print(\"Start Execution\")\n",
    "begin_time=time.time()\n",
    "print(\"---------------------------------------------------------------------------------\\n\")\n",
    "print(\"Transfrom the column of the actors\")\n",
    "start_time_one=time.time()\n",
    "transform_columns(\"actors_unified\", \"clean_actors\", dataset_nlp_tokenization, \"process_actors\", False)\n",
    "print(f\"Finished the actors transformation after: {format_timespan(time.time()-start_time_one)}\\n\")\n",
    "\n",
    "print(\"Transfrom the column of the plot summary\")\n",
    "start_time_two=time.time()\n",
    "transform_columns(\"plot_overview\", \"clean_plot_summary\", dataset_nlp_tokenization, \"process_plot\", False)\n",
    "print(f\"Finished the plot transformation after: {format_timespan(time.time()-start_time_two)}\\n\")\n",
    "\n",
    "print(\"Transfrom the column of the movie reviews\")\n",
    "start_time_four=time.time()\n",
    "transform_columns(\"reviews_enriched\", \"clean_reviews\", dataset_nlp_tokenization, \"process_reviews\", False)\n",
    "print(f\"Finished the reviews_pruned transformation after: {format_timespan(time.time()-start_time_four)}\\n\")\n",
    "\n",
    "print(\"Transfrom the column of the movie title\")\n",
    "start_time_five=time.time()\n",
    "transform_columns(\"title\", \"clean_movie_title\", dataset_nlp_tokenization, \"process_title\", False)\n",
    "print(f\"Finished the movie title transformation after: {format_timespan(time.time()-start_time_five)}\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------\\n\")\n",
    "print(f\"Finished Execution after: {format_timespan(time.time()-begin_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>clean_actors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, clean_actors]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Check how many actors from the cleaned actors column, have a name equal to string length 1\"\"\"\n",
    "mask=dataset_nlp_tokenization[\"clean_actors\"].str.split(\",\").explode().str.len().eq(1)\n",
    "res=dataset_nlp_tokenization[[\"title\", \"clean_actors\"]].loc[np.unique(mask.loc[mask].index)]\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy story tom hanks tim allen don rickles jim varney wallace shawn john ratzenberger annie potts john morris erik von detten laurie metcalf r. lee ermey sarah freeman penn jillette jack angel spencer aste john lasseter led difference room heart happily buzz afraid eventually bring toy andy aside owner losing plot lightyear learn circumstance live scene separate place woody birthday duo adventure animation children'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Create the column Combined Features again with the plot_overview column\"\"\"\n",
    "def combine_features(row):\n",
    "    \"\"\"\n",
    "    Purpose: This function will create the column 'movie_features'. The column will combine text content from multiple columns of the dataframe.\n",
    "             Specifically the text combined derives from the following columns:\n",
    "             1) Title,\n",
    "             2) Actors,\n",
    "             3) Director,\n",
    "             4) Plot summary,\n",
    "             5) Genres\n",
    "    Argument: Dataset row\n",
    "    Output: The unified text of five columns into one column.\n",
    "    \"\"\"\n",
    "    return row[\"clean_movie_title\"].lower() + \" \" + \" \".join(row[\"clean_actors\"].split(\",\")).lower() + \" \" + row[\"director\"].lower() + \" \" + row[\"clean_plot_summary\"].lower() + \" \" + \" \".join(row[\"reduced_genres\"]).lower()\n",
    "\n",
    "dataset_nlp_tokenization[\"clean_combined_features\"]=dataset_nlp_tokenization.apply(combine_features, axis=1)\n",
    "dataset_nlp_tokenization[\"clean_combined_features\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andy\\'s toys live a reasonable life of fun and peace, their only worries are birthdays and Christmases, when new toys could easily replace those already there. One such birthday Andy\\'s top toy, Woody the cowboy, finds himself in direct competition with Andy\\'s new Buzz Lightyear doll. When rivalries boil over Woody tries to hide Buzz down the side of the bed but accidentally pushes him out the window, the other tops expel Woody, and he leaves with no choice but to find Buzz and return him to the house. But with only two days before Andy moves house, time is of the essence. Given how often the same mix of animation, wit, jokes and kids humour has been used since Toy Story (Ice Age, Monsters Inc, Bugs Life) it is easy to forget how refreshing it was when it first came out. I have just watched it again and it is dating a little in comparison to more recent twists on the formula. It seems each one has to be sharper and have more references etc in the background. However it is still very funny and deserves praise for being the first of a successful formula. The plot is simple but effective and actually has genuine drama and excitement to it. The main story is fun but the degree of character development is what really shores it up. The conflict between Buzz and Woody is taken deeper than this and, when confronted by the truth of his status as a toy, Buzz\\'s turmoil is very real as opposed to him being a cartoon character and nothing more. Despite the two strong leads there is a real depth in the support cast. They may not actually have that many lines, but they have all the funniest lines. Most of the `adult\\' wit comes from the Potato Head, dinosaur, the pig and slinky dog. They are funny and are very well used. In fact the majority of this humour and plot will go right over kids heads. Looking back on it, I do feel a cynical edge on it in so much as this film must really have helped sales of the toy companies in the film. It\\'s hard not to see the marketing department standing behind this film rubbing their hands. However the actual product is so wonderfully fun that I forgot this quickly. The voice work is excellent and the characters match the actors. Hanks is good as Woody and Allen has a good B-movie type voice for Buzz. Varney, Ratzenberger, Ermey (doing his usual), Rickles and others are all really good in the support roles and, probably, come out as the favourite characters for adults. Overall this is a classic film that will appeal to adults as much as to kids (if not more). A good plot and a really sharp script make the already short running time fly by. The only downside is that your kids will want you to go out and buy the damn things! I am a big fan of the animated movies coming from the Pixar Studios. They are always looking for the newest technological possibilities to use in their movies, creating movies that are more than just worth a watch, even when they were made a decade ago. The movie is about toys that come to life when their owner is asleep or not in the same room. When the young boy\\'s birthday is coming up, all the toys are nervous. They don\\'t want to be ignored when the new one arrives. Woody the cowboy is their \"leader\" because he\\'s the most popular one of them all. He\\'s the only one that hasn\\'t got to be afraid, but than a new favorite arrives . . . Buzz Lightyear. He hates him and tries everything possible to get rid of him, but as the time passes by they learn to appreciate each other. . . When you see Toy Story, you may think that the different human like characters (Woody the cowboy for instance) aren\\'t always as perfect as we are used to see in todays animated movies. Perhaps that\\'s true, but if you keep in mind that all this was done in 1995, when computers weren\\'t yet as strong and the technology for creating such movies was almost unknown, than you can only have a lot of respect for what the creators did. I loved the story and liked the animations a lot. I give it an 8. 5/10. This is a very clever animated story that was a big hit, and justifiably so. It had a terrific sequel and if a third film came out, that would probably be a hit, too. When this came out, computer technology just was beginning to strut its stuff. Man, this looked awesome. Now, it\\'s routine because animation, which took a giant leap with this movie, has made a lot more giant strides. The humor in here, however, is what made this so popular. There are tons of funny lines, issued by characters voiced by Tom Hanks, Tim Allen, Jim Varney, Don Rickles, Wallace Shawn and John Ratzenberger, among others. As good as Hanks is as \"Woody\" and Allen as \"Buzz Armstrong, \" I think the supporting characters just about stole the show: Mr. Potato Head, Slinky, Rex the dinosaur, etc. Multiple viewings don\\'t diminish the entertainment, either. There are so many things to catch, audibly and visually, that you always seem to discover something new. The colors in here are beautiful, too. This is a guaranteed \"winner\" as is the sequel.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Before pre-processing the raw text of the first review about Toy Story. Text has been many stop words, punctuations and words in many different tense!\n",
    "\"\"\"\n",
    "dataset_nlp_tokenization['reviews_enriched'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toy story tom hanks tim allen don rickles jim varney wallace shawn john ratzenberger annie potts john morris erik von detten laurie metcalf r. lee ermey sarah freeman penn jillette jack angel spencer aste john lasseter led difference room heart happily buzz afraid eventually bring toy andy aside owner losing plot lightyear learn circumstance live scene separate place woody birthday duo adventure animation children'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "After pre-processing the raw text of the first review about Toy Story. Text has been lemmatized and cleaned off most of the noise!\n",
    "\"\"\"\n",
    "dataset_nlp_tokenization['clean_combined_features'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C://Users//spano//Desktop//nlp_github//datasets//dataset_part_3.1_inputs_cleaned_22022021.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Having cleaned the dataset appropriately it is imperative to serialize and save a version of it.\n",
    "This action will save us approximately 2 hours of re-applying again the cleaning functions\n",
    "\"\"\"\n",
    "joblib.dump(dataset_nlp_tokenization,f\"{datasets_path}//dataset_part_3.1_inputs_cleaned_{version_data_control}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Stratified Shuffle Split using the train_test_split function (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Split the dataset into train & test sets (stratified shuffle split)\n",
      "\n",
      "Finished train-test split after: 0.09 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Split the dataset into train & validation sets (stratified shuffle split)\n",
      "\n",
      "Finished train-test split after: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Firstly, apply stratified shuffle split in the dataset to split it into training and test samples.\n",
    "Secondly, apply stratified shuffle split in the training sample to split it into validation and training sample.\n",
    "\n",
    "Split ratio of the first split: 80% training - 20% sample\n",
    "Split ratio of the second split: 80% training - 20% sample\n",
    "For imbalanced datasets and specifically for classification models, the stratification comes in handy because it ensures that the data will be splitted uniformly in both the train, test and validation samples.\n",
    "\"\"\"\n",
    "# Split the dataset into train & set sets\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nSplit the dataset into train & test sets (stratified shuffle split)\\n\")\n",
    "start_time=time.time()\n",
    "X_train, X_test, y_train, y_test=split_dataset(\"stratify\", dataset_nlp_tokenization.iloc[:, 19:36], dataset_nlp_tokenization, 0.2)\n",
    "assert X_train.shape[0]==y_train.shape[0]\n",
    "assert X_test.shape[0]==y_test.shape[0]\n",
    "print(\"Finished train-test split after: {0}\".format(format_timespan(time.time()-start_time)))\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nSplit the dataset into train & validation sets (stratified shuffle split)\\n\")\n",
    "start_time=time.time()\n",
    "X_train, X_validation, y_train, y_validation=split_dataset(\"stratify\", y_train, X_train, 0.2)\n",
    "assert X_train.shape[0]==y_train.shape[0]\n",
    "assert X_validation.shape[0]==y_validation.shape[0]\n",
    "\n",
    "assert X_train.shape[0]+X_validation.shape[0]+X_test.shape[0]==dataset_nlp_tokenization.shape[0]\n",
    "assert y_train.shape[0]+y_validation.shape[0]+y_test.shape[0]==dataset_nlp_tokenization.shape[0]\n",
    "\n",
    "print(\"Finished train-test split after: {0}\".format(format_timespan(time.time()-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (31169, 7)\n",
      "X_test shape: (9741, 7)\n",
      "X_validation shape: (7793, 7)\n",
      "\n",
      "y_train shape: (31169, 17)\n",
      "y_test shape: (9741, 17)\n",
      "y_validation shape: (7793, 17)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The shape of the X_train, X_test, y_train, y_test splitted and shuffled randomly\n",
    "\"\"\"\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"X_validation shape: {X_validation.shape}\\n\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"y_validation shape: {y_validation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama          25.372\n",
       "Comedy         15.956\n",
       "Action          7.580\n",
       "Romance         7.222\n",
       "Thriller        7.024\n",
       "Horror          5.734\n",
       "Crime           5.423\n",
       "Documentary     4.316\n",
       "Adventure       4.281\n",
       "Sci-Fi          3.010\n",
       "Mystery         2.721\n",
       "Children        2.520\n",
       "Animation       2.365\n",
       "Fantasy         2.288\n",
       "War             1.685\n",
       "Western         1.355\n",
       "Musical         1.145\n",
       "Name: reduced_genres, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(dataset_nlp_tokenization[\"reduced_genres\"].explode().value_counts(normalize=True)*100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drama          25.404\n",
      "Comedy         15.997\n",
      "Action          7.539\n",
      "Romance         7.243\n",
      "Thriller        7.041\n",
      "Horror          5.708\n",
      "Crime           5.438\n",
      "Documentary     4.308\n",
      "Adventure       4.282\n",
      "Sci-Fi          3.012\n",
      "Mystery         2.800\n",
      "Children        2.480\n",
      "Fantasy         2.298\n",
      "Animation       2.287\n",
      "War             1.670\n",
      "Western         1.361\n",
      "Musical         1.133\n",
      "Name: reduced_genres, dtype: float64 \n",
      "\n",
      "Drama          25.155\n",
      "Comedy         16.098\n",
      "Action          7.711\n",
      "Romance         7.176\n",
      "Thriller        6.990\n",
      "Horror          5.756\n",
      "Crime           5.424\n",
      "Documentary     4.562\n",
      "Adventure       4.157\n",
      "Sci-Fi          3.013\n",
      "Mystery         2.704\n",
      "Children        2.484\n",
      "Animation       2.473\n",
      "Fantasy         2.140\n",
      "War             1.707\n",
      "Western         1.295\n",
      "Musical         1.155\n",
      "Name: reduced_genres, dtype: float64 \n",
      "\n",
      "Drama          25.514\n",
      "Comedy         15.615\n",
      "Action          7.584\n",
      "Romance         7.199\n",
      "Thriller        6.996\n",
      "Horror          5.814\n",
      "Crime           5.366\n",
      "Adventure       4.435\n",
      "Documentary     4.044\n",
      "Sci-Fi          3.001\n",
      "Children        2.721\n",
      "Animation       2.540\n",
      "Fantasy         2.435\n",
      "Mystery         2.428\n",
      "War             1.721\n",
      "Western         1.406\n",
      "Musical         1.182\n",
      "Name: reduced_genres, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The stratification worked!\n",
    "\"\"\"\n",
    "print(round(X_train[\"reduced_genres\"].explode().value_counts(normalize=True)*100,3),\"\\n\")\n",
    "print(round(X_test[\"reduced_genres\"].explode().value_counts(normalize=True)*100,3),\"\\n\")\n",
    "print(round(X_validation[\"reduced_genres\"].explode().value_counts(normalize=True)*100,3),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\spano\\\\Desktop\\\\nlp_github\\\\NLP_Applications\\\\multi-label-text-classification-on-movies\\\\train_test_validation_all_inputs//y_validation_all_inputs_22022021.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The below cell serialises the training, validation and test samples created by the stratified split.\n",
    "\"\"\"\n",
    "joblib.dump(X_train,os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//X_train_all_inputs_{version_data_control}.pkl\"))\n",
    "joblib.dump(X_test,os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//X_test_all_inputs_{version_data_control}.pkl\"))\n",
    "joblib.dump(X_validation,os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//X_validation_all_inputs_{version_data_control}.pkl\"))\n",
    "\n",
    "joblib.dump(y_train,os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//y_train_all_inputs_{version_data_control}.pkl\"))\n",
    "joblib.dump(y_test,os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//y_test_all_inputs_{version_data_control}.pkl\"))\n",
    "joblib.dump(y_validation,os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//y_validation_all_inputs_{version_data_control}.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read from the local disk the serialized training, validation and test samples.\n",
    "\"\"\"\n",
    "X_train=joblib.load(os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//X_train_all_inputs_{version_data_control}.pkl\"))\n",
    "X_test=joblib.load(os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//X_test_all_inputs_{version_data_control}.pkl\"))\n",
    "X_validation=joblib.load(os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//X_validation_all_inputs_{version_data_control}.pkl\"))\n",
    "\n",
    "y_train=joblib.load(os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//y_train_all_inputs_{version_data_control}.pkl\"))\n",
    "y_test=joblib.load(os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//y_test_all_inputs_{version_data_control}.pkl\"))\n",
    "y_validation=joblib.load(os.path.join(os.getcwd(), f\"train_test_validation_all_inputs//y_validation_all_inputs_{version_data_control}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that the data is splitted, we separated each column of interest to a different X_train and X_test\n",
    "Those train and text X sets will be later used for tokenization and padding\n",
    "\"\"\"\n",
    "# Separate each different input column (actors, plot, features, reviews, title)\n",
    "\n",
    "X_train_actors=X_train[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "X_train_plot=X_train[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "X_train_features=X_train[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "X_train_reviews=X_train[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "X_train_title=X_train[[\"title\", \"clean_movie_title\", \"reduced_genres\"]]\n",
    "assert X_train_actors.shape==X_train_plot.shape==X_train_features.shape==X_train_reviews.shape==X_train_title.shape\n",
    "\n",
    "X_test_actors=X_test[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "X_test_plot=X_test[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "X_test_features=X_test[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "X_test_reviews=X_test[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "X_test_title=X_test[[\"title\", \"clean_movie_title\", \"reduced_genres\"]]\n",
    "assert X_test_actors.shape==X_test_plot.shape==X_test_features.shape==X_test_reviews.shape==X_test_title.shape\n",
    "\n",
    "X_validation_actors=X_validation[[\"title\", \"clean_actors\", \"reduced_genres\"]]\n",
    "X_validation_plot=X_validation[[\"title\", \"clean_plot_summary\", \"reduced_genres\"]]\n",
    "X_validation_features=X_validation[[\"title\", \"clean_combined_features\", \"reduced_genres\"]]\n",
    "X_validation_reviews=X_validation[[\"title\", \"clean_reviews\", \"reduced_genres\"]]\n",
    "X_validation_title=X_validation[[\"title\", \"clean_movie_title\", \"reduced_genres\"]]\n",
    "assert X_validation_actors.shape==\\\n",
    "       X_validation_plot.shape==\\\n",
    "       X_validation_features.shape==\\\n",
    "       X_validation_reviews.shape==\\\n",
    "       X_validation_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>clean_actors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The City of Lost Children</td>\n",
       "      <td>ron perlman,daniel emilfork,judith vittet,dominique pinon,jean-claude dreyfus,geneviève brunet,odile mallet,mireille mossé,serge merlin,rufus,ticky holgado,joseph lucien,mapi galán,briac barthélémy,pierre-quentin faesch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Heidi Fleiss: Hollywood Madam</td>\n",
       "      <td>nick broomfield,nina xining zuo,madam alex,corinne bohrer,mike brambles,cookie,elisa fleiss,heidi fleiss,jason fleiss,jesse fleiss,kim fleiss,paul fleiss,shannon fleiss,gabby,daryl gates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>The Arrival</td>\n",
       "      <td>charlie sheen,lindsay crouse,richard schiff,shane,ron silver,teri polo,phyllis applegate,alan coates,leon rippy,buddy joe hooker,javier morga,tony t. johnson,catalina botello,georg lillitsch,david villalpando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Lost Horizon</td>\n",
       "      <td>ronald colman,jane wyatt,edward everett horton,john howard,thomas mitchell,margo,isabel jewell,h.b. warner,sam jaffe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>Delicatessen</td>\n",
       "      <td>pascal benezech,dominique pinon,marie-laure dougnac,jean-claude dreyfus,karin viard,ticky holgado,anne-marie pisani,boban janevski,mikael todde,edith ker,rufus,jacques mathou,howard vernon,chick ortega,silvie laguna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47496</th>\n",
       "      <td>Iracema</td>\n",
       "      <td>paulo césar peréio,edna de cássia,lúcio dos santos,elma martins,natal,fernando neves,wilmar nunes,sidney piñon,rose rodrigues,conceição senna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47646</th>\n",
       "      <td>Moonlight on the Prairie</td>\n",
       "      <td>dick foran,smoke,sheila bromley,george e. stone,joe sawyer,joe king,robert barrat,dickie jones,bill elliott,herbert heywood,raymond brown,richard carle,milton kibbee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48468</th>\n",
       "      <td>Krishnanum Radhayum</td>\n",
       "      <td>souparnika,rupa jith,devika,ajit,ajayan,hanifa,prathyush,navajyot pandit,varsha pandit,sherij,liji,navindran,sunil,santhosh pandit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48485</th>\n",
       "      <td>Uzhaippali</td>\n",
       "      <td>s.s. chandran,charlie,goundamani,kavitha,mayilsamy,prathapachandran,rajinikanth,nizhalgal ravi,radha ravi,roja,srividya,sujatha,vijayakumar,visu,vivek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48579</th>\n",
       "      <td>Io c'è</td>\n",
       "      <td>edoardo leo,margherita buy,giuseppe battiston,giulia michelini,massimiliano bruno,gisella burinato,claudia della seta,franco pinelli,gegia,andrea purgatori,vittorio hamarz vasfi,isabella merafino,luciano miele,lorenzo gioielli,ana brigitte fernández</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "28         The City of Lost Children   \n",
       "97     Heidi Fleiss: Hollywood Madam   \n",
       "716                      The Arrival   \n",
       "890                     Lost Horizon   \n",
       "1093                    Delicatessen   \n",
       "...                              ...   \n",
       "47496                        Iracema   \n",
       "47646       Moonlight on the Prairie   \n",
       "48468            Krishnanum Radhayum   \n",
       "48485                     Uzhaippali   \n",
       "48579                         Io c'è   \n",
       "\n",
       "                                                                                                                                                                                                                                                    clean_actors  \n",
       "28                                   ron perlman,daniel emilfork,judith vittet,dominique pinon,jean-claude dreyfus,geneviève brunet,odile mallet,mireille mossé,serge merlin,rufus,ticky holgado,joseph lucien,mapi galán,briac barthélémy,pierre-quentin faesch  \n",
       "97                                                                    nick broomfield,nina xining zuo,madam alex,corinne bohrer,mike brambles,cookie,elisa fleiss,heidi fleiss,jason fleiss,jesse fleiss,kim fleiss,paul fleiss,shannon fleiss,gabby,daryl gates  \n",
       "716                                             charlie sheen,lindsay crouse,richard schiff,shane,ron silver,teri polo,phyllis applegate,alan coates,leon rippy,buddy joe hooker,javier morga,tony t. johnson,catalina botello,georg lillitsch,david villalpando  \n",
       "890                                                                                                                                         ronald colman,jane wyatt,edward everett horton,john howard,thomas mitchell,margo,isabel jewell,h.b. warner,sam jaffe  \n",
       "1093                                     pascal benezech,dominique pinon,marie-laure dougnac,jean-claude dreyfus,karin viard,ticky holgado,anne-marie pisani,boban janevski,mikael todde,edith ker,rufus,jacques mathou,howard vernon,chick ortega,silvie laguna  \n",
       "...                                                                                                                                                                                                                                                          ...  \n",
       "47496                                                                                                              paulo césar peréio,edna de cássia,lúcio dos santos,elma martins,natal,fernando neves,wilmar nunes,sidney piñon,rose rodrigues,conceição senna  \n",
       "47646                                                                                      dick foran,smoke,sheila bromley,george e. stone,joe sawyer,joe king,robert barrat,dickie jones,bill elliott,herbert heywood,raymond brown,richard carle,milton kibbee  \n",
       "48468                                                                                                                         souparnika,rupa jith,devika,ajit,ajayan,hanifa,prathyush,navajyot pandit,varsha pandit,sherij,liji,navindran,sunil,santhosh pandit  \n",
       "48485                                                                                                     s.s. chandran,charlie,goundamani,kavitha,mayilsamy,prathapachandran,rajinikanth,nizhalgal ravi,radha ravi,roja,srividya,sujatha,vijayakumar,visu,vivek  \n",
       "48579  edoardo leo,margherita buy,giuseppe battiston,giulia michelini,massimiliano bruno,gisella burinato,claudia della seta,franco pinelli,gegia,andrea purgatori,vittorio hamarz vasfi,isabella merafino,luciano miele,lorenzo gioielli,ana brigitte fernández  \n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now the movies do not contain actors with names of single, 2, or 3 letters\n",
    "mask=X_validation_actors.clean_actors.str.split(\",\").explode().str.len().eq(5)\n",
    "res=X_validation_actors[['title', 'clean_actors']].loc[np.unique(mask.loc[mask].index)]\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------\n",
    "Actors-CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spano\\Miniconda3\\envs\\demoEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length of CountVectorizer of the actors corpus: 260913\n",
      "(260913, 2)\n",
      "Wall time: 8.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the actor names\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "-> Probably the infrequent tokens will make a better classification\n",
    "\"\"\"\n",
    "def actors_split(s):\n",
    "    return s.split(',')\n",
    "\n",
    "corpus_actors=dataset_nlp_tokenization['clean_actors'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(encoding='utf-8', tokenizer=actors_split, max_df=1.0, min_df=1) #keep this to 1 to include all the words/tokens\n",
    "c_vectorizer.fit(corpus_actors)\n",
    "print(f\"Vocabulary length of CountVectorizer of the actors corpus: {len(c_vectorizer.vocabulary_)}\")\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_actors)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_actors=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_actors.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "actors_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_actors = dict((k, v) for k, v in actors_frequency_dictionary.items() if v >= 80) # v = popularity (frequency) or number of movies played\n",
    "# sorted(d_actors.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of actors that exist in the dataset is: 260913\n",
      "The 95% (247867) of the actors will be tokenized and the rest 5% (13046) of the actors will be removed due to sparsity\n"
     ]
    }
   ],
   "source": [
    "percent_tokenized=0.95\n",
    "print(f\"The total number of actors that exist in the dataset is: {len(c_vectorizer.vocabulary_)}\")\n",
    "countvectorizer_actors_tokenized=int(len(c_vectorizer.vocabulary_)*percent_tokenized)\n",
    "print(f\"The 95% ({countvectorizer_actors_tokenized}) of the actors will be tokenized and the rest 5% ({len(c_vectorizer.vocabulary_)-countvectorizer_actors_tokenized}) of the actors will be removed due to sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------\n",
    "Plot summary-CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spano\\Miniconda3\\envs\\demoEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length of CountVectorizer of the plot corpus: 63033\n",
      "(63033, 2)\n",
      "Wall time: 7.4 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find the most frequent words among the movie plots\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def plot_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_plot=dataset_nlp_tokenization['clean_plot_summary'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(encoding='utf-8', tokenizer=plot_split, max_df=1.0, min_df=1) #keep this to 1 to include all the words/tokens\n",
    "\n",
    "c_vectorizer.fit(corpus_plot)\n",
    "print(f\"Vocabulary length of CountVectorizer of the plot corpus: {len(c_vectorizer.vocabulary_)}\")\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_plot)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df_plot=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df_plot.reset_index(drop=True)\n",
    "token_frequency_df_plot=token_frequency_df_plot.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df_plot.shape)\n",
    "#token_frequency_df_pruned_plot=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_plot.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "plot_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_plot = dict((k, v) for k, v in plot_frequency_dictionary.items() if v >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of plot tokens that exist in the dataset is: 63033\n",
      "The 95% (59881) of the plot summary tokens will be tokenized and the rest 5% (3152) of the plot words will be removed due to sparsity\n"
     ]
    }
   ],
   "source": [
    "percent_tokenized=0.95\n",
    "print(f\"The total number of plot tokens that exist in the dataset is: {len(c_vectorizer.vocabulary_)}\")\n",
    "countvectorizer_plot_words_tokenized=int(len(c_vectorizer.vocabulary_)*percent_tokenized)\n",
    "print(f\"The 95% ({countvectorizer_plot_words_tokenized}) of the plot summary tokens will be tokenized and the rest 5% ({len(c_vectorizer.vocabulary_)-countvectorizer_plot_words_tokenized}) of the plot words will be removed due to sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------\n",
    "Features-CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spano\\Miniconda3\\envs\\demoEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length of CountVectorizer of the features corpus: 191341\n",
      "(191341, 2)\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Find the most frequent words among the movie features\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def movie_features_split(s):\n",
    "    return s.split(' ')\n",
    "### -------------------------------------------------------------------------------------------------------------------------------------\n",
    "Actors-CountVectorizer\n",
    "corpus_features=dataset_nlp_tokenization['clean_combined_features'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(encoding='utf-8', tokenizer=movie_features_split, max_df=1.0, min_df=1) #keep this to 1 to include all the words/tokens\n",
    "\n",
    "c_vectorizer.fit(corpus_features)\n",
    "print(f\"Vocabulary length of CountVectorizer of the features corpus: {len(c_vectorizer.vocabulary_)}\")\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_features)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_features=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_features.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "features_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_features = dict((k, v) for k, v in features_frequency_dictionary.items() if v >= 90)\n",
    "# sorted(d_features.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of feature tokens that exist in the dataset is: 191341\n",
      "The 95% (181773) of the feature tokens will be tokenized and the rest 5% (9568) of the words will be removed due to sparsity\n"
     ]
    }
   ],
   "source": [
    "percent_tokenized=0.95\n",
    "print(f\"The total number of feature tokens that exist in the dataset is: {len(c_vectorizer.vocabulary_)}\")\n",
    "countvectorizer_features_words_tokenized=int(len(c_vectorizer.vocabulary_)*percent_tokenized)\n",
    "print(f\"The 95% ({countvectorizer_features_words_tokenized}) of the feature tokens will be tokenized and the rest 5% ({len(c_vectorizer.vocabulary_)-countvectorizer_features_words_tokenized}) of the words will be removed due to sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------\n",
    "Reviews-CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spano\\Miniconda3\\envs\\demoEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length of CountVectorizer of the reviews corpus: 187348\n",
      "(187348, 2)\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find the most frequent words among the movie reviews\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def reviews_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_reviews=dataset_nlp_tokenization['clean_reviews'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(encoding='utf-8', tokenizer=reviews_split, max_df=1.0, min_df=1) #keep this to 1 to include all the words/tokens\n",
    "\n",
    "c_vectorizer.fit(corpus_reviews)\n",
    "print(f\"Vocabulary length of CountVectorizer of the reviews corpus: {len(c_vectorizer.vocabulary_)}\")\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_reviews)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_reviews=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_reviews.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "reviews_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_reviews = dict((k, v) for k, v in reviews_frequency_dictionary.items() if v >= 100)\n",
    "# sorted(d_reviews.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44351"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Get the frequency of word 'film' (i.e in how many reviews this word is used)\"\"\"\n",
    "reviews_frequency_dictionary['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of review tokens that exist in the dataset is: 187348\n",
      "The 95% (177980) of the review tokens will be tokenized and the rest 5% (9368) of the words will be removed due to sparsity\n"
     ]
    }
   ],
   "source": [
    "percent_tokenized=0.95\n",
    "print(f\"The total number of review tokens that exist in the dataset is: {len(c_vectorizer.vocabulary_)}\")\n",
    "countvectorizer_reviews_words_tokenized=int(len(c_vectorizer.vocabulary_)*percent_tokenized)\n",
    "print(f\"The 95% ({countvectorizer_reviews_words_tokenized}) of the review tokens will be tokenized and the rest 5% ({len(c_vectorizer.vocabulary_)-countvectorizer_reviews_words_tokenized}) of the words will be removed due to sparsity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------\n",
    "Title-CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spano\\Miniconda3\\envs\\demoEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length of CountVectorizer of the title corpus: 23462\n",
      "(23462, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find the most frequent words among the movie titles\n",
    "In the end the number of rows will be equal to the number of maximum features tokenized by the Tokenizer.\n",
    "\"\"\"\n",
    "def movie_title_split(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "corpus_title=dataset_nlp_tokenization['clean_movie_title'].values.tolist()\n",
    "c_vectorizer=CountVectorizer(encoding='utf-8', tokenizer=movie_title_split, max_df=1.0, min_df=1) #keep this to 1 to include all the words/tokens\n",
    "\n",
    "c_vectorizer.fit(corpus_title)\n",
    "print(f\"Vocabulary length of CountVectorizer of the title corpus: {len(c_vectorizer.vocabulary_)}\")\n",
    "\n",
    "X=c_vectorizer.fit_transform(corpus_title)\n",
    "X_words=c_vectorizer.inverse_transform(X)\n",
    "\n",
    "tokens_list=c_vectorizer.get_feature_names()\n",
    "count_list = np.asarray(X.sum(axis=0)).ravel().tolist()\n",
    "\n",
    "token_frequency_df=pd.DataFrame({'term': c_vectorizer.get_feature_names(), 'token_frequency': count_list})\n",
    "token_frequency_df=token_frequency_df.sort_values(by='token_frequency', ascending=False)\n",
    "print(token_frequency_df.shape)\n",
    "#token_frequency_df_pruned_title=token_frequency_df[token_frequency_df['token_frequency']>=3]\n",
    "#print(token_frequency_df_pruned_title.shape)\n",
    "\n",
    "# The below code sample creates a dictionary that shows only the n-frequent actors\n",
    "title_frequency_dictionary=dict(zip(tokens_list, count_list))\n",
    "d_title = dict((k, v) for k, v in title_frequency_dictionary.items() if v >= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of movie title tokens that exist in the dataset is: 23462\n",
      "The 95% (22288) of the movie title tokens will be tokenized and the rest 5% (1174) of the words will be removed due to sparsity\n"
     ]
    }
   ],
   "source": [
    "percent_tokenized=0.95\n",
    "print(f\"The total number of movie title tokens that exist in the dataset is: {len(c_vectorizer.vocabulary_)}\")\n",
    "countvectorizer_title_words_tokenized=int(len(c_vectorizer.vocabulary_)*0.95)\n",
    "print(f\"The 95% ({countvectorizer_title_words_tokenized}) of the movie title tokens will be tokenized and the rest 5% ({len(c_vectorizer.vocabulary_)-countvectorizer_title_words_tokenized}) of the words will be removed due to sparsity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actors_tokenized: 247867\n",
      "plot_words_tokenized: 59881\n",
      "features_words_tokenized: 181773\n",
      "reviews_words_tokenized: 177980\n",
      "title_words_tokenized: 22288\n"
     ]
    }
   ],
   "source": [
    "\"\"\"95% of the words per input that will be tokenized\"\"\"\n",
    "print(f\"actors_tokenized: {countvectorizer_actors_tokenized}\")\n",
    "print(f\"plot_words_tokenized: {countvectorizer_plot_words_tokenized}\")\n",
    "print(f\"features_words_tokenized: {countvectorizer_features_words_tokenized}\")\n",
    "print(f\"reviews_words_tokenized: {countvectorizer_reviews_words_tokenized}\")\n",
    "print(f\"title_words_tokenized: {countvectorizer_title_words_tokenized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code blocks above we implemented the CountVectorizer way to calculate the unique tokens per input\n",
    "### -------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize the dataset (using the keras tokenizer class)\n",
      "\n",
      "------------\n",
      "Actors corpus\n",
      "------------\n",
      "Maximum length of unique tokens is: 260913\n",
      "Number of words to be tokenized is the 95% of those unique tokens, equal to: 247867\n",
      "The rest 5% or 13046 is not tokenized.\n",
      "\n",
      "Keras Tokenizer result is equal to Count Vectorizer result!\n",
      "Number of words mapped: 247866. The extra 1 index represents the OVV token, which is not included in the CountVectorizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spano\\Miniconda3\\envs\\demoEnv\\lib\\site-packages\\pandas\\core\\indexing.py:1781: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actors tokenized with maximum number of words: 247867\n",
      "Finished the actors corpus tokenization after: 7.09 seconds\n",
      "\n",
      "------------\n",
      "Plot Summary corpus\n",
      "------------\n",
      "Maximum length of unique tokens is: 63034\n",
      "Number of words to be tokenized is the 95% of those unique tokens, equal to: 59882\n",
      "The rest 5% or 3152 is not tokenized.\n",
      "\n",
      "Keras Tokenizer result is not equal to Count Vectorizer result!\n",
      "Number of words mapped: 59881. The extra 1 index represents the OVV token, which is not included in the CountVectorizer.\n",
      "ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather is equal to: 59882\n",
      "Correct length: 59881.\n",
      "Plot Summary tokenized with maximum number of words: 59882\n",
      "Finished the plot corpus tokenization after: 3.11 seconds\n",
      "\n",
      "------------\n",
      "Movie Features corpus\n",
      "------------\n",
      "Maximum length of unique tokens is: 191341\n",
      "Number of words to be tokenized is the 95% of those unique tokens, equal to: 181774\n",
      "The rest 5% or 9567 is not tokenized.\n",
      "\n",
      "Keras Tokenizer result is not equal to Count Vectorizer result!\n",
      "Number of words mapped: 181773. The extra 1 index represents the OVV token, which is not included in the CountVectorizer.\n",
      "ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather is equal to: 181773\n",
      "Correct length: 181773.\n",
      "Movie Features tokenized with maximum number of words: 181773\n",
      "Finished the movie features corpus tokenization after: 8.73 seconds\n",
      "\n",
      "------------\n",
      "Movie Reviews corpus\n",
      "------------\n",
      "Maximum length of unique tokens is: 187349\n",
      "Number of words to be tokenized is the 95% of those unique tokens, equal to: 177982\n",
      "The rest 5% or 9367 is not tokenized.\n",
      "\n",
      "Keras Tokenizer result is not equal to Count Vectorizer result!\n",
      "Number of words mapped: 177981. The extra 1 index represents the OVV token, which is not included in the CountVectorizer.\n",
      "ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather is equal to: 177982\n",
      "Correct length: 177980.\n",
      "Movie Reviews tokenized with maximum number of words: 177982\n",
      "Finished the movie reviews corpus tokenization after: 17.91 seconds\n",
      "\n",
      "------------\n",
      "Movie Title corpus\n",
      "------------\n",
      "Maximum length of unique tokens is: 23462\n",
      "Number of words to be tokenized is the 95% of those unique tokens, equal to: 22289\n",
      "The rest 5% or 1173 is not tokenized.\n",
      "\n",
      "Keras Tokenizer result is not equal to Count Vectorizer result!\n",
      "Number of words mapped: 22288. The extra 1 index represents the OVV token, which is not included in the CountVectorizer.\n",
      "ERROR: The length of the vocabulary is not equal to the number of word_index dictionary, but rather is equal to: 22288\n",
      "Correct length: 22288.\n",
      "Movie Title tokenized with maximum number of words: 22288\n",
      "Finished the movie title corpus tokenization after: 5.06 seconds\n",
      "\n",
      "Finished tokenization of all 5 trainable columns after: 41.91 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data tokenization is one of the most important parts when dealing with text data.\n",
    "Since I am going to deploy keras models, I use the python api of Keras Tokenizer,\n",
    "more details about its use on: https://keras.io/preprocessing/text/\n",
    "\"\"\"\n",
    "print(\"\\nTokenize the dataset (using the keras tokenizer class)\\n\")\n",
    "begin_time=time.time()\n",
    "start_time_one=time.time()\n",
    "print(\"------------\\nActors corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_actors, tokenizer_actors=keras_tokenization(\"actors\", countvectorizer_actors_tokenized, dataset_nlp_tokenization, \"clean_actors\", X_train_actors, X_test_actors, X_validation_actors, \"actors_seq\")\n",
    "print(f\"Actors tokenized with maximum number of words: {vocabulary_size_frequent_words_actors}\")\n",
    "\n",
    "# Serialize the Actors Tokenizer\n",
    "joblib.dump(tokenizer_actors, f\"word_tokenizers//actors_tokenizer_{vocabulary_size_frequent_words_actors}_{version_data_control}.pkl\")\n",
    "print(f\"Finished the actors corpus tokenization after: {format_timespan(time.time()-start_time_one)}\")\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "start_time_two=time.time()\n",
    "print(\"\\n------------\\nPlot Summary corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_plot, tokenizer_plot=keras_tokenization(\"plot\", countvectorizer_plot_words_tokenized, dataset_nlp_tokenization, \"clean_plot_summary\", X_train_plot, X_test_plot, X_validation_plot, \"plot_seq\")\n",
    "print(f\"Plot Summary tokenized with maximum number of words: {vocabulary_size_frequent_words_plot}\")\n",
    "\n",
    "# Serialize the Plot Tokenizer\n",
    "joblib.dump(tokenizer_plot, f\"word_tokenizers//plot_tokenizer_{vocabulary_size_frequent_words_plot}_{version_data_control}.pkl\")\n",
    "print(f\"Finished the plot corpus tokenization after: {format_timespan(time.time()-start_time_two)}\")\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "start_time_three=time.time()\n",
    "print(\"\\n------------\\nMovie Features corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_features, tokenizer_features=keras_tokenization(\"features\", countvectorizer_features_words_tokenized, dataset_nlp_tokenization, \"clean_combined_features\", X_train_features, X_test_features, X_validation_features, \"features_seq\")\n",
    "print(f\"Movie Features tokenized with maximum number of words: {vocabulary_size_frequent_words_features}\")\n",
    "\n",
    "# Serialize the Movie Features Tokenizer\n",
    "joblib.dump(tokenizer_features, f\"word_tokenizers//features_tokenizer_{vocabulary_size_frequent_words_features}_{version_data_control}.pkl\")\n",
    "print(f\"Finished the movie features corpus tokenization after: {format_timespan(time.time()-start_time_three)}\")\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "start_time_four=time.time()\n",
    "print(\"\\n------------\\nMovie Reviews corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_reviews, tokenizer_reviews=keras_tokenization(\"reviews\", countvectorizer_reviews_words_tokenized, dataset_nlp_tokenization, \"clean_reviews\", X_train_reviews, X_test_reviews, X_validation_reviews, \"reviews_seq\")\n",
    "print(f\"Movie Reviews tokenized with maximum number of words: {vocabulary_size_frequent_words_reviews}\")\n",
    "\n",
    "# Serialize the Reviews Tokenizer\n",
    "joblib.dump(tokenizer_reviews, f\"word_tokenizers//reviews_tokenizer_{vocabulary_size_frequent_words_reviews}_{version_data_control}.pkl\")\n",
    "print(f\"Finished the movie reviews corpus tokenization after: {format_timespan(time.time()-start_time_four)}\")\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "start_time_five=time.time()\n",
    "print(\"\\n------------\\nMovie Title corpus\\n------------\")\n",
    "vocabulary_size_frequent_words_title, tokenizer_title=keras_tokenization(\"movie_title\", countvectorizer_title_words_tokenized, dataset_nlp_tokenization, \"clean_movie_title\", X_train_title, X_test_title,  X_validation_title, \"title_seq\")\n",
    "print(f\"Movie Title tokenized with maximum number of words: {vocabulary_size_frequent_words_title}\")\n",
    "\n",
    "# Serialize the Title Tokenizer\n",
    "joblib.dump(tokenizer_title, f\"word_tokenizers//title_tokenizer_{vocabulary_size_frequent_words_title}_{version_data_control}.pkl\")\n",
    "print(f\"Finished the movie title corpus tokenization after: {format_timespan(time.time()-start_time_five)}\")\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "assert len(tokenizer_actors.word_index)==vocabulary_size_frequent_words_actors\n",
    "assert len(tokenizer_plot.word_index)==vocabulary_size_frequent_words_plot\n",
    "assert len(tokenizer_features.word_index)==vocabulary_size_frequent_words_features\n",
    "assert len(tokenizer_reviews.word_index)==vocabulary_size_frequent_words_reviews\n",
    "assert len(tokenizer_title.word_index)==vocabulary_size_frequent_words_title\n",
    "\n",
    "print(f\"\\nFinished tokenization of all 5 trainable columns after: {format_timespan(time.time()-begin_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actors_tokenized': 247867,\n",
       " 'plot_words_tokenized': 59882,\n",
       " 'features_words_tokenized': 181773,\n",
       " 'reviews_words_tokenized': 177982,\n",
       " 'title_words_tokenized': 22288}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Serialize in a dictionary the number of words tokenized per input column\"\"\"\n",
    "words_tokenized_per_trainable_feature={}\n",
    "words_tokenized_per_trainable_feature['actors_tokenized']=vocabulary_size_frequent_words_actors\n",
    "words_tokenized_per_trainable_feature['plot_words_tokenized']=vocabulary_size_frequent_words_plot\n",
    "words_tokenized_per_trainable_feature['features_words_tokenized']=vocabulary_size_frequent_words_features\n",
    "words_tokenized_per_trainable_feature['reviews_words_tokenized']=vocabulary_size_frequent_words_reviews\n",
    "words_tokenized_per_trainable_feature['title_words_tokenized']=vocabulary_size_frequent_words_title\n",
    "words_tokenized_per_trainable_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_tokenizers//words_tokenized_22022021.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(words_tokenized_per_trainable_feature, f\"word_tokenizers//words_tokenized_{version_data_control}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Specify the length of the maxlen variable (length is a parameter for the optimal padding execution)\n",
      "\n",
      "Max Length of the pad sequence for actors_seq: 15\n",
      "Max Length of the pad sequence for plot_seq: 54\n",
      "Max Length of the pad sequence for features_seq: 91\n",
      "Max Length of the pad sequence for reviews_seq: 442\n",
      "Max Length of the pad sequence for title_seq: 6\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Create the padding sequence of texts\n",
      "\n",
      "\n",
      "Actors padded sequences created\n",
      "\n",
      "Plot padded sequences created\n",
      "\n",
      "Movie Features padded sequences created\n",
      "\n",
      "Movie Reviews padded sequences created\n",
      "\n",
      "Movie Title padded sequences created\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate the maximum length per sequence for each of the five inputs.\n",
    "Then pad the sequences of that input to the maximum length calculated.\n",
    "\"\"\"\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nSpecify the length of the maxlen variable (length is a parameter for the optimal padding execution)\\n\")\n",
    "\n",
    "maxlen_actors=padding_sequence_length(\"actors_seq\", 95, X_train_actors, X_test_actors, X_validation_actors)\n",
    "maxlen_plot=padding_sequence_length(\"plot_seq\", 95, X_train_plot, X_test_plot, X_validation_plot)\n",
    "maxlen_features=padding_sequence_length(\"features_seq\", 95, X_train_features, X_test_features, X_validation_features)\n",
    "maxlen_reviews=padding_sequence_length(\"reviews_seq\", 95, X_train_reviews, X_test_reviews, X_validation_reviews)\n",
    "maxlen_title=padding_sequence_length(\"title_seq\", 95, X_train_title, X_test_title, X_validation_title)\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------------\")\n",
    "print(\"\\nCreate the padding sequence of texts\\n\")\n",
    "\n",
    "X_train_seq_actors, X_test_seq_actors, X_validation_seq_actors=padding_sequence(\"actors_seq\", X_train_actors, X_test_actors, X_validation_actors, y_train, y_test, y_validation, maxlen_actors)\n",
    "print(\"\\nActors padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_plot, X_test_seq_plot, X_validation_seq_plot=padding_sequence(\"plot_seq\", X_train_plot, X_test_plot, X_validation_plot, y_train, y_test, y_validation, maxlen_plot)\n",
    "print(\"Plot padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_features, X_test_seq_features, X_validation_seq_features=padding_sequence(\"features_seq\", X_train_features, X_test_features, X_validation_features, y_train, y_test, y_validation, maxlen_features)\n",
    "print(\"Movie Features padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_reviews, X_test_seq_reviews, X_validation_seq_reviews=padding_sequence(\"reviews_seq\", X_train_reviews, X_test_reviews, X_validation_reviews, y_train, y_test, y_validation, maxlen_reviews)\n",
    "print(\"Movie Reviews padded sequences created\\n\")\n",
    "\n",
    "X_train_seq_title, X_test_seq_title, X_validation_seq_title=padding_sequence(\"title_seq\", X_train_title, X_test_title, X_validation_title, y_train, y_test, y_validation, maxlen_title)\n",
    "print(\"Movie Title padded sequences created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data split:** 80-20 split\n",
    "* **Non-balanced** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X_train, X_test, X_validation with <b>80-20</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_seq_actors shape: (31169, 15)\n",
      "X_train_seq_plot shape: (31169, 54)\n",
      "X_train_seq_features shape: (31169, 91)\n",
      "X_train_seq_reviews shape: (31169, 442)\n",
      "X_train_seq_title shape: (31169, 6)\n",
      "\n",
      "X_test_seq_actors shape: (9741, 15)\n",
      "X_test_seq_plot shape: (9741, 54)\n",
      "X_test_seq_features shape: (9741, 91)\n",
      "X_test_seq_reviews shape: (9741, 442)\n",
      "X_test_seq_title shape: (9741, 6)\n",
      "\n",
      "X_validation_seq_actors shape: (7793, 15)\n",
      "X_validation_seq_plot shape: (7793, 54)\n",
      "X_validation_seq_features shape: (7793, 91)\n",
      "X_validation_seq_reviews shape: (7793, 442)\n",
      "X_validation_seq_title shape: (7793, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_seq_actors shape: {X_train_seq_actors.shape}\")\n",
    "print(f\"X_train_seq_plot shape: {X_train_seq_plot.shape}\")\n",
    "print(f\"X_train_seq_features shape: {X_train_seq_features.shape}\")\n",
    "print(f\"X_train_seq_reviews shape: {X_train_seq_reviews.shape}\")\n",
    "print(f\"X_train_seq_title shape: {X_train_seq_title.shape}\\n\")\n",
    "\n",
    "print(f\"X_test_seq_actors shape: {X_test_seq_actors.shape}\")\n",
    "print(f\"X_test_seq_plot shape: {X_test_seq_plot.shape}\")\n",
    "print(f\"X_test_seq_features shape: {X_test_seq_features.shape}\")\n",
    "print(f\"X_test_seq_reviews shape: {X_test_seq_reviews.shape}\")\n",
    "print(f\"X_test_seq_title shape: {X_test_seq_title.shape}\\n\")\n",
    "\n",
    "print(f\"X_validation_seq_actors shape: {X_validation_seq_actors.shape}\")\n",
    "print(f\"X_validation_seq_plot shape: {X_validation_seq_plot.shape}\")\n",
    "print(f\"X_validation_seq_features shape: {X_validation_seq_features.shape}\")\n",
    "print(f\"X_validation_seq_reviews shape: {X_validation_seq_reviews.shape}\")\n",
    "print(f\"X_validation_seq_title shape: {X_validation_seq_title.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y_train & y_test with <b>80-20</b> split and <b>non-balanced genre</b> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape:(31169, 17)\n",
      "y_test shape:(9741, 17)\n",
      "y_validation shape:(7793, 17)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train shape:{y_train.shape}\")\n",
    "print(f\"y_test shape:{y_test.shape}\")\n",
    "print(f\"y_validation shape:{y_validation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Serialized the tokenized and padded sequences per input features. Those sequences will be the main input layer for the NLP classifier on which the model with trained, validated and finally tested\"\"\"\n",
    "split_ratio=\"80_20\"\n",
    "data_balance=\"non_balanced\"\n",
    "#Training sample\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_train_seq_actors_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_actors}_{version_data_control}\"), X_train_seq_actors)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_train_seq_plot_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_plot}_{version_data_control}\"), X_train_seq_plot)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_train_seq_features_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_features}_{version_data_control}\"), X_train_seq_features)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_train_seq_reviews_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_reviews}_{version_data_control}\"), X_train_seq_reviews)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_train_seq_title_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_title}_{version_data_control}\"), X_train_seq_title)\n",
    "\n",
    "#Test sample\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_test_seq_actors_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_actors}_{version_data_control}\"), X_test_seq_actors)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_test_seq_plot_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_plot}_{version_data_control}\"), X_test_seq_plot)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_test_seq_features_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_features}_{version_data_control}\"), X_test_seq_features)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_test_seq_reviews_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_reviews}_{version_data_control}\"), X_test_seq_reviews)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_test_seq_title_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_title}_{version_data_control}\"), X_test_seq_title)\n",
    "\n",
    "#Validation sample\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_validation_seq_actors_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_actors}_{version_data_control}\"), X_validation_seq_actors)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_validation_seq_plot_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_plot}_{version_data_control}\"), X_validation_seq_plot)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_validation_seq_features_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_features}_{version_data_control}\"), X_validation_seq_features)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_validation_seq_reviews_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_reviews}_{version_data_control}\"), X_validation_seq_reviews)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//x_validation_seq_title_{split_ratio}_{data_balance}_{vocabulary_size_frequent_words_title}_{version_data_control}\"), X_validation_seq_title)\n",
    "\n",
    "#Dependent variable\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//y_train_{split_ratio}_{data_balance}_{version_data_control}\"), y_train)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//y_test_{split_ratio}_{data_balance}_{version_data_control}\"), y_test)\n",
    "np.save(os.path.join(os.getcwd(), f\"text_tokenized_padded_sequences//y_validation_{split_ratio}_{data_balance}_{version_data_control}\"), y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Serialize the dataset with the added cleaned columns of actors, plot, features, reviews and title for use in part 4 & 5.\n",
    "#### (2) X_test dataset is already serialized for use in part 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C://Users//spano//Desktop//nlp_github//datasets//dataset_part_3.2_and_4_22022021.pkl']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(dataset_nlp_tokenization, f\"{datasets_path}//dataset_part_3.2_and_4_{version_data_control}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THIS IS THE END OF PART 3.1 - Data Tokenization & Sequence padding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demoEnv",
   "language": "python",
   "name": "demoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
